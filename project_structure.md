# Entity Resolution Pipeline: Complete Project Structure

## Overview

This entity resolution pipeline processes Yale University Library Catalog data (derived from MARC 21 records) to identify and resolve entity matches across catalog entries. The system combines vector embeddings, feature engineering, and machine learning to achieve **99.55% precision** and **82.48% recall** in production.

## Pipeline Architecture

The system implements a **dual classification approach**:

1. **Main Pipeline**: Entity resolution using logistic regression with engineered features
2. **Auxiliary Classification**: Individual record taxonomy classification using parallel API processing and SetFit models

### Core Pipeline Stages

```
Input CSV â†’ Preprocessing â†’ Embedding & Indexing â†’ Training â†’ Classification â†’ Reporting
     â†“            â†“              â†“              â†“            â†“            â†“
  Hash-based   OpenAI API    Feature Eng.   Similarity   Clustering   HTML/CSV
  Dedup        Weaviate DB   Scaling        Matching     Results      Reports
```

## Project Directory Structure

```
entity_resolver/
â”œâ”€â”€ ğŸ“ Core Pipeline Components
â”‚   â”œâ”€â”€ main.py                          # CLI entry point with stage control
â”‚   â”œâ”€â”€ config.yml                       # Main pipeline configuration
â”‚   â”œâ”€â”€ scaling_config.yml               # Feature scaling configuration
â”‚   â”œâ”€â”€ docker-compose.yml               # Weaviate vector database setup
â”‚   â””â”€â”€ requirements.txt                 # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“ src/                              # Core pipeline modules
â”‚   â”œâ”€â”€ orchestrating.py                # Pipeline orchestration & stage management
â”‚   â”œâ”€â”€ preprocessing.py                # CSV processing & hash-based deduplication
â”‚   â”œâ”€â”€ embedding_and_indexing.py       # OpenAI embeddings & Weaviate integration
â”‚   â”œâ”€â”€ feature_engineering.py          # Feature calculation & caching system
â”‚   â”œâ”€â”€ training.py                     # Logistic regression classifier training
â”‚   â”œâ”€â”€ classifying.py                  # Entity matching & transitive clustering
â”‚   â”œâ”€â”€ reporting.py                    # Metrics, visualizations & HTML dashboards
â”‚   â”œâ”€â”€ scaling_bridge.py               # Feature scaling coordination
â”‚   â”œâ”€â”€ robust_scaler.py                # Domain-specific scaling strategies
â”‚   â”œâ”€â”€ checkpoint_manager.py           # Pipeline state persistence & resumption
â”‚   â”œâ”€â”€ taxonomy_feature.py             # SetFit taxonomy integration
â”‚   â”œâ”€â”€ birth_death_regexes.py          # Temporal entity matching patterns
â”‚   â”œâ”€â”€ custom_features.py              # Extensible feature registration system
â”‚   â”œâ”€â”€ utils.py                        # Utilities & helper functions
â”‚   â”œâ”€â”€ visualization.py                # Feature importance & performance plots
â”‚   â””â”€â”€ vector_diagnostics.py           # Vector similarity debugging tools
â”‚
â”œâ”€â”€ ğŸ“ scripts/                         # Individual Record Classification System
â”‚   â”œâ”€â”€ verify_individual_classifications.py         # Sequential record classification
â”‚   â””â”€â”€ verify_individual_classifications_parallel.py # Parallel API processing with rate limiting
â”‚
â”œâ”€â”€ ğŸ“ setfit/                          # SetFit Hierarchical Taxonomy Classification
â”‚   â”œâ”€â”€ train_setfit_classifier.py      # SetFit model training pipeline
â”‚   â”œâ”€â”€ predict_setfit_classifier.py    # SetFit prediction & classification
â”‚   â”œâ”€â”€ train_setfit_simple.py          # Simplified SetFit training (recently updated)
â”‚   â”œâ”€â”€ train_setfit_*.py               # Additional training variants (memory, server, etc.)
â”‚   â”œâ”€â”€ SETFIT_README.md               # SetFit system documentation
â”‚   â”œâ”€â”€ setfit_requirements.txt         # SetFit-specific dependencies
â”‚   â””â”€â”€ model_multilingual_minilm/      # Trained SetFit model artifacts
â”‚       â”œâ”€â”€ metadata.json
â”‚       â”œâ”€â”€ metadata.pkl
â”‚       â””â”€â”€ setfit_model/               # Model weights & configuration
â”‚
â”œâ”€â”€ ğŸ“ data/                            # Data Storage & Results
â”‚   â”œâ”€â”€ input/                          # Source datasets & taxonomies
â”‚   â”‚   â”œâ”€â”€ training_dataset_classified.csv           # Main classified training data
â”‚   â”‚   â”œâ”€â”€ training_dataset_classified_2025-06-17.csv # Updated training classifications
â”‚   â”‚   â”œâ”€â”€ revised_taxonomy_final.json               # SetFit taxonomy structure
â”‚   â”‚   â”œâ”€â”€ parallel_classifications.json             # Individual record classifications
â”‚   â”‚   â””â”€â”€ taxonomy*.json                            # Various taxonomy versions
â”‚   â”‚
â”‚   â”œâ”€â”€ checkpoints/                    # Pipeline State Persistence
â”‚   â”‚   â”œâ”€â”€ pipeline_state.json         # Current pipeline stage status
â”‚   â”‚   â”œâ”€â”€ classification_checkpoint.pkl # Classification progress checkpoints
â”‚   â”‚   â”œâ”€â”€ hash_lookup.pkl             # PersonId â†’ field hash mappings
â”‚   â”‚   â”œâ”€â”€ string_dict.pkl             # Hash â†’ original string mappings
â”‚   â”‚   â”œâ”€â”€ field_hash_mapping.pkl      # Hash â†’ field type relationships
â”‚   â”‚   â”œâ”€â”€ processed_hashes.pkl        # Embedding processing checkpoints
â”‚   â”‚   â””â”€â”€ *.pkl                       # Various stage-specific checkpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ output/                         # Results, Reports & Visualizations
â”‚   â”‚   â”œâ”€â”€ ğŸ“Š Performance Results
â”‚   â”‚   â”‚   â”œâ”€â”€ test_results_filtered.csv            # Detailed prediction analysis (false positives)
â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline_metrics.json               # Overall performance metrics
â”‚   â”‚   â”‚   â”œâ”€â”€ cluster_summary_report_*.json       # Clustering analysis results
â”‚   â”‚   â”‚   â”œâ”€â”€ entity_matches.csv                  # Identified entity matches
â”‚   â”‚   â”‚   â””â”€â”€ entity_clusters.json                # Transitive clustering results
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ğŸ“ˆ Visualizations & Reports
â”‚   â”‚   â”‚   â”œâ”€â”€ plots/                               # Feature analysis visualizations
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ feature_distributions/           # Individual feature distribution plots
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ class_separation/                # ROC/PR curves for each feature
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ feature_importance.png
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ probability_distribution.png
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ reports/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ feature_visualization_report.html # Interactive feature analysis
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ viz/                                 # Pipeline diagrams
â”‚   â”‚   â”‚       â”œâ”€â”€ pipeline.svg                     # Main pipeline architecture
â”‚   â”‚   â”‚       â””â”€â”€ taxonomy.svg                     # Taxonomy structure diagram
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ğŸ“‹ Classification Results
â”‚   â”‚   â”‚   â”œâ”€â”€ parallel_classifications.json        # Individual record classifications
â”‚   â”‚   â”‚   â”œâ”€â”€ individual_classifications.json.partial # Partial classification results
â”‚   â”‚   â”‚   â””â”€â”€ updated_identity_classification_map_*.json # Identity mappings
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ ğŸ“Š Telemetry & Monitoring
â”‚   â”‚       â””â”€â”€ telemetry/                           # Performance monitoring data
â”‚   â”‚           â””â”€â”€ telemetry_*.json                 # Timestamped execution metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ ground_truth/                   # Training Data
â”‚   â”‚   â””â”€â”€ labeled_matches.csv         # Ground truth entity pairs (left,right,match)
â”‚   â”‚
â”‚   â””â”€â”€ tmp/                           # Temporary Processing Files
â”‚       â”œâ”€â”€ Yale_catalog_disambiguated_names*.csv    # Catalog processing intermediates
â”‚       â”œâ”€â”€ clusters.csv
â”‚       â”œâ”€â”€ missing.csv
â”‚       â””â”€â”€ *.csv                       # Various temporary datasets
â”‚
â”œâ”€â”€ ğŸ“ logs/                            # Execution Logs
â”‚   â”œâ”€â”€ pipeline.log                    # Main pipeline execution log
â”‚   â”œâ”€â”€ individual_classification_verification.log          # Sequential classification log
â”‚   â””â”€â”€ individual_classification_verification_parallel.log # Parallel classification log
â”‚
â”œâ”€â”€ ğŸ“ Data Extraction & Processing      # Pre-Pipeline Data Preparation
â”‚   â”œâ”€â”€ extract-names-benchmark-2024-12-16-csv.xq   # XQuery BIBFRAMEâ†’CSV conversion
â”‚   â”œâ”€â”€ extract-names-2024-12-16.xq                 # Original XQuery extraction script
â”‚   â””â”€â”€ fix-classifications.xq                      # Classification correction script
â”‚
â”œâ”€â”€ ğŸ“ Analysis & Debugging Tools       # Development & Analysis Scripts
â”‚   â”œâ”€â”€ analyze_false_positives.py      # False positive analysis
â”‚   â”œâ”€â”€ inspect_test_data.py            # Test data inspection
â”‚   â”œâ”€â”€ test_taxonomy_*.py              # Taxonomy feature testing
â”‚   â”œâ”€â”€ debug_*.py                      # Various debugging utilities
â”‚   â”œâ”€â”€ regenerate_*.py                 # Plot regeneration scripts
â”‚   â””â”€â”€ classification_embedding_script_*.py # Embedding analysis tools
â”‚
â”œâ”€â”€ ğŸ“ Visualization & Presentation     # Documentation & Presentation
â”‚   â”œâ”€â”€ create_presentation_visuals.py  # Presentation chart generation
â”‚   â”œâ”€â”€ svg_to_png.py                   # Format conversion utility
â”‚   â”œâ”€â”€ README.md                       # Main project documentation
â”‚   â”œâ”€â”€ project_structure.md            # This file - detailed project structure
â”‚   â”œâ”€â”€ Executive_Summary_Lightning_Talk.md # Project summary
â”‚   â”œâ”€â”€ Lightning_Talk_Script.md        # Presentation script
â”‚   â”œâ”€â”€ Real_Data_Examples_Summary.md   # Data examples documentation
â”‚   â””â”€â”€ *.md                           # Additional documentation files
â”‚
â””â”€â”€ ğŸ“ Development Environment          # Development Setup
    â”œâ”€â”€ venv/                           # Python virtual environment
    â”œâ”€â”€ LICENSE                         # Project license
    â””â”€â”€ .env                           # Environment variables (API keys)
```

## Key Data Formats

### Input Data Structure
```csv
composite,person,roles,title,provision,subjects,personId,setfit_prediction,is_parent_category
"Contributor: Bach, Johann Sebastian, 1685-1750...",Bach\, Johann Sebastian,Contributor,The Well-Tempered Clavier,...,...,...,12345#Agent700-1,Music\, Sound\, and Sonic Arts,FALSE
```

### Ground Truth Format
```csv
left,right,match
16044091#Agent700-32,9356808#Agent100-11,true
16044091#Agent700-32,9940747#Hub240-13-Agent,true
```

### Pipeline Output
- **Entity Matches**: CSV with predicted pairs and confidence scores
- **Entity Clusters**: JSON with transitive clustering results  
- **Classification Results**: JSON with individual record classifications
- **Performance Metrics**: Detailed confusion matrix and feature analysis

## Feature Engineering System

### Currently Active Features (5 total)
1. **person_cosine**: Cosine similarity between person name embeddings
2. **person_title_squared**: Squared person-title interaction term
3. **composite_cosine**: Full record composite similarity
4. **taxonomy_dissimilarity**: SetFit domain classification differences
5. **birth_death_match**: Binary temporal matching with tolerance

### Feature Scaling Strategy
- **Feature Groups**: Domain-specific scaling (person: 98th percentile, title: 95th, context: 90th)
- **Binary Preservation**: Maintains exact 0.0/1.0 values for binary indicators
- **Training/Production Consistency**: Identical scaling in both environments

## Technology Stack

### Core Dependencies
- **Vector Database**: Weaviate with HNSW indexing
- **Embeddings**: OpenAI text-embedding-3-small (1536D)
- **ML Framework**: Custom logistic regression with gradient descent
- **Taxonomy Classification**: SetFit (Sentence Transformers + logistic head)
- **Parallel Processing**: asyncio/aiohttp for API rate limit optimization

### Production Features
- **Checkpointing**: Complete pipeline state persistence and resumption
- **Rate Limiting**: Anthropic API optimization for high-tier accounts (4,000 RPM)
- **Error Resilience**: Comprehensive exception handling and retry logic
- **Monitoring**: Detailed telemetry, memory usage tracking, and performance metrics
- **Scalability**: Configurable batch processing and worker allocation

## Performance Characteristics

### Latest Results (Test Set: 14,930 pairs)
- **Precision**: 99.55% (9,955 TP, 45 FP)
- **Recall**: 82.48% (2,114 FN, 9,955 TP)
- **F1-Score**: 90.22%
- **Specificity**: 98.43%
- **Accuracy**: 85.54%

### Computational Efficiency
- **ANN Reduction**: 99.23% reduction in pairwise comparisons via vector similarity
- **Cluster Analysis**: 163 clusters, 4,672 entities, 83,921 comparisons
- **Throughput**: Optimized for high-volume library catalog processing

## Development Workflow

### Pipeline Execution
```bash
# Complete pipeline
python main.py --config config.yml

# Stage-specific execution  
python main.py --start training --end classification

# Resume from checkpoints
python main.py --resume

# Reset specific stages
python main.py --reset training classifying
```

### Individual Record Classification
```bash
# Parallel processing (optimized for API limits)
python scripts/verify_individual_classifications_parallel.py \
    --csv data/input/training_dataset.csv \
    --taxonomy data/input/revised_taxonomy_final.json \
    --output data/output/parallel_classifications.json \
    --concurrency 5
```

### SetFit Training
```bash
# Train hierarchy-aware taxonomy classifier
python setfit/train_setfit_classifier.py \
    --csv_path data/input/training_dataset.csv \
    --ground_truth_path data/output/updated_identity_classification_map_v6_pruned.json \
    --output_dir ./setfit_model_output
```

## Recent Developments

### Individual Record Classification Enhancement
- **Parallel API Processing**: Optimized for Anthropic rate limits (200K tokens/min)
- **Rate Limiting**: Automatic token usage analysis and concurrency adjustment
- **Progress Logging**: Detailed classification progress tracking
- **Compatibility**: Updated training scripts to use personId instead of identity mapping

### Pipeline Improvements
- **Feature Analysis**: Identified inactive features (taxonomy_dissimilarity, birth_death_match)
- **Performance Optimization**: Reduced ANN search overhead by 99.23%
- **Error Analysis**: Comprehensive false positive pattern analysis
- **Visualization**: Enhanced feature importance and distribution plots

This structure represents a production-ready entity resolution system with comprehensive tooling for analysis, debugging, and performance optimization.