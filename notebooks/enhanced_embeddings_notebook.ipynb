{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Words to Meaning: The Evolution of Text Embeddings\n",
    "## A Complete Journey from Word2Vec to Sentence Transformers\n",
    "\n",
    "*How computers learned to understand human language through the lens of library metadata*\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [The Fundamental Challenge](#fundamental)\n",
    "2. [Word Embeddings: The Foundation](#word-embeddings)\n",
    "3. [From Words to Sentences](#sentence-embeddings)\n",
    "4. [Modern Text Understanding](#text-embeddings)\n",
    "5. [Real-World Application: Entity Resolution](#entity-resolution)\n",
    "6. [Cross-Language Capabilities](#multilingual)\n",
    "7. [Practical Implementation](#implementation)\n",
    "\n",
    "---\n",
    "\n",
    "## The Story We're Telling\n",
    "\n",
    "This notebook explores how artificial intelligence learned to understand human language by examining three generations of text representation:\n",
    "\n",
    "üî§ **Word Embeddings** (2013): Individual words ‚Üí numerical vectors  \n",
    "üìù **Sentence Embeddings** (2019): Complete sentences ‚Üí meaningful representations  \n",
    "üåê **Text Embeddings** (2022+): Any text ‚Üí contextual understanding  \n",
    "\n",
    "We'll use a real challenge from Yale University Library as our guide: **How do you tell when two catalog records refer to the same person?**\n",
    "\n",
    "Consider these records:\n",
    "- `\"Schubert, Franz (1797-1828). String Quartet in D minor\"`\n",
    "- `\"Schubert, Franz. Archaeological Photography Methods. 1978\"`\n",
    "\n",
    "To a human, it's obvious these refer to different people. To a computer, they're just strings with similar characters. This notebook shows how embeddings solve this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Prerequisites\n",
    "\n",
    "Let's install the libraries we'll need for our journey through embeddings evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for our embeddings journey\n",
    "!pip install sentence-transformers gensim sklearn matplotlib seaborn pandas numpy plotly weaviate-client\n",
    "\n",
    "# For interactive visualizations\n",
    "!pip install umap-learn[plot] wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"üöÄ Ready to explore the evolution of text embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 1: The Fundamental Challenge {#fundamental}\n",
    "\n",
    "## Why Computers Struggle with Human Language\n",
    "\n",
    "Before we dive into solutions, let's understand the core problem. Computers process numbers, but human language is symbolic, contextual, and infinitely creative.\n",
    "\n",
    "### The ASCII Illusion\n",
    "\n",
    "When you type \"revolution\", here's what the computer actually sees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What humans see vs. what computers see\n",
    "word = \"revolution\"\n",
    "ascii_codes = [ord(char) for char in word]\n",
    "\n",
    "print(f\"Human sees: {word}\")\n",
    "print(f\"Computer sees: {ascii_codes}\")\n",
    "print(f\"As characters: {[chr(code) for code in ascii_codes]}\")\n",
    "\n",
    "# The fundamental problem: these numbers tell us nothing about meaning\n",
    "print(\"\\nü§î The Problem:\")\n",
    "print(\"- The number 114 ('r') has no inherent relationship to concepts\")\n",
    "print(\"- ASCII codes are arbitrary assignments\")\n",
    "print(\"- No way to compute that 'revolution' relates to 'change' or 'uprising'\")\n",
    "\n",
    "# Visualize the meaninglessness of ASCII\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# ASCII values\n",
    "ax1.bar(range(len(word)), ascii_codes, color='lightcoral')\n",
    "ax1.set_title('ASCII Values: Meaningless Numbers')\n",
    "ax1.set_xlabel('Character Position')\n",
    "ax1.set_ylabel('ASCII Value')\n",
    "ax1.set_xticks(range(len(word)))\n",
    "ax1.set_xticklabels(list(word))\n",
    "\n",
    "# Character frequency (also meaningless for meaning)\n",
    "char_counts = {char: word.count(char) for char in set(word)}\n",
    "ax2.bar(char_counts.keys(), char_counts.values(), color='lightblue')\n",
    "ax2.set_title('Character Frequency: Still No Meaning')\n",
    "ax2.set_xlabel('Character')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° The Insight: We need representations that capture semantic relationships, not just character patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Library Catalog Challenge\n",
    "\n",
    "Let's ground this in a real problem from Yale University Library's catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real catalog records that need disambiguation\n",
    "catalog_records = {\n",
    "    \"record_1\": {\n",
    "        \"text\": \"Schubert, Franz (1797-1828). String Quartet in D minor 'Death and the Maiden'. C.F. Peters, Leipzig, 1831. Subject: Chamber music; String quartets\",\n",
    "        \"person\": \"Franz Schubert (composer)\",\n",
    "        \"domain\": \"Music\",\n",
    "        \"year\": 1828\n",
    "    },\n",
    "    \"record_2\": {\n",
    "        \"text\": \"Schubert, Franz. Arch√§ologie und Photographie: f√ºnfzig Beispiele zur Geschichte und Methode. P. von Zabern, Mainz, 1978. Subject: Archaeology; Photography\",\n",
    "        \"person\": \"Franz August Schubert (archaeologist)\", \n",
    "        \"domain\": \"Archaeology\",\n",
    "        \"year\": 1978\n",
    "    },\n",
    "    \"record_3\": {\n",
    "        \"text\": \"Garc√≠a M√°rquez, Gabriel. One Hundred Years of Solitude. 1967. Literature.\",\n",
    "        \"person\": \"Gabriel Garc√≠a M√°rquez\",\n",
    "        \"domain\": \"Literature\", \n",
    "        \"year\": 1967\n",
    "    },\n",
    "    \"record_4\": {\n",
    "        \"text\": \"Gabriel Garcia Marquez. Cien a√±os de soledad. 1967. Literature.\",\n",
    "        \"person\": \"Gabriel Garc√≠a M√°rquez\",\n",
    "        \"domain\": \"Literature\",\n",
    "        \"year\": 1967\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display the challenge\n",
    "print(\"üèõÔ∏è Yale Library Catalog Challenge:\")\n",
    "print(\"\\nWhich of these records refer to the same person?\\n\")\n",
    "\n",
    "for key, record in catalog_records.items():\n",
    "    print(f\"{key.upper()}:\")\n",
    "    print(f\"  Text: {record['text'][:80]}...\")\n",
    "    print(f\"  Domain: {record['domain']} | Year: {record['year']}\")\n",
    "    print()\n",
    "\n",
    "print(\"ü§ñ Traditional string matching fails here because:\")\n",
    "print(\"  ‚Ä¢ 'Schubert, Franz' appears in both Record 1 and 2 (different people!)\")\n",
    "print(\"  ‚Ä¢ 'Garc√≠a M√°rquez' vs 'Garcia Marquez' (same person, different spelling!)\")\n",
    "print(\"  ‚Ä¢ Context matters: music vs archaeology, 1828 vs 1978\")\n",
    "print(\"\\nüí° We need AI that understands meaning, not just character patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 2: Word Embeddings - The Foundation {#word-embeddings}\n",
    "\n",
    "## The Breakthrough: Learning Meaning from Context\n",
    "\n",
    "In 2013, Google researchers introduced Word2Vec, based on a simple but powerful insight:\n",
    "\n",
    "> **\"You shall know a word by the company it keeps\"** - J.R. Firth (1957)\n",
    "\n",
    "Words that appear in similar contexts tend to have similar meanings.\n",
    "\n",
    "### How Word2Vec Works\n",
    "\n",
    "Let's see this in action using a real Word2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Word2Vec model\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(\"üì• Loading Word2Vec model (this may take a few minutes...)\")\n",
    "print(\"Model: Google News vectors (3 million words, 300 dimensions)\")\n",
    "\n",
    "# Load a smaller model for demonstration\n",
    "try:\n",
    "    # Try the Google News model first\n",
    "    word2vec_model = api.load('word2vec-google-news-300')\n",
    "    print(\"‚úÖ Loaded Google News Word2Vec model\")\nexcept:\n",
    "    # Fallback to a smaller model\n",
    "    print(\"‚ö†Ô∏è Google News model too large, using smaller glove-wiki-gigaword-50\")\n",
    "    word2vec_model = api.load('glove-wiki-gigaword-50')\n",
    "    print(\"‚úÖ Loaded GloVe model as Word2Vec demonstration\")\n",
    "\n",
    "print(f\"üìä Model contains {len(word2vec_model)} words\")\n",
    "print(f\"üî¢ Each word is represented by {word2vec_model.vector_size} numbers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Magic of Semantic Relationships\n",
    "\n",
    "Word2Vec learned to capture semantic relationships through vector arithmetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate semantic relationships\n",
    "def explore_word_relationships(model, word, top_n=5):\n",
    "    \"\"\"Find words most similar to the given word\"\"\"\n",
    "    if word in model:\n",
    "        similar_words = model.most_similar(word, topn=top_n)\n",
    "        print(f\"üîç Words most similar to '{word}':\")\n",
    "        for similar_word, similarity in similar_words:\n",
    "            print(f\"   {similar_word}: {similarity:.3f}\")\n",
    "    else:\n",
    "        print(f\"‚ùå '{word}' not found in model vocabulary\")\n",
    "    print()\n",
    "\n",
    "# Test with words relevant to our library catalog\n",
    "test_words = ['music', 'composer', 'literature', 'archaeology', 'revolution']\n",
    "\n",
    "print(\"üéµ Exploring Semantic Neighborhoods:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for word in test_words:\n",
    "    explore_word_relationships(word2vec_model, word, top_n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Famous King - Man + Woman = Queen Example\n",
    "\n",
    "Word2Vec can solve analogy problems through vector arithmetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector arithmetic for analogies\n",
    "def solve_analogy(model, word1, word2, word3, top_n=3):\n",
    "    \"\"\"\n",
    "    Solve analogies: word1 is to word2 as word3 is to ?\n",
    "    Using vector arithmetic: word2 - word1 + word3\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = model.most_similar(\n",
    "            positive=[word2, word3], \n",
    "            negative=[word1], \n",
    "            topn=top_n\n",
    "        )\n",
    "        print(f\"üßÆ {word1} is to {word2} as {word3} is to:\")\n",
    "        for word, similarity in result:\n",
    "            print(f\"   {word} ({similarity:.3f})\")\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå Word not found: {e}\")\n",
    "    print()\n",
    "\n",
    "# Test classic analogies\n",
    "print(\"üß† Vector Arithmetic Analogies:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Gender relationships\n",
    "solve_analogy(word2vec_model, 'man', 'woman', 'king')\n",
    "solve_analogy(word2vec_model, 'man', 'woman', 'actor')\n",
    "\n",
    "# Geographic relationships\n",
    "solve_analogy(word2vec_model, 'France', 'Paris', 'Germany')\n",
    "solve_analogy(word2vec_model, 'Japan', 'Tokyo', 'England')\n",
    "\n",
    "# Professional relationships\n",
    "solve_analogy(word2vec_model, 'teach', 'teacher', 'write')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Word Embeddings\n",
    "\n",
    "Let's visualize how Word2Vec represents our library catalog terms in high-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select words related to our catalog domains\n",
    "catalog_terms = [\n",
    "    # Music domain\n",
    "    'music', 'composer', 'symphony', 'piano', 'orchestra',\n",
    "    # Literature domain  \n",
    "    'literature', 'novel', 'poetry', 'author', 'book',\n",
    "    # Archaeology domain\n",
    "    'archaeology', 'artifact', 'excavation', 'ancient', 'museum',\n",
    "    # General academic terms\n",
    "    'research', 'study', 'analysis', 'theory', 'method'\n",
    "]\n",
    "\n",
    "# Get embeddings for terms that exist in our model\n",
    "available_terms = [term for term in catalog_terms if term in word2vec_model]\n",
    "embeddings = np.array([word2vec_model[term] for term in available_terms])\n",
    "\n",
    "print(f\"üìä Visualizing {len(available_terms)} catalog terms\")\n",
    "print(f\"üìè Original dimensions: {embeddings.shape[1]}\")\n",
    "\n",
    "# Reduce to 2D using t-SNE for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(available_terms)-1))\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Create an interactive scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Color by domain\n",
    "colors = []\n",
    "music_terms = ['music', 'composer', 'symphony', 'piano', 'orchestra']\n",
    "lit_terms = ['literature', 'novel', 'poetry', 'author', 'book']\n",
    "arch_terms = ['archaeology', 'artifact', 'excavation', 'ancient', 'museum']\n",
    "\n",
    "for term in available_terms:\n",
    "    if term in music_terms:\n",
    "        colors.append('üéµ Music')\n",
    "    elif term in lit_terms:\n",
    "        colors.append('üìö Literature') \n",
    "    elif term in arch_terms:\n",
    "        colors.append('üè∫ Archaeology')\n",
    "    else:\n",
    "        colors.append('üî¨ General')\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=embeddings_2d[:, 0],\n",
    "    y=embeddings_2d[:, 1],\n",
    "    mode='markers+text',\n",
    "    text=available_terms,\n",
    "    textposition='middle right',\n",
    "    marker=dict(\n",
    "        size=12,\n",
    "        color=[hash(c) for c in colors],\n",
    "        colorscale='Set3',\n",
    "        showscale=False\n",
    "    ),\n",
    "    hovertemplate='<b>%{text}</b><br>Domain: %{customdata}<extra></extra>',\n",
    "    customdata=colors\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Word2Vec Embeddings: Catalog Terms in Semantic Space',\n",
    "    xaxis_title='t-SNE Dimension 1',\n",
    "    yaxis_title='t-SNE Dimension 2',\n",
    "    width=800,\n",
    "    height=600,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüí° Notice how semantically related words cluster together!\")\n",
    "print(\"   This is the breakthrough: meaning becomes spatial relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Applied to Our Catalog Problem\n",
    "\n",
    "Let's see how Word2Vec handles our Franz Schubert disambiguation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key terms from our catalog records\n",
    "record_terms = {\n",
    "    \"Schubert_composer\": ['music', 'quartet', 'chamber', 'composer'],\n",
    "    \"Schubert_archaeologist\": ['archaeology', 'photography', 'method', 'history'],\n",
    "    \"Garcia_Marquez_1\": ['literature', 'novel', 'solitude', 'years'],\n",
    "    \"Garcia_Marquez_2\": ['literature', 'novel', 'soledad', 'a√±os']  # Spanish version\n",
    "}\n",
    "\n",
    "def calculate_record_similarity(terms1, terms2, model):\n",
    "    \"\"\"Calculate similarity between two records using average word embeddings\"\"\"\n",
    "    # Get embeddings for available terms\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "    \n",
    "    for term in terms1:\n",
    "        if term in model:\n",
    "            vec1.append(model[term])\n",
    "    \n",
    "    for term in terms2:\n",
    "        if term in model:\n",
    "            vec2.append(model[term])\n",
    "    \n",
    "    if not vec1 or not vec2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Average the word vectors\n",
    "    avg_vec1 = np.mean(vec1, axis=0)\n",
    "    avg_vec2 = np.mean(vec2, axis=0)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity([avg_vec1], [avg_vec2])[0][0]\n",
    "    return similarity\n",
    "\n",
    "# Calculate all pairwise similarities\n",
    "print(\"üîç Word2Vec Record Similarity Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "records = list(record_terms.keys())\n",
    "similarity_matrix = np.zeros((len(records), len(records)))\n",
    "\n",
    "for i, record1 in enumerate(records):\n",
    "    for j, record2 in enumerate(records):\n",
    "        similarity = calculate_record_similarity(\n",
    "            record_terms[record1], \n",
    "            record_terms[record2], \n",
    "            word2vec_model\n",
    "        )\n",
    "        similarity_matrix[i][j] = similarity\n",
    "        \n",
    "        if i < j:  # Only print upper triangle\n",
    "            print(f\"{record1} ‚Üî {record2}: {similarity:.3f}\")\n",
    "\n",
    "# Visualize the similarity matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    similarity_matrix, \n",
    "    annot=True, \n",
    "    fmt='.3f',\n",
    "    xticklabels=records,\n",
    "    yticklabels=records,\n",
    "    cmap='coolwarm',\n",
    "    center=0.5,\n",
    "    square=True\n",
    ")\n",
    "plt.title('Word2Vec Similarity Matrix: Catalog Records')\n",
    "plt.xlabel('Record')\n",
    "plt.ylabel('Record')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Analysis:\")\n",
    "print(f\"‚úÖ Garc√≠a M√°rquez records (same person): {similarity_matrix[2][3]:.3f}\")\n",
    "print(f\"‚ùå Two Schubert records (different people): {similarity_matrix[0][1]:.3f}\")\n",
    "print(\"\\nüí° Word2Vec successfully distinguishes domains but misses person identity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Limitations of Word2Vec\n",
    "\n",
    "Word2Vec was revolutionary, but it has important limitations for our catalog problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Word2Vec limitations\n",
    "print(\"‚ö†Ô∏è Word2Vec Limitations for Library Catalogs:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Out-of-vocabulary problem\n",
    "test_names = ['Schubert', 'Garc√≠a', 'M√°rquez']\n",
    "print(\"1. üìù Name Recognition:\")\n",
    "for name in test_names:\n",
    "    if name in word2vec_model:\n",
    "        print(f\"   ‚úÖ '{name}' found in vocabulary\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå '{name}' NOT in vocabulary\")\n",
    "\n",
    "# 2. No context sensitivity\n",
    "print(\"\\n2. üé≠ Context Insensitivity:\")\n",
    "if 'bank' in word2vec_model:\n",
    "    bank_vector = word2vec_model['bank']\n",
    "    print(\"   ‚Ä¢ 'bank' has the same representation in:\")\n",
    "    print(\"     - 'The bank approved the loan' (financial)\")\n",
    "    print(\"     - 'We sat by the river bank' (geographic)\")\n",
    "    print(f\"   ‚Ä¢ Single vector: {bank_vector[:5]}... (same for both contexts)\")\n",
    "\n",
    "# 3. Composition challenge\n",
    "print(\"\\n3. üß© Composition Problem:\")\n",
    "print(\"   ‚Ä¢ How do you represent: 'Franz Schubert String Quartet'?\")\n",
    "print(\"   ‚Ä¢ Simple averaging loses important relationships\")\n",
    "print(\"   ‚Ä¢ Word order and syntax matter for meaning\")\n",
    "\n",
    "# 4. Fixed vocabulary\n",
    "print(\"\\n4. üìö Fixed Vocabulary:\")\n",
    "print(\"   ‚Ä¢ Cannot handle new authors, titles, or terminology\")\n",
    "print(\"   ‚Ä¢ Retraining required for domain-specific vocabulary\")\n",
    "print(\"   ‚Ä¢ No handling of typos or variant spellings\")\n",
    "\n",
    "print(\"\\nüîÆ The Next Evolution: We need embeddings that understand...\")\n",
    "print(\"   ‚Ä¢ Complete sentences and their structure\")\n",
    "print(\"   ‚Ä¢ Context-dependent meanings\")\n",
    "print(\"   ‚Ä¢ Out-of-vocabulary terms and proper names\")\n",
    "print(\"   ‚Ä¢ Multiple languages and cultural contexts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 3: From Words to Sentences {#sentence-embeddings}\n",
    "\n",
    "## The Transformer Revolution (2017-2019)\n",
    "\n",
    "The next breakthrough came with the **Transformer architecture** and models like **BERT** (2018), which introduced:\n",
    "\n",
    "üéØ **Contextual Embeddings**: Same word, different vectors based on context  \n",
    "üîÑ **Bidirectional Understanding**: Processing text in both directions  \n",
    "üìù **Sentence-Level Representations**: Understanding complete thoughts  \n",
    "\n",
    "### Loading a Modern Sentence Transformer\n",
    "\n",
    "Let's load the same model used in Yale's entity resolution pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"ü§ñ Loading Sentence Transformer...\")\n",
    "print(\"Model: paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "print(\"‚Ä¢ Understands 50+ languages\")\n",
    "print(\"‚Ä¢ 110M parameters (much smaller than GPT models!)\")\n",
    "print(\"‚Ä¢ Produces 384-dimensional embeddings\")\n",
    "\n",
    "# Load the same model used at Yale\n",
    "sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded successfully!\")\n",
    "print(f\"üìè Embedding dimensions: {sentence_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"üî§ Max sequence length: {sentence_model.max_seq_length} tokens\")\n",
    "\n",
    "# Test with a simple sentence\n",
    "test_sentence = \"Franz Schubert composed symphonies.\"\n",
    "test_embedding = sentence_model.encode(test_sentence)\n",
    "\n",
    "print(f\"\\nüß™ Test encoding:\")\n",
    "print(f\"Input: '{test_sentence}'\")\n",
    "print(f\"Output: {test_embedding.shape} vector\")\n",
    "print(f\"Sample values: [{test_embedding[0]:.3f}, {test_embedding[1]:.3f}, {test_embedding[2]:.3f}, ...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Sensitivity: The Game Changer\n",
    "\n",
    "Unlike Word2Vec, sentence transformers understand context. Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate context sensitivity\n",
    "context_examples = [\n",
    "    \"The bank approved the mortgage loan for the house.\",\n",
    "    \"We sat by the river bank watching the sunset.\",\n",
    "    \"The data bank contains millions of records.\"\n",
    "]\n",
    "\n",
    "print(\"üé≠ Context Sensitivity Demonstration:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get embeddings for each context\n",
    "context_embeddings = sentence_model.encode(context_examples)\n",
    "\n",
    "# Calculate similarities between different uses of \"bank\"\n",
    "similarities = cosine_similarity(context_embeddings)\n",
    "\n",
    "for i, sentence1 in enumerate(context_examples):\n",
    "    print(f\"\\n{i+1}. {sentence1}\")\n",
    "    for j, sentence2 in enumerate(context_examples):\n",
    "        if i < j:\n",
    "            sim = similarities[i][j]\n",
    "            print(f\"   ‚Üî Similarity to sentence {j+1}: {sim:.3f}\")\n",
    "\n",
    "# Visualize the context relationships\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    similarities,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    xticklabels=[f\"Bank {i+1}\" for i in range(len(context_examples))],\n",
    "    yticklabels=[f\"Bank {i+1}\" for i in range(len(context_examples))],\n",
    "    cmap='coolwarm',\n",
    "    center=0.5,\n",
    "    square=True\n",
    ")\n",
    "plt.title('Context-Aware Similarity: Different Meanings of \"Bank\"')\n",
    "plt.xlabel('Sentence')\n",
    "plt.ylabel('Sentence')\n",
    "\n",
    "# Add context labels\n",
    "context_labels = ['Financial', 'Geographic', 'Digital']\n",
    "for i, label in enumerate(context_labels):\n",
    "    plt.text(i + 0.5, -0.1, f\"({label})\", ha='center', va='top', \n",
    "             transform=plt.gca().get_xaxis_transform())\n",
    "    plt.text(-0.1, i + 0.5, f\"({label})\", ha='right', va='center',\n",
    "             transform=plt.gca().get_yaxis_transform())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice how financial 'bank' is most similar to digital 'bank'\")\n",
    "print(\"   but both are different from geographic 'bank'!\")\n",
    "print(\"\\nüéØ This is impossible with Word2Vec - same word, different meanings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Catalog Problem with Sentence Embeddings\n",
    "\n",
    "Now let's apply sentence transformers to our Franz Schubert disambiguation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our catalog records as complete sentences\n",
    "catalog_sentences = {\n",
    "    \"Franz Schubert (Composer)\": \"Franz Schubert, 1797-1828, String Quartet in D minor, chamber music, classical composer\",\n",
    "    \"Franz Schubert (Archaeologist)\": \"Franz Schubert, Archaeology and Photography, historical methods, 1978, research\",\n",
    "    \"Garc√≠a M√°rquez (English)\": \"Gabriel Garc√≠a M√°rquez, One Hundred Years of Solitude, 1967, Literature, magical realism\",\n",
    "    \"Garc√≠a M√°rquez (Spanish)\": \"Gabriel Garcia Marquez, Cien a√±os de soledad, 1967, Literatura, realismo m√°gico\",\n",
    "    \"Einstein (Physics)\": \"Albert Einstein, Special Theory of Relativity, 1905, Physics, theoretical physics\",\n",
    "    \"Einstein (Different)\" : \"Einstein Bagels, New York breakfast chain, founded 1995, restaurant business\"\n",
    "}\n",
    "\n",
    "print(\"üß¨ Sentence Embeddings for Entity Resolution:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get embeddings for all records\n",
    "sentences = list(catalog_sentences.values())\n",
    "labels = list(catalog_sentences.keys())\n",
    "embeddings = sentence_model.encode(sentences)\n",
    "\n",
    "# Calculate similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Create a comprehensive heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(similarity_matrix, dtype=bool), k=1)\n",
    "\n",
    "sns.heatmap(\n",
    "    similarity_matrix,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    cmap='RdYlBu_r',\n",
    "    center=0.5,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "\n",
    "plt.title('Sentence Transformer Similarity Matrix\\nSolving the Entity Resolution Problem', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Entity Records', fontweight='bold')\n",
    "plt.ylabel('Entity Records', fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze the results\n",
    "print(\"\\nüìä Key Similarity Scores:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Same person comparisons\n",
    "garcia_sim = similarity_matrix[2][3]  # Garc√≠a M√°rquez English vs Spanish\n",
    "print(f\"‚úÖ Garc√≠a M√°rquez (EN ‚Üî ES): {garcia_sim:.3f} - SAME PERSON\")\n",
    "\n",
    "# Different person comparisons  \n",
    "schubert_sim = similarity_matrix[0][1]  # Composer vs Archaeologist\n",
    "einstein_sim = similarity_matrix[4][5]  # Physicist vs Bagel chain\n",
    "print(f\"‚ùå Franz Schubert (Composer ‚Üî Archaeologist): {schubert_sim:.3f} - DIFFERENT PEOPLE\")\n",
    "print(f\"‚ùå Einstein (Physicist ‚Üî Bagel Chain): {einstein_sim:.3f} - DIFFERENT ENTITIES\")\n",
    "\n",
    "# Cross-domain comparisons\n",
    "cross_domain = similarity_matrix[0][2]  # Schubert Composer vs Garc√≠a M√°rquez\n",
    "print(f\"üîÑ Cross-domain (Schubert ‚Üî Garc√≠a M√°rquez): {cross_domain:.3f} - DIFFERENT PEOPLE\")\n",
    "\n",
    "print(f\"\\nüéØ Success Metrics:\")\n",
    "print(f\"   ‚Ä¢ Same person threshold: ‚â• 0.7 ‚Üí Garc√≠a M√°rquez: {garcia_sim:.3f} ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Different people: < 0.7 ‚Üí Franz Schuberts: {schubert_sim:.3f} ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Semantic understanding enables entity resolution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual Magic\n",
    "\n",
    "One of the most powerful features is cross-language understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multilingual understanding\n",
    "multilingual_examples = {\n",
    "    \"English\": \"The revolution transformed society completely\",\n",
    "    \"Spanish\": \"La revoluci√≥n transform√≥ la sociedad completamente\", \n",
    "    \"French\": \"La r√©volution a transform√© la soci√©t√© compl√®tement\",\n",
    "    \"German\": \"Die Revolution ver√§nderte die Gesellschaft vollst√§ndig\",\n",
    "    \"Portuguese\": \"A revolu√ß√£o transformou a sociedade completamente\"\n",
    "}\n",
    "\n",
    "print(\"üåç Multilingual Semantic Understanding:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get embeddings for all languages\n",
    "languages = list(multilingual_examples.keys())\n",
    "texts = list(multilingual_examples.values())\n",
    "multilingual_embeddings = sentence_model.encode(texts)\n",
    "\n",
    "# Calculate cross-language similarities\n",
    "cross_lang_similarities = cosine_similarity(multilingual_embeddings)\n",
    "\n",
    "# Display the results\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"\\n{languages[i]:>10}: {text}\")\n",
    "\n",
    "print(\"\\nüìä Cross-Language Similarity Matrix:\")\n",
    "\n",
    "# Create a beautiful multilingual heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cross_lang_similarities,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    xticklabels=languages,\n",
    "    yticklabels=languages,\n",
    "    cmap='viridis',\n",
    "    center=0.85,\n",
    "    square=True,\n",
    "    linewidths=1,\n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "\n",
    "plt.title('Cross-Language Semantic Similarity\\n\"The revolution transformed society\"', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Language', fontweight='bold')\n",
    "plt.ylabel('Language', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate average cross-language similarity\n",
    "off_diagonal = cross_lang_similarities[np.triu_indices_from(cross_lang_similarities, k=1)]\n",
    "avg_similarity = np.mean(off_diagonal)\n",
    "\n",
    "print(f\"\\nüéØ Average cross-language similarity: {avg_similarity:.3f}\")\n",
    "print(f\"\\nüåü This is revolutionary for research:\")\n",
    "print(\"   ‚Ä¢ Compare concepts across languages\")\n",
    "print(\"   ‚Ä¢ Unified analysis of multilingual corpora\")\n",
    "print(\"   ‚Ä¢ Translation validation and quality assessment\")\n",
    "print(\"   ‚Ä¢ Cultural studies across linguistic boundaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Architecture Behind the Magic\n",
    "\n",
    "Let's peek under the hood at what makes sentence transformers work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the model architecture\n",
    "print(\"üèóÔ∏è Sentence Transformer Architecture:\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Model structure\n",
    "print(f\"üìã Model Configuration:\")\n",
    "print(f\"   ‚Ä¢ Base Model: {sentence_model._modules['0'].__class__.__name__}\")\n",
    "print(f\"   ‚Ä¢ Pooling Strategy: {sentence_model._modules['1'].__class__.__name__}\")\n",
    "print(f\"   ‚Ä¢ Output Dimensions: {sentence_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"   ‚Ä¢ Max Token Length: {sentence_model.max_seq_length}\")\n",
    "\n",
    "# Demonstrate the pooling process\n",
    "sample_text = \"Franz Schubert composed beautiful music\"\n",
    "print(f\"\\nüîç Encoding Process for: '{sample_text}'\")\n",
    "\n",
    "# Tokenize the text (approximate - actual tokenization is more complex)\n",
    "words = sample_text.split()\n",
    "print(f\"   1. Tokenization: {words}\")\n",
    "print(f\"   2. Each token ‚Üí {sentence_model.get_sentence_embedding_dimension()}-dimensional vector\")\n",
    "print(f\"   3. Pooling (averaging) ‚Üí Single {sentence_model.get_sentence_embedding_dimension()}-dimensional sentence vector\")\n",
    "\n",
    "# Show the actual embedding\n",
    "embedding = sentence_model.encode(sample_text)\n",
    "print(f\"   4. Final embedding shape: {embedding.shape}\")\n",
    "print(f\"   5. Sample values: [{embedding[0]:.3f}, {embedding[1]:.3f}, {embedding[2]:.3f}, ...]\")\n",
    "\n",
    "# Visualize the process\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Step 1: Tokens\n",
    "axes[0].bar(range(len(words)), [1] * len(words), color='lightblue')\n",
    "axes[0].set_xticks(range(len(words)))\n",
    "axes[0].set_xticklabels(words, rotation=45)\n",
    "axes[0].set_title('1. Input Tokens')\n",
    "axes[0].set_ylabel('Token')\n",
    "\n",
    "# Step 2: Individual embeddings (simulated)\n",
    "fake_embeddings = np.random.randn(len(words), 5)  # Simplified to 5 dims for visualization\n",
    "im = axes[1].imshow(fake_embeddings.T, cmap='coolwarm', aspect='auto')\n",
    "axes[1].set_xticks(range(len(words)))\n",
    "axes[1].set_xticklabels(words, rotation=45)\n",
    "axes[1].set_title('2. Token Embeddings\\n(384 dims each)')\n",
    "axes[1].set_ylabel('Embedding Dimension')\n",
    "\n",
    "# Step 3: Pooled embedding\n",
    "pooled = np.mean(fake_embeddings, axis=0)\n",
    "axes[2].bar(range(len(pooled)), pooled, color='orange')\n",
    "axes[2].set_title('3. Sentence Embedding\\n(384 dims total)')\n",
    "axes[2].set_xlabel('Dimension')\n",
    "axes[2].set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ Each word gets context from ALL other words (attention mechanism)\")\n",
    "print(\"   ‚Ä¢ Pooling creates fixed-size representation regardless of sentence length\")\n",
    "print(\"   ‚Ä¢ Training on millions of sentence pairs teaches semantic similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 4: Modern Text Understanding {#text-embeddings}\n",
    "\n",
    "## The Current State: Production-Ready Embeddings\n",
    "\n",
    "Today's text embeddings (2022+) represent the culmination of this evolution:\n",
    "\n",
    "üöÄ **Industrial Scale**: Handle billions of documents  \n",
    "üéØ **Domain Adaptable**: Fine-tune for specific tasks  \n",
    "üåê **Truly Multilingual**: 100+ languages with cultural understanding  \n",
    "‚ö° **Real-Time**: Millisecond inference speeds  \n",
    "\n",
    "### Yale's Production Entity Resolution System\n",
    "\n",
    "Let's examine how these embeddings solve real problems in Yale's library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Yale's entity resolution pipeline\n",
    "class EntityResolutionPipeline:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.similarity_threshold = 0.75\n",
    "        \n",
    "    def extract_features(self, record1, record2):\n",
    "        \"\"\"Extract multiple similarity features like Yale's pipeline\"\"\"\n",
    "        # Get full record embeddings\n",
    "        emb1 = self.model.encode(record1)\n",
    "        emb2 = self.model.encode(record2)\n",
    "        composite_similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
    "        \n",
    "        # Extract names for name-specific similarity\n",
    "        name1 = record1.split(',')[0] if ',' in record1 else record1.split()[0]\n",
    "        name2 = record2.split(',')[0] if ',' in record2 else record2.split()[0]\n",
    "        name_emb1 = self.model.encode(name1)\n",
    "        name_emb2 = self.model.encode(name2)\n",
    "        name_similarity = cosine_similarity([name_emb1], [name_emb2])[0][0]\n",
    "        \n",
    "        # Simple temporal matching (extract years)\n",
    "        import re\n",
    "        years1 = re.findall(r'\\b(1[789]\\d{2}|20\\d{2})\\b', record1)\n",
    "        years2 = re.findall(r'\\b(1[789]\\d{2}|20\\d{2})\\b', record2)\n",
    "        \n",
    "        temporal_match = 0.0\n",
    "        if years1 and years2:\n",
    "            # Check if years are within reasonable range for same person\n",
    "            year_diff = abs(int(years1[0]) - int(years2[0]))\n",
    "            temporal_match = 1.0 if year_diff <= 50 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'composite_similarity': composite_similarity,\n",
    "            'name_similarity': name_similarity,\n",
    "            'temporal_match': temporal_match,\n",
    "            'years_1': years1,\n",
    "            'years_2': years2\n",
    "        }\n",
    "    \n",
    "    def predict_match(self, record1, record2):\n",
    "        \"\"\"Predict if two records refer to the same entity\"\"\"\n",
    "        features = self.extract_features(record1, record2)\n",
    "        \n",
    "        # Simple decision rule (Yale's actual system uses trained classifier)\n",
    "        if features['temporal_match'] == 0.0 and features['years_1'] and features['years_2']:\n",
    "            # Temporal impossibility - different time periods\n",
    "            return False, 0.0, features\n",
    "        \n",
    "        # Weighted combination of features\n",
    "        score = (\n",
    "            0.4 * features['composite_similarity'] + \n",
    "            0.4 * features['name_similarity'] + \n",
    "            0.2 * features['temporal_match']\n",
    "        )\n",
    "        \n",
    "        return score >= self.similarity_threshold, score, features\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = EntityResolutionPipeline(sentence_model)\n",
    "\n",
    "print(\"üèõÔ∏è Yale University Library Entity Resolution Pipeline\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline with real catalog scenarios\n",
    "test_cases = [\n",
    "    # Same person cases\n",
    "    {\n",
    "        \"record1\": \"Garc√≠a M√°rquez, Gabriel. One Hundred Years of Solitude. 1967. Literature.\",\n",
    "        \"record2\": \"Gabriel Garcia Marquez. Cien a√±os de soledad. 1967. Literature.\",\n",
    "        \"expected\": True,\n",
    "        \"description\": \"Same author, different language/spelling\"\n",
    "    },\n",
    "    {\n",
    "        \"record1\": \"Einstein, Albert. On the Electrodynamics of Moving Bodies. 1905. Physics.\",\n",
    "        \"record2\": \"Einstein, A. Special Theory of Relativity. 1905. Physics.\",\n",
    "        \"expected\": True,\n",
    "        \"description\": \"Same person, abbreviated name\"\n",
    "    },\n",
    "    \n",
    "    # Different person cases\n",
    "    {\n",
    "        \"record1\": \"Schubert, Franz (1797-1828). String Quartet in D minor. 1824. Music.\",\n",
    "        \"record2\": \"Schubert, Franz. Archaeological Photography. 1978. Archaeology.\",\n",
    "        \"expected\": False,\n",
    "        \"description\": \"Different people, same name, different eras\"\n",
    "    },\n",
    "    {\n",
    "        \"record1\": \"Smith, John. Political Treatise. 1745. Political Theory.\",\n",
    "        \"record2\": \"Smith, John. Computer Programming Guide. 1995. Computer Science.\",\n",
    "        \"expected\": False,\n",
    "        \"description\": \"Common name, different time periods\"\n",
    "    },\n",
    "    {\n",
    "        \"record1\": \"Johnson, Mary. Organic Chemistry Textbook. 1985. Chemistry.\",\n",
    "        \"record2\": \"Johnson, Mary. Medieval Poetry Collection. 1985. Literature.\",\n",
    "        \"expected\": False,\n",
    "        \"description\": \"Same name/year, incompatible domains\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test each case\n",
    "results = []\n",
    "for i, case in enumerate(test_cases):\n",
    "    is_match, score, features = pipeline.predict_match(case['record1'], case['record2'])\n",
    "    \n",
    "    correct = is_match == case['expected']\n",
    "    results.append({\n",
    "        'case': i + 1,\n",
    "        'description': case['description'],\n",
    "        'predicted': is_match,\n",
    "        'expected': case['expected'],\n",
    "        'correct': correct,\n",
    "        'score': score,\n",
    "        'features': features\n",
    "    })\n",
    "    \n",
    "    status = \"‚úÖ\" if correct else \"‚ùå\"\n",
    "    print(f\"\\n{status} Case {i+1}: {case['description']}\")\n",
    "    print(f\"   Record 1: {case['record1'][:60]}...\")\n",
    "    print(f\"   Record 2: {case['record2'][:60]}...\")\n",
    "    print(f\"   Prediction: {'MATCH' if is_match else 'NO MATCH'} (score: {score:.3f})\")\n",
    "    print(f\"   Expected: {'MATCH' if case['expected'] else 'NO MATCH'}\")\n",
    "    print(f\"   Features:\")\n",
    "    print(f\"     ‚Ä¢ Composite similarity: {features['composite_similarity']:.3f}\")\n",
    "    print(f\"     ‚Ä¢ Name similarity: {features['name_similarity']:.3f}\")\n",
    "    print(f\"     ‚Ä¢ Temporal match: {features['temporal_match']:.1f}\")\n",
    "    if features['years_1'] or features['years_2']:\n",
    "        print(f\"     ‚Ä¢ Years found: {features['years_1']} vs {features['years_2']}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(1 for r in results if r['correct'])\n",
    "accuracy = correct_predictions / len(results)\n",
    "\n",
    "print(f\"\\nüìä Pipeline Performance:\")\n",
    "print(f\"   Accuracy: {accuracy:.1%} ({correct_predictions}/{len(results)} correct)\")\n",
    "print(f\"   Threshold: {pipeline.similarity_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Feature Space\n",
    "\n",
    "Let's visualize how the embedding space separates matching from non-matching pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature visualization\n",
    "import plotly.express as px\n",
    "\n",
    "# Prepare data for visualization\n",
    "viz_data = []\n",
    "for result in results:\n",
    "    viz_data.append({\n",
    "        'Case': f\"Case {result['case']}\",\n",
    "        'Composite Similarity': result['features']['composite_similarity'],\n",
    "        'Name Similarity': result['features']['name_similarity'],\n",
    "        'Temporal Match': result['features']['temporal_match'],\n",
    "        'Prediction': 'Match' if result['predicted'] else 'No Match',\n",
    "        'Correct': 'Correct' if result['correct'] else 'Wrong',\n",
    "        'Score': result['score'],\n",
    "        'Description': result['description']\n",
    "    })\n",
    "\n",
    "df_viz = pd.DataFrame(viz_data)\n",
    "\n",
    "# Create 3D scatter plot of the feature space\n",
    "fig = px.scatter_3d(\n",
    "    df_viz,\n",
    "    x='Composite Similarity',\n",
    "    y='Name Similarity', \n",
    "    z='Temporal Match',\n",
    "    color='Prediction',\n",
    "    symbol='Correct',\n",
    "    size='Score',\n",
    "    hover_data=['Description'],\n",
    "    title='Entity Resolution Feature Space<br>3D Visualization of Matching Decision'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='Composite Similarity',\n",
    "        yaxis_title='Name Similarity',\n",
    "        zaxis_title='Temporal Match'\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Also create 2D projections\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Composite vs Name similarity\n",
    "for prediction in ['Match', 'No Match']:\n",
    "    mask = df_viz['Prediction'] == prediction\n",
    "    axes[0].scatter(\n",
    "        df_viz[mask]['Composite Similarity'],\n",
    "        df_viz[mask]['Name Similarity'],\n",
    "        label=prediction,\n",
    "        s=100,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "axes[0].set_xlabel('Composite Similarity')\n",
    "axes[0].set_ylabel('Name Similarity')\n",
    "axes[0].set_title('Semantic Features')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Score distribution\n",
    "match_scores = df_viz[df_viz['Prediction'] == 'Match']['Score']\n",
    "no_match_scores = df_viz[df_viz['Prediction'] == 'No Match']['Score']\n",
    "\n",
    "axes[1].hist(match_scores, alpha=0.7, label='Match', bins=5, color='green')\n",
    "axes[1].hist(no_match_scores, alpha=0.7, label='No Match', bins=5, color='red')\n",
    "axes[1].axvline(pipeline.similarity_threshold, color='black', linestyle='--', label=f'Threshold ({pipeline.similarity_threshold})')\n",
    "axes[1].set_xlabel('Combined Score')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Score Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance (simplified)\n",
    "feature_names = ['Composite\\nSimilarity', 'Name\\nSimilarity', 'Temporal\\nMatch']\n",
    "feature_weights = [0.4, 0.4, 0.2]\n",
    "\n",
    "bars = axes[2].bar(feature_names, feature_weights, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "axes[2].set_ylabel('Weight in Decision')\n",
    "axes[2].set_title('Feature Importance')\n",
    "axes[2].set_ylim(0, 0.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, weight in zip(bars, feature_weights):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{weight:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ Key Insights from Feature Space:\")\n",
    "print(\"   ‚Ä¢ Semantic similarity alone isn't enough (Garc√≠a M√°rquez vs Einstein)\")\n",
    "print(\"   ‚Ä¢ Temporal information provides crucial disambiguation (Schubert case)\")\n",
    "print(\"   ‚Ä¢ Multi-feature approach handles edge cases and ambiguity\")\n",
    "print(\"   ‚Ä¢ Embeddings capture domain knowledge crucial for entity resolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 5: Real-World Application - Entity Resolution {#entity-resolution}\n",
    "\n",
    "## Production Scale: Yale's Complete System\n",
    "\n",
    "Yale's actual system processes **17.6 million catalog records** with incredible accuracy:\n",
    "\n",
    "üìä **99.55% Precision** - Only 45 false matches out of 10,000 predictions  \n",
    "üéØ **82.48% Recall** - Catches most true matches  \n",
    "‚ö° **99.23% Efficiency** - Reduces comparisons from 155 billion to 84,000  \n",
    "\n",
    "### The Production Architecture\n",
    "\n",
    "Let's simulate the key components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Yale's production architecture\n",
    "class ProductionEntityResolver:\n",
    "    def __init__(self, sentence_model):\n",
    "        self.sentence_model = sentence_model\n",
    "        self.vector_store = {}  # Simulated Weaviate\n",
    "        self.performance_stats = {\n",
    "            'total_records': 0,\n",
    "            'comparisons_made': 0,\n",
    "            'matches_found': 0,\n",
    "            'processing_time': 0\n",
    "        }\n",
    "    \n",
    "    def index_record(self, record_id, record_text):\n",
    "        \"\"\"Add a record to the vector index\"\"\"\n",
    "        embedding = self.sentence_model.encode(record_text)\n",
    "        self.vector_store[record_id] = {\n",
    "            'text': record_text,\n",
    "            'embedding': embedding\n",
    "        }\n",
    "        self.performance_stats['total_records'] += 1\n",
    "    \n",
    "    def find_similar_records(self, query_record_id, top_k=5, similarity_threshold=0.7):\n",
    "        \"\"\"Find records similar to the query record\"\"\"\n",
    "        if query_record_id not in self.vector_store:\n",
    "            return []\n",
    "        \n",
    "        query_embedding = self.vector_store[query_record_id]['embedding']\n",
    "        similarities = []\n",
    "        \n",
    "        for record_id, record_data in self.vector_store.items():\n",
    "            if record_id != query_record_id:\n",
    "                similarity = cosine_similarity(\n",
    "                    [query_embedding], \n",
    "                    [record_data['embedding']]\n",
    "                )[0][0]\n",
    "                \n",
    "                if similarity >= similarity_threshold:\n",
    "                    similarities.append((record_id, similarity))\n",
    "                \n",
    "                self.performance_stats['comparisons_made'] += 1\n",
    "        \n",
    "        # Sort by similarity and return top k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def resolve_entity_cluster(self, seed_record_id):\n",
    "        \"\"\"Find all records that refer to the same entity\"\"\"\n",
    "        cluster = {seed_record_id}\n",
    "        to_process = [seed_record_id]\n",
    "        \n",
    "        while to_process:\n",
    "            current_id = to_process.pop(0)\n",
    "            similar_records = self.find_similar_records(current_id, top_k=10)\n",
    "            \n",
    "            for similar_id, similarity in similar_records:\n",
    "                if similar_id not in cluster:\n",
    "                    cluster.add(similar_id)\n",
    "                    to_process.append(similar_id)\n",
    "                    self.performance_stats['matches_found'] += 1\n",
    "        \n",
    "        return cluster\n",
    "\n",
    "# Initialize the production resolver\n",
    "prod_resolver = ProductionEntityResolver(sentence_model)\n",
    "\n",
    "print(\"üè≠ Production Entity Resolution System\")\n",
    "print(\"=\"*45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a realistic library catalog\n",
    "import time\n",
    "\n",
    "# Sample catalog records (simulating real Yale data)\n",
    "catalog_records = {\n",
    "    'record_001': 'Schubert, Franz (1797-1828). String Quartet No. 14 in D minor \"Death and the Maiden\". Chamber music, Classical period',\n",
    "    'record_002': 'Franz Schubert. Quartet in D minor, D810. String quartet, Romantic era, Austrian composer',\n",
    "    'record_003': 'Schubert, F. Death and the Maiden quartet. Classical music, Vienna, 19th century',\n",
    "    'record_004': 'Schubert, Franz. Archaeological photography techniques. Mainz: von Zabern, 1978. Archaeology methodology',\n",
    "    'record_005': 'Garc√≠a M√°rquez, Gabriel. One Hundred Years of Solitude. Nobel Prize literature, magical realism',\n",
    "    'record_006': 'Gabriel Garcia Marquez. Cien a√±os de soledad. Literatura latinoamericana, realismo m√°gico',\n",
    "    'record_007': 'Garc√≠a M√°rquez, G. Love in the Time of Cholera. Colombian literature, 20th century',\n",
    "    'record_008': 'Einstein, Albert. On the Electrodynamics of Moving Bodies. Annalen der Physik, 1905',\n",
    "    'record_009': 'A. Einstein. Special Theory of Relativity. Physics, theoretical physics, space-time',\n",
    "    'record_010': 'Einstein, Albert. General Theory of Relativity. Physics Nobel Prize, Princeton University',\n",
    "    'record_011': 'Mozart, Wolfgang Amadeus. Symphony No. 40 in G minor. Classical music, Austrian composer',\n",
    "    'record_012': 'W.A. Mozart. Piano Sonata No. 11 in A major. Classical period, keyboard music',\n",
    "    'record_013': 'Bach, Johann Sebastian. The Well-Tempered Clavier. Baroque music, keyboard compositions',\n",
    "    'record_014': 'J.S. Bach. Brandenburg Concertos. Baroque concertos, German composer',\n",
    "    'record_015': 'Beethoven, Ludwig van. Symphony No. 9 in D minor \"Choral\". Romantic music, German composer'\n",
    "}\n",
    "\n",
    "print(f\"üìö Indexing {len(catalog_records)} catalog records...\")\n",
    "\n",
    "# Index all records\n",
    "start_time = time.time()\n",
    "for record_id, record_text in catalog_records.items():\n",
    "    prod_resolver.index_record(record_id, record_text)\n",
    "\n",
    "indexing_time = time.time() - start_time\n",
    "print(f\"‚úÖ Indexing completed in {indexing_time:.3f} seconds\")\n",
    "\n",
    "# Resolve entity clusters\n",
    "print(\"\\nüîç Resolving entity clusters...\")\n",
    "\n",
    "processed_records = set()\n",
    "entity_clusters = []\n",
    "\n",
    "resolution_start = time.time()\n",
    "for record_id in catalog_records.keys():\n",
    "    if record_id not in processed_records:\n",
    "        cluster = prod_resolver.resolve_entity_cluster(record_id)\n",
    "        if len(cluster) > 1:  # Only keep clusters with multiple records\n",
    "            entity_clusters.append(cluster)\n",
    "        processed_records.update(cluster)\n",
    "\n",
    "resolution_time = time.time() - resolution_start\n",
    "\n",
    "print(f\"‚úÖ Resolution completed in {resolution_time:.3f} seconds\")\n",
    "print(f\"\\nüìä Results Summary:\")\n",
    "print(f\"   ‚Ä¢ Total records processed: {prod_resolver.performance_stats['total_records']}\")\n",
    "print(f\"   ‚Ä¢ Pairwise comparisons: {prod_resolver.performance_stats['comparisons_made']}\")\n",
    "print(f\"   ‚Ä¢ Entity clusters found: {len(entity_clusters)}\")\n",
    "print(f\"   ‚Ä¢ Matches identified: {prod_resolver.performance_stats['matches_found']}\")\n",
    "\n",
    "# Display the entity clusters\n",
    "print(\"\\nüéØ Discovered Entity Clusters:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, cluster in enumerate(entity_clusters, 1):\n",
    "    print(f\"\\nCluster {i} ({len(cluster)} records):\")\n",
    "    \n",
    "    # Get the person/entity name from first record\n",
    "    first_record = catalog_records[list(cluster)[0]]\n",
    "    entity_name = first_record.split(',')[0] if ',' in first_record else first_record.split('.')[0]\n",
    "    print(f\"Entity: {entity_name}\")\n",
    "    \n",
    "    for record_id in sorted(cluster):\n",
    "        record_text = catalog_records[record_id]\n",
    "        print(f\"   {record_id}: {record_text[:80]}...\")\n",
    "    \n",
    "    # Calculate cluster coherence\n",
    "    embeddings = [prod_resolver.vector_store[rid]['embedding'] for rid in cluster]\n",
    "    if len(embeddings) > 1:\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        avg_similarity = np.mean(similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)])\n",
    "        print(f\"   Cluster coherence: {avg_similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis and Optimization\n",
    "\n",
    "Let's analyze the computational efficiency gains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze computational complexity\n",
    "n_records = len(catalog_records)\n",
    "naive_comparisons = n_records * (n_records - 1) // 2\n",
    "actual_comparisons = prod_resolver.performance_stats['comparisons_made']\n",
    "reduction_ratio = actual_comparisons / naive_comparisons\n",
    "\n",
    "print(\"‚ö° Computational Efficiency Analysis:\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Dataset size: {n_records} records\")\n",
    "print(f\"Naive O(n¬≤) comparisons: {naive_comparisons:,}\")\n",
    "print(f\"Vector-optimized comparisons: {actual_comparisons:,}\")\n",
    "print(f\"Reduction ratio: {reduction_ratio:.1%}\")\n",
    "print(f\"Efficiency gain: {(1 - reduction_ratio):.1%}\")\n",
    "\n",
    "# Extrapolate to Yale's scale\n",
    "yale_records = 17_590_104\n",
    "yale_naive = yale_records * (yale_records - 1) // 2\n",
    "yale_optimized = int(yale_naive * reduction_ratio)\n",
    "\n",
    "print(f\"\\nüèõÔ∏è Scaled to Yale University Library:\")\n",
    "print(f\"Total catalog records: {yale_records:,}\")\n",
    "print(f\"Naive comparisons: {yale_naive:,} ({yale_naive/1e12:.1f} trillion)\")\n",
    "print(f\"Optimized comparisons: {yale_optimized:,} ({yale_optimized/1e6:.1f} million)\")\n",
    "print(f\"Processing time savings: {(yale_naive - yale_optimized) / yale_naive:.1%}\")\n",
    "\n",
    "# Visualize the efficiency gains\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Comparison counts\n",
    "methods = ['Naive O(n¬≤)', 'Vector Optimized']\n",
    "comparisons = [naive_comparisons, actual_comparisons]\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "\n",
    "bars = ax1.bar(methods, comparisons, color=colors)\n",
    "ax1.set_ylabel('Number of Comparisons')\n",
    "ax1.set_title('Computational Complexity Comparison')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, comparisons):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1, \n",
    "             f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Processing time scaling\n",
    "record_sizes = [100, 1000, 10000, 100000, 1000000]\n",
    "naive_times = [(n * (n-1) // 2) / 1000 for n in record_sizes]  # Simulated time units\n",
    "optimized_times = [t * reduction_ratio for t in naive_times]\n",
    "\n",
    "ax2.plot(record_sizes, naive_times, 'o-', label='Naive O(n¬≤)', color='red', linewidth=2)\n",
    "ax2.plot(record_sizes, optimized_times, 'o-', label='Vector Optimized', color='green', linewidth=2)\n",
    "ax2.set_xlabel('Number of Records')\n",
    "ax2.set_ylabel('Relative Processing Time')\n",
    "ax2.set_title('Scalability Comparison')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Optimization Insights:\")\n",
    "print(\"   ‚Ä¢ Vector similarity enables intelligent candidate selection\")\n",
    "print(\"   ‚Ä¢ Semantic clustering reduces irrelevant comparisons\")\n",
    "print(\"   ‚Ä¢ Embeddings make O(n¬≤) problems tractable at scale\")\n",
    "print(\"   ‚Ä¢ Real-world efficiency enables production deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 6: Cross-Language Capabilities {#multilingual}\n",
    "\n",
    "## Breaking Language Barriers in Research\n",
    "\n",
    "One of the most powerful aspects of modern embeddings is their ability to understand meaning across languages. This opens up entirely new research possibilities.\n",
    "\n",
    "### Setting Up Weaviate for Production Search\n",
    "\n",
    "Let's set up a production-grade vector database for cross-language search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector database setup for multilingual search\n",
    "try:\n",
    "    import weaviate\n",
    "    from weaviate.classes.config import Configure, Property, DataType, VectorDistances\n",
    "    from weaviate.util import generate_uuid5\n",
    "    \n",
    "    # Connect to Weaviate (start with: docker run -p 8080:8080 cr.weaviate.io/semitechnologies/weaviate:1.25.1)\n",
    "    client = weaviate.connect_to_local(host=\"localhost\", port=8080)\n",
    "    \n",
    "    if client.is_ready():\n",
    "        print(\"‚úÖ Connected to Weaviate vector database\")\n",
    "        \n",
    "        # Create multilingual document collection\n",
    "        try:\n",
    "            client.collections.delete(\"MultilingualCatalog\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        collection = client.collections.create(\n",
    "            name=\"MultilingualCatalog\",\n",
    "            description=\"Multilingual library catalog for semantic search\",\n",
    "            vector_index_config=Configure.VectorIndex.hnsw(\n",
    "                distance_metric=VectorDistances.COSINE\n",
    "            ),\n",
    "            properties=[\n",
    "                Property(name=\"text\", data_type=DataType.TEXT),\n",
    "                Property(name=\"language\", data_type=DataType.TEXT),\n",
    "                Property(name=\"author\", data_type=DataType.TEXT),\n",
    "                Property(name=\"domain\", data_type=DataType.TEXT),\n",
    "                Property(name=\"year\", data_type=DataType.INT)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Created multilingual catalog collection\")\n",
    "        weaviate_available = True\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Weaviate not ready\")\n",
    "        weaviate_available = False\n",
    "        \nexcept Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Weaviate connection failed: {e}\")\n",
    "    print(\"üí° To use vector search, start Weaviate with:\")\n",
    "    print(\"   docker run -p 8080:8080 cr.weaviate.io/semitechnologies/weaviate:1.25.1\")\n",
    "    weaviate_available = False\n",
    "    client = None\n",
    "    collection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multilingual research corpus\n",
    "multilingual_corpus = [\n",
    "    # English sources\n",
    "    {\n",
    "        \"text\": \"Franz Schubert composed his String Quartet No. 14 in D minor, known as 'Death and the Maiden', in 1824 during the late Classical period.\",\n",
    "        \"language\": \"en\",\n",
    "        \"author\": \"Franz Schubert\",\n",
    "        \"domain\": \"music\",\n",
    "        \"year\": 1824\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Gabriel Garc√≠a M√°rquez's masterpiece 'One Hundred Years of Solitude' revolutionized Latin American literature through magical realism.\",\n",
    "        \"language\": \"en\", \n",
    "        \"author\": \"Gabriel Garc√≠a M√°rquez\",\n",
    "        \"domain\": \"literature\",\n",
    "        \"year\": 1967\n",
    "    },\n",
    "    \n",
    "    # Spanish sources\n",
    "    {\n",
    "        \"text\": \"Gabriel Garc√≠a M√°rquez escribi√≥ 'Cien a√±os de soledad' en 1967, una obra maestra del realismo m√°gico que narra la historia de la familia Buend√≠a.\",\n",
    "        \"language\": \"es\",\n",
    "        \"author\": \"Gabriel Garc√≠a M√°rquez\", \n",
    "        \"domain\": \"literatura\",\n",
    "        \"year\": 1967\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"La m√∫sica cl√°sica de Franz Schubert incluye sinfon√≠as, cuartetos de cuerda y lieder que expresan profunda emotividad rom√°ntica.\",\n",
    "        \"language\": \"es\",\n",
    "        \"author\": \"Franz Schubert\",\n",
    "        \"domain\": \"m√∫sica\", \n",
    "        \"year\": 1820\n",
    "    },\n",
    "    \n",
    "    # French sources\n",
    "    {\n",
    "        \"text\": \"Franz Schubert, compositeur autrichien de la p√©riode romantique, a cr√©√© plus de 600 lieder qui r√©volutionnent l'art vocal.\",\n",
    "        \"language\": \"fr\",\n",
    "        \"author\": \"Franz Schubert\",\n",
    "        \"domain\": \"musique\",\n",
    "        \"year\": 1825\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Gabriel Garc√≠a M√°rquez remporte le Prix Nobel de litt√©rature en 1982 pour son ≈ìuvre qui m√©lange r√©alit√© et fantaisie.\",\n",
    "        \"language\": \"fr\",\n",
    "        \"author\": \"Gabriel Garc√≠a M√°rquez\",\n",
    "        \"domain\": \"litt√©rature\", \n",
    "        \"year\": 1982\n",
    "    },\n",
    "    \n",
    "    # German sources\n",
    "    {\n",
    "        \"text\": \"Franz Schubert komponierte seine ber√ºhmte Sinfonie Nr. 8 in h-Moll, die 'Unvollendete', die als Meisterwerk der Romantik gilt.\",\n",
    "        \"language\": \"de\",\n",
    "        \"author\": \"Franz Schubert\",\n",
    "        \"domain\": \"Musik\",\n",
    "        \"year\": 1822\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Die arch√§ologische Fotografie entwickelte sich im 19. Jahrhundert als wichtiges Dokumentationswerkzeug f√ºr historische St√§tten.\",\n",
    "        \"language\": \"de\",\n",
    "        \"author\": \"Franz Schubert (Archaeologist)\",\n",
    "        \"domain\": \"Arch√§ologie\",\n",
    "        \"year\": 1978\n",
    "    },\n",
    "    \n",
    "    # Portuguese sources\n",
    "    {\n",
    "        \"text\": \"Gabriel Garc√≠a M√°rquez influenciou profundamente a literatura latino-americana com seu realismo m√°gico e narrativas envolventes.\",\n",
    "        \"language\": \"pt\",\n",
    "        \"author\": \"Gabriel Garc√≠a M√°rquez\",\n",
    "        \"domain\": \"literatura\",\n",
    "        \"year\": 1970\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìö Created multilingual corpus with {len(multilingual_corpus)} documents\")\n",
    "print(f\"üåç Languages: {set(doc['language'] for doc in multilingual_corpus)}\")\n",
    "print(f\"üë• Authors: {set(doc['author'] for doc in multilingual_corpus)}\")\n",
    "\n",
    "# Index documents in Weaviate if available\n",
    "if weaviate_available and collection:\n",
    "    print(\"\\nüìù Indexing multilingual documents...\")\n",
    "    \n",
    "    with collection.batch.fixed_size(batch_size=100) as batch:\n",
    "        for i, doc in enumerate(multilingual_corpus):\n",
    "            # Generate embedding\n",
    "            embedding = sentence_model.encode(doc['text'])\n",
    "            \n",
    "            # Create unique ID\n",
    "            uuid = generate_uuid5(doc['text'])\n",
    "            \n",
    "            # Add to batch\n",
    "            batch.add_object(\n",
    "                properties=doc,\n",
    "                uuid=uuid,\n",
    "                vector=embedding.tolist()\n",
    "            )\n",
    "            \n",
    "            print(f\"   üìÑ Indexed: {doc['language']} - {doc['text'][:50]}...\")\n",
    "    \n",
    "    print(\"‚úÖ All documents indexed successfully\")\nelse:\n",
    "    print(\"‚ö†Ô∏è Skipping Weaviate indexing (not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Language Semantic Search\n",
    "\n",
    "Now let's demonstrate the power of cross-language understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-language search function\n",
    "def cross_language_search(query, language=\"auto\", limit=5):\n",
    "    \"\"\"Search across all languages for semantically similar content\"\"\"\n",
    "    \n",
    "    if weaviate_available and collection:\n",
    "        # Use Weaviate for production search\n",
    "        query_embedding = sentence_model.encode(query)\n",
    "        \n",
    "        try:\n",
    "            response = collection.query.near_vector(\n",
    "                near_vector=query_embedding.tolist(),\n",
    "                limit=limit,\n",
    "                return_metadata=['distance']\n",
    "            )\n",
    "            \n",
    "            results = []\n",
    "            for obj in response.objects:\n",
    "                similarity = 1 - obj.metadata.distance\n",
    "                results.append({\n",
    "                    'text': obj.properties['text'],\n",
    "                    'language': obj.properties['language'],\n",
    "                    'author': obj.properties['author'],\n",
    "                    'domain': obj.properties['domain'],\n",
    "                    'year': obj.properties['year'],\n",
    "                    'similarity': similarity\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Weaviate search error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    else:\n",
    "        # Fallback to local similarity search\n",
    "        query_embedding = sentence_model.encode(query)\n",
    "        results = []\n",
    "        \n",
    "        for doc in multilingual_corpus:\n",
    "            doc_embedding = sentence_model.encode(doc['text'])\n",
    "            similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
    "            \n",
    "            results.append({\n",
    "                'text': doc['text'],\n",
    "                'language': doc['language'],\n",
    "                'author': doc['author'], \n",
    "                'domain': doc['domain'],\n",
    "                'year': doc['year'],\n",
    "                'similarity': similarity\n",
    "            })\n",
    "        \n",
    "        # Sort by similarity and return top results\n",
    "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return results[:limit]\n",
    "\n",
    "# Test cross-language searches\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"classical music composition string quartet\",\n",
    "        \"language\": \"en\",\n",
    "        \"description\": \"English query about music\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"literatura realismo m√°gico\",\n",
    "        \"language\": \"es\", \n",
    "        \"description\": \"Spanish query about literature\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"musique romantique compositeur\",\n",
    "        \"language\": \"fr\",\n",
    "        \"description\": \"French query about romantic music\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"arqueologia fotografia m√©todos\",\n",
    "        \"language\": \"pt\",\n",
    "        \"description\": \"Portuguese query about archaeological methods\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üîç Cross-Language Semantic Search Results:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for test in test_queries:\n",
    "    print(f\"\\nüåç Query: '{test['query']}' ({test['description']})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = cross_language_search(test['query'], test['language'], limit=3)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        flag_emoji = {\n",
    "            'en': 'üá∫üá∏',\n",
    "            'es': 'üá™üá∏', \n",
    "            'fr': 'üá´üá∑',\n",
    "            'de': 'üá©üá™',\n",
    "            'pt': 'üáµüáπ'\n",
    "        }\n",
    "        \n",
    "        flag = flag_emoji.get(result['language'], 'üåê')\n",
    "        \n",
    "        print(f\"   {i}. {flag} [{result['language'].upper()}] Similarity: {result['similarity']:.3f}\")\n",
    "        print(f\"      Author: {result['author']} | Domain: {result['domain']} | Year: {result['year']}\")\n",
    "        print(f\"      Text: {result['text'][:80]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual Analysis and Insights\n",
    "\n",
    "Let's analyze the cross-language relationships in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cross-language relationships\n",
    "print(\"üìä Multilingual Corpus Analysis:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Get embeddings for all documents\n",
    "corpus_texts = [doc['text'] for doc in multilingual_corpus]\n",
    "corpus_embeddings = sentence_model.encode(corpus_texts)\n",
    "\n",
    "# Calculate full similarity matrix\n",
    "similarity_matrix = cosine_similarity(corpus_embeddings)\n",
    "\n",
    "# Create labels for visualization\n",
    "labels = []\n",
    "for i, doc in enumerate(multilingual_corpus):\n",
    "    author = doc['author'].split(',')[0]  # First name only\n",
    "    if \"Archaeologist\" in doc['author']:\n",
    "        author = \"F.Schubert(Arch)\"\n",
    "    elif \"Franz Schubert\" in doc['author']:\n",
    "        author = \"F.Schubert(Music)\"\n",
    "    elif \"Garc√≠a\" in doc['author']:\n",
    "        author = \"Garc√≠a M√°rquez\"\n",
    "    \n",
    "    labels.append(f\"{author}\\n({doc['language'].upper()})\")\n",
    "\n",
    "# Create comprehensive heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    similarity_matrix,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    xticklabels=labels,\n",
    "    yticklabels=labels,\n",
    "    cmap='RdYlBu_r',\n",
    "    center=0.5,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "\n",
    "plt.title('Cross-Language Semantic Similarity Matrix\\nMultilingual Entity Resolution', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Documents (Author + Language)', fontweight='bold')\n",
    "plt.ylabel('Documents (Author + Language)', fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze cross-language author similarities\n",
    "print(\"\\nüéØ Cross-Language Author Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Find Garc√≠a M√°rquez documents\n",
    "garcia_indices = [i for i, doc in enumerate(multilingual_corpus) if 'Garc√≠a M√°rquez' in doc['author']]\n",
    "schubert_music_indices = [i for i, doc in enumerate(multilingual_corpus) \n",
    "                         if 'Franz Schubert' in doc['author'] and 'Archaeologist' not in doc['author']]\n",
    "schubert_arch_indices = [i for i, doc in enumerate(multilingual_corpus) \n",
    "                        if 'Archaeologist' in doc['author']]\n",
    "\n",
    "# Calculate average similarities within and across authors\n",
    "def calculate_group_similarity(indices, matrix):\n",
    "    if len(indices) < 2:\n",
    "        return 0.0\n",
    "    similarities = []\n",
    "    for i in range(len(indices)):\n",
    "        for j in range(i+1, len(indices)):\n",
    "            similarities.append(matrix[indices[i]][indices[j]])\n",
    "    return np.mean(similarities)\n",
    "\n",
    "garcia_similarity = calculate_group_similarity(garcia_indices, similarity_matrix)\n",
    "schubert_music_similarity = calculate_group_similarity(schubert_music_indices, similarity_matrix)\n",
    "\n",
    "# Cross-author similarity\n",
    "cross_similarities = []\n",
    "for i in garcia_indices:\n",
    "    for j in schubert_music_indices:\n",
    "        cross_similarities.append(similarity_matrix[i][j])\n",
    "avg_cross_similarity = np.mean(cross_similarities)\n",
    "\n",
    "# Archaeological Schubert vs Music Schubert\n",
    "arch_music_similarities = []\n",
    "for i in schubert_arch_indices:\n",
    "    for j in schubert_music_indices:\n",
    "        arch_music_similarities.append(similarity_matrix[i][j])\n",
    "avg_arch_music_similarity = np.mean(arch_music_similarities) if arch_music_similarities else 0.0\n",
    "\n",
    "print(f\"‚úÖ Garc√≠a M√°rquez (same author, different languages): {garcia_similarity:.3f}\")\n",
    "print(f\"‚úÖ Franz Schubert Music (same author, different languages): {schubert_music_similarity:.3f}\")\n",
    "print(f\"‚ùå Garc√≠a M√°rquez ‚Üî Franz Schubert (different authors): {avg_cross_similarity:.3f}\")\n",
    "print(f\"‚ùå Franz Schubert Music ‚Üî Archaeology (different people): {avg_arch_music_similarity:.3f}\")\n",
    "\n",
    "# Language distribution analysis\n",
    "language_counts = {}\n",
    "for doc in multilingual_corpus:\n",
    "    lang = doc['language']\n",
    "    if lang not in language_counts:\n",
    "        language_counts[lang] = 0\n",
    "    language_counts[lang] += 1\n",
    "\n",
    "print(f\"\\nüìà Language Distribution:\")\n",
    "for lang, count in sorted(language_counts.items()):\n",
    "    percentage = (count / len(multilingual_corpus)) * 100\n",
    "    print(f\"   {lang.upper()}: {count} documents ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüåü Key Insights:\")\n",
    "print(f\"   ‚Ä¢ Same authors cluster together across languages\")\n",
    "print(f\"   ‚Ä¢ Content similarity transcends language barriers\")\n",
    "print(f\"   ‚Ä¢ Entity disambiguation works in multilingual contexts\")\n",
    "print(f\"   ‚Ä¢ Enables truly global research and discovery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 7: Practical Implementation {#implementation}\n",
    "\n",
    "## From Tutorial to Production\n",
    "\n",
    "Now that you understand the evolution and capabilities of text embeddings, let's explore how to implement these technologies in your own research projects.\n",
    "\n",
    "### Cost-Benefit Analysis for Research Projects\n",
    "\n",
    "Let's calculate the real costs and benefits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost analysis for different embedding approaches\n",
    "def calculate_embedding_costs(num_documents, avg_tokens_per_doc=100):\n",
    "    \"\"\"Calculate costs for different embedding services\"\"\"\n",
    "    \n",
    "    costs = {\n",
    "        \"OpenAI text-embedding-3-small\": {\n",
    "            \"price_per_1k_tokens\": 0.00002,  # $0.02 per 1M tokens\n",
    "            \"dimensions\": 1536,\n",
    "            \"total_tokens\": num_documents * avg_tokens_per_doc,\n",
    "            \"total_cost\": (num_documents * avg_tokens_per_doc / 1000) * 0.00002\n",
    "        },\n",
    "        \"OpenAI text-embedding-3-large\": {\n",
    "            \"price_per_1k_tokens\": 0.00013,  # $0.13 per 1M tokens\n",
    "            \"dimensions\": 3072,\n",
    "            \"total_tokens\": num_documents * avg_tokens_per_doc,\n",
    "            \"total_cost\": (num_documents * avg_tokens_per_doc / 1000) * 0.00013\n",
    "        },\n",
    "        \"Local Sentence Transformer\": {\n",
    "            \"price_per_1k_tokens\": 0.0,  # Free after setup\n",
    "            \"dimensions\": 384,\n",
    "            \"total_tokens\": num_documents * avg_tokens_per_doc,\n",
    "            \"total_cost\": 0.0,\n",
    "            \"setup_cost\": 50  # Approximate GPU compute cost\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return costs\n",
    "\n",
    "# Analyze costs for different research scales\n",
    "research_scales = {\n",
    "    \"Small Project\": 1_000,\n",
    "    \"Medium Project\": 10_000, \n",
    "    \"Large Project\": 100_000,\n",
    "    \"Yale Scale\": 17_590_104\n",
    "}\n",
    "\n",
    "print(\"üí∞ Embedding Cost Analysis for Research Projects:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "cost_comparison = []\n",
    "\n",
    "for scale_name, num_docs in research_scales.items():\n",
    "    costs = calculate_embedding_costs(num_docs)\n",
    "    \n",
    "    print(f\"\\nüìä {scale_name} ({num_docs:,} documents):\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    for service, details in costs.items():\n",
    "        total_cost = details['total_cost']\n",
    "        if 'setup_cost' in details:\n",
    "            total_cost += details['setup_cost']\n",
    "        \n",
    "        print(f\"   {service:30} ${total_cost:.2f}\")\n",
    "        \n",
    "        cost_comparison.append({\n",
    "            'Scale': scale_name,\n",
    "            'Service': service,\n",
    "            'Cost': total_cost,\n",
    "            'Documents': num_docs\n",
    "        })\n",
    "\n",
    "# Visualize cost comparison\n",
    "df_costs = pd.DataFrame(cost_comparison)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Cost by scale\n",
    "for service in df_costs['Service'].unique():\n",
    "    service_data = df_costs[df_costs['Service'] == service]\n",
    "    ax1.plot(service_data['Documents'], service_data['Cost'], \n",
    "            'o-', label=service, linewidth=2, markersize=8)\n",
    "\n",
    "ax1.set_xlabel('Number of Documents')\n",
    "ax1.set_ylabel('Total Cost ($)')\n",
    "ax1.set_title('Embedding Costs by Project Scale')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cost per document\n",
    "df_costs['Cost_Per_Doc'] = df_costs['Cost'] / df_costs['Documents']\n",
    "\n",
    "pivot_data = df_costs.pivot(index='Scale', columns='Service', values='Cost_Per_Doc')\n",
    "pivot_data.plot(kind='bar', ax=ax2, width=0.8)\n",
    "ax2.set_ylabel('Cost Per Document ($)')\n",
    "ax2.set_title('Cost Per Document by Service')\n",
    "ax2.set_yscale('log')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.legend(title='Service', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Cost Analysis Insights:\")\n",
    "print(\"   ‚Ä¢ Local models: high setup, zero marginal cost\")\n",
    "print(\"   ‚Ä¢ API services: zero setup, linear scaling\")\n",
    "print(\"   ‚Ä¢ Break-even point depends on project scale\")\n",
    "print(\"   ‚Ä¢ Consider computational requirements and expertise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Decision Framework\n",
    "\n",
    "Here's a practical guide for choosing the right approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision framework for embedding implementation\n",
    "class EmbeddingDecisionFramework:\n",
    "    def __init__(self):\n",
    "        self.criteria = {\n",
    "            'dataset_size': {\n",
    "                'small': (0, 10_000),\n",
    "                'medium': (10_000, 100_000), \n",
    "                'large': (100_000, 1_000_000),\n",
    "                'massive': (1_000_000, float('inf'))\n",
    "            },\n",
    "            'budget': {\n",
    "                'minimal': (0, 100),\n",
    "                'moderate': (100, 1_000),\n",
    "                'substantial': (1_000, 10_000),\n",
    "                'enterprise': (10_000, float('inf'))\n",
    "            },\n",
    "            'technical_expertise': {\n",
    "                'beginner': 1,\n",
    "                'intermediate': 2,\n",
    "                'advanced': 3,\n",
    "                'expert': 4\n",
    "            },\n",
    "            'time_constraints': {\n",
    "                'immediate': 1,\n",
    "                'weeks': 2,\n",
    "                'months': 3,\n",
    "                'flexible': 4\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def recommend_solution(self, dataset_size, budget, expertise_level, time_frame):\n",
    "        \"\"\"Recommend best embedding solution based on constraints\"\"\"\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # API-based solutions\n",
    "        if budget > 100 and expertise_level >= 1 and time_frame <= 2:\n",
    "            if dataset_size <= 100_000:\n",
    "                recommendations.append({\n",
    "                    'solution': 'OpenAI Embeddings API',\n",
    "                    'pros': ['Immediate deployment', 'High quality', 'No infrastructure'],\n",
    "                    'cons': ['Ongoing costs', 'API dependency', 'Rate limits'],\n",
    "                    'score': 85\n",
    "                })\n",
    "        \n",
    "        # Local sentence transformers\n",
    "        if expertise_level >= 2 and time_frame >= 2:\n",
    "            recommendations.append({\n",
    "                'solution': 'Local Sentence Transformers',\n",
    "                'pros': ['No ongoing costs', 'Full control', 'Privacy'],\n",
    "                'cons': ['Setup complexity', 'Hardware requirements', 'Maintenance'],\n",
    "                'score': 70 + (expertise_level * 5) + (time_frame * 5)\n",
    "            })\n",
    "        \n",
    "        # Hybrid approach\n",
    "        if budget > 500 and expertise_level >= 3:\n",
    "            recommendations.append({\n",
    "                'solution': 'Hybrid: API + Local Fine-tuning',\n",
    "                'pros': ['Best performance', 'Domain adaptation', 'Scalable'],\n",
    "                'cons': ['Complex setup', 'Higher costs', 'Expertise required'],\n",
    "                'score': 60 + (expertise_level * 10) + (budget / 1000 * 10)\n",
    "            })\n",
    "        \n",
    "        # Managed vector databases\n",
    "        if budget > 200 and dataset_size > 50_000:\n",
    "            recommendations.append({\n",
    "                'solution': 'Managed Vector Database (Pinecone/Weaviate Cloud)',\n",
    "                'pros': ['Scalable', 'Managed infrastructure', 'Production ready'],\n",
    "                'cons': ['Monthly costs', 'Vendor lock-in', 'Limited customization'],\n",
    "                'score': 75\n",
    "            })\n",
    "        \n",
    "        # Sort by score and return top recommendations\n",
    "        recommendations.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return recommendations[:3]\n",
    "\n",
    "# Test the decision framework\n",
    "framework = EmbeddingDecisionFramework()\n",
    "\n",
    "# Test scenarios\n",
    "scenarios = [\n",
    "    {\n",
    "        \"name\": \"Graduate Student Thesis\",\n",
    "        \"dataset_size\": 5_000,\n",
    "        \"budget\": 200,\n",
    "        \"expertise\": 2,\n",
    "        \"time_frame\": 2\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Digital Humanities Project\", \n",
    "        \"dataset_size\": 50_000,\n",
    "        \"budget\": 1_000,\n",
    "        \"expertise\": 3,\n",
    "        \"time_frame\": 3\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Library Production System\",\n",
    "        \"dataset_size\": 1_000_000,\n",
    "        \"budget\": 5_000,\n",
    "        \"expertise\": 4,\n",
    "        \"time_frame\": 4\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üéØ Embedding Solution Recommendations:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"\\nüìã Scenario: {scenario['name']}\")\n",
    "    print(f\"   Dataset: {scenario['dataset_size']:,} documents\")\n",
    "    print(f\"   Budget: ${scenario['budget']:,}\")\n",
    "    print(f\"   Expertise: {scenario['expertise']}/4\")\n",
    "    print(f\"   Timeline: {scenario['time_frame']}/4\")\n",
    "    \n",
    "    recommendations = framework.recommend_solution(\n",
    "        scenario['dataset_size'],\n",
    "        scenario['budget'],\n",
    "        scenario['expertise'],\n",
    "        scenario['time_frame']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n   üèÜ Top Recommendations:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"\\n   {i}. {rec['solution']} (Score: {rec['score']})\")\n",
    "        print(f\"      ‚úÖ Pros: {', '.join(rec['pros'])}\")\n",
    "        print(f\"      ‚ö†Ô∏è  Cons: {', '.join(rec['cons'])}\")\n",
    "    \n",
    "    print(\"-\" * 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Implementation Template\n",
    "\n",
    "Here's a production-ready template you can adapt for your research:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready embedding pipeline template\n",
    "class ResearchEmbeddingPipeline:\n",
    "    \"\"\"\n",
    "    Complete embedding pipeline for research applications\n",
    "    Supports multiple embedding providers and vector databases\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.embedding_model = None\n",
    "        self.vector_store = None\n",
    "        self.metadata = {\n",
    "            'total_documents': 0,\n",
    "            'processing_time': 0,\n",
    "            'index_size': 0\n",
    "        }\n",
    "        \n",
    "        self._initialize_components()\n",
    "    \n",
    "        def _initialize_components(self):
        """Initialize embedding model and vector store based on config"""
        
        # Initialize embedding model
        if self.config['embedding_provider'] == 'openai':
            # Would use OpenAI API here
            print(f"üîó Using OpenAI embeddings: {self.config['embedding_model']}")
            self.embedding_model = sentence_model  # Fallback for demo
        elif self.config['embedding_provider'] == 'sentence_transformers':
            print(f"ü§ñ Loading local model: {self.config['embedding_model']}")
            self.embedding_model = SentenceTransformer(self.config['embedding_model'])
        
        # Initialize vector store
        if self.config['vector_store'] == 'weaviate':
            print("üóÑÔ∏è Connecting to Weaviate...")
            # Vector store setup code would go here
        elif self.config['vector_store'] == 'local':
            print("üíæ Using local vector storage")
            self.vector_store = {}
    
    def process_documents(self, documents, batch_size=100):
        """Process documents in batches for efficiency"""
        
        print(f"üìù Processing {len(documents)} documents in batches of {batch_size}")
        
        start_time = time.time()
        processed = 0
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            # Generate embeddings for batch
            batch_texts = [doc['text'] for doc in batch]
            embeddings = self.embedding_model.encode(batch_texts, show_progress_bar=False)
            
            # Store embeddings with metadata
            for j, (doc, embedding) in enumerate(zip(batch, embeddings)):
                doc_id = f"doc_{i + j}"
                self.vector_store[doc_id] = {
                    'text': doc['text'],
                    'embedding': embedding,
                    'metadata': {k: v for k, v in doc.items() if k != 'text'}
                }
            
            processed += len(batch)
            
            if processed % 500 == 0:
                print(f"   ‚úÖ Processed {processed}/{len(documents)} documents")
        
        processing_time = time.time() - start_time
        self.metadata['total_documents'] = len(documents)
        self.metadata['processing_time'] = processing_time
        self.metadata['index_size'] = len(self.vector_store)
        
        print(f"üéâ Processing complete!")
        print(f"   Time: {processing_time:.2f} seconds")
        print(f"   Rate: {len(documents)/processing_time:.1f} docs/second")
    
    def semantic_search(self, query, top_k=5, filters=None):
        """Perform semantic search with optional metadata filtering"""
        
        # Generate query embedding
        query_embedding = self.embedding_model.encode(query)
        
        # Calculate similarities
        results = []
        for doc_id, doc_data in self.vector_store.items():
            # Apply filters if specified
            if filters:
                skip = False
                for filter_key, filter_value in filters.items():
                    if filter_key in doc_data['metadata']:
                        if doc_data['metadata'][filter_key] != filter_value:
                            skip = True
                            break
                if skip:
                    continue
            
            # Calculate similarity
            similarity = cosine_similarity(
                [query_embedding], 
                [doc_data['embedding']]
            )[0][0]
            
            results.append({
                'doc_id': doc_id,
                'text': doc_data['text'],
                'similarity': similarity,
                'metadata': doc_data['metadata']
            })
        
        # Sort by similarity and return top k
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]
    
    def get_statistics(self):
        """Get pipeline statistics and performance metrics"""
        return {
            'total_documents': self.metadata['total_documents'],
            'processing_time': self.metadata['processing_time'],
            'average_processing_time': self.metadata['processing_time'] / max(1, self.metadata['total_documents']),
            'index_size': self.metadata['index_size'],
            'embedding_model': self.config['embedding_model'],
            'vector_store': self.config['vector_store']
        }

# Example configuration for different use cases
configs = {
    'small_research_project': {
        'embedding_provider': 'sentence_transformers',
        'embedding_model': 'paraphrase-multilingual-MiniLM-L12-v2',
        'vector_store': 'local',
        'batch_size': 50
    },
    'medium_digital_humanities': {
        'embedding_provider': 'openai',
        'embedding_model': 'text-embedding-3-small',
        'vector_store': 'weaviate',
        'batch_size': 100
    },
    'large_library_system': {
        'embedding_provider': 'openai',
        'embedding_model': 'text-embedding-3-large',
        'vector_store': 'weaviate',
        'batch_size': 500
    }
}

# Demonstrate the pipeline
print("üöÄ Research Embedding Pipeline Demonstration:")
print("=" * 50)

# Use small research project configuration
config = configs['small_research_project']
pipeline = ResearchEmbeddingPipeline(config)

# Test with our multilingual corpus
print(f"\nüìä Pipeline Configuration:")
for key, value in config.items():
    print(f"   {key}: {value}")

# Process documents
pipeline.process_documents(multilingual_corpus)

# Test semantic search
print(f"\nüîç Testing Semantic Search:")
test_queries = [
    "classical music composition",
    "literatura latinoamericana", 
    "archaeological methods"
]

for query in test_queries:
    print(f"\n   Query: '{query}'")
    results = pipeline.semantic_search(query, top_k=3)
    
    for i, result in enumerate(results, 1):
        print(f"   {i}. [{result['metadata'].get('language', 'unknown').upper()}] "
              f"Similarity: {result['similarity']:.3f}")
        print(f"      {result['text'][:60]}...")

# Display statistics
print(f"\nüìà Pipeline Statistics:")
stats = pipeline.get_statistics()
for key, value in stats.items():
    if isinstance(value, float):
        print(f"   {key}: {value:.3f}")
    else:
        print(f"   {key}: {value}")

print("\n‚úÖ Pipeline demonstration complete!")
print("üéØ This template can be adapted for your specific research needs")
print("üí° Key customization points:")
print("   ‚Ä¢ Embedding provider (OpenAI, Hugging Face, local)")
print("   ‚Ä¢ Vector database (Weaviate, Pinecone, FAISS, local)")
print("   ‚Ä¢ Batch processing parameters")
print("   ‚Ä¢ Metadata filtering and search logic")
print("   ‚Ä¢ Performance monitoring and optimization")
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Requirements and Planning\n",
    "\n",
    "Here's a practical guide for planning your embedding infrastructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource planning calculator\n",
    "def calculate_resource_requirements(num_documents, avg_length=100, embedding_dims=384):\n",
    "    \"\"\"Calculate computational and storage requirements\"\"\"\n",
    "    \n",
    "    # Storage calculations (in MB)\n",
    "    text_storage = (num_documents * avg_length) / (1024 * 1024)  # Assume 1 byte per char\n",
    "    embedding_storage = (num_documents * embedding_dims * 4) / (1024 * 1024)  # 4 bytes per float\n",
    "    metadata_storage = (num_documents * 200) / (1024 * 1024)  # Estimate 200 bytes metadata\n",
    "    total_storage = text_storage + embedding_storage + metadata_storage\n",
    "    \n",
    "    # Processing time estimates (optimistic)\n",
    "    local_gpu_rate = 1000  # docs per second\n",
    "    local_cpu_rate = 100   # docs per second\n",
    "    api_rate = 50          # docs per second (rate limited)\n",
    "    \n",
    "    local_gpu_time = num_documents / local_gpu_rate / 3600  # hours\n",
    "    local_cpu_time = num_documents / local_cpu_rate / 3600  # hours\n",
    "    api_time = num_documents / api_rate / 3600             # hours\n",
    "    \n",
    "    return {\n",
    "        'storage': {\n",
    "            'text_mb': text_storage,\n",
    "            'embeddings_mb': embedding_storage,\n",
    "            'metadata_mb': metadata_storage,\n",
    "            'total_mb': total_storage,\n",
    "            'total_gb': total_storage / 1024\n",
    "        },\n",
    "        'processing_time_hours': {\n",
    "            'local_gpu': local_gpu_time,\n",
    "            'local_cpu': local_cpu_time,\n",
    "            'api_service': api_time\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Calculate for different project scales\n",
    "project_scales = {\n",
    "    \"Thesis Project\": 1_000,\n",
    "    \"Small Research\": 10_000,\n",
    "    \"Medium Project\": 100_000,\n",
    "    \"Large Archive\": 1_000_000,\n",
    "    \"Yale Library\": 17_590_104\n",
    "}\n",
    "\n",
    "print(\"üìä Resource Requirements Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "resource_data = []\n",
    "\n",
    "for project_name, num_docs in project_scales.items():\n",
    "    reqs = calculate_resource_requirements(num_docs)\n",
    "    \n",
    "    print(f\"\\nüìã {project_name} ({num_docs:,} documents):\")\n",
    "    print(f\"   Storage needed: {reqs['storage']['total_gb']:.1f} GB\")\n",
    "    print(f\"   Processing time:\")\n",
    "    print(f\"     ‚Ä¢ Local GPU: {reqs['processing_time_hours']['local_gpu']:.1f} hours\")\n",
    "    print(f\"     ‚Ä¢ Local CPU: {reqs['processing_time_hours']['local_cpu']:.1f} hours\")\n",
    "    print(f\"     ‚Ä¢ API Service: {reqs['processing_time_hours']['api_service']:.1f} hours\")\n",
    "    \n",
    "    resource_data.append({\n",
    "        'Project': project_name,\n",
    "        'Documents': num_docs,\n",
    "        'Storage_GB': reqs['storage']['total_gb'],\n",
    "        'GPU_Hours': reqs['processing_time_hours']['local_gpu'],\n",
    "        'CPU_Hours': reqs['processing_time_hours']['local_cpu'],\n",
    "        'API_Hours': reqs['processing_time_hours']['api_service']\n",
    "    })\n",
    "\n",
    "# Visualize resource scaling\n",
    "df_resources = pd.DataFrame(resource_data)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Storage requirements\n",
    "ax1.bar(range(len(df_resources)), df_resources['Storage_GB'], color='lightblue')\n",
    "ax1.set_xticks(range(len(df_resources)))\n",
    "ax1.set_xticklabels(df_resources['Project'], rotation=45, ha='right')\n",
    "ax1.set_ylabel('Storage (GB)')\n",
    "ax1.set_title('Storage Requirements')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Processing time comparison\n",
    "width = 0.25\n",
    "x = np.arange(len(df_resources))\n",
    "\n",
    "ax2.bar(x - width, df_resources['GPU_Hours'], width, label='Local GPU', alpha=0.8)\n",
    "ax2.bar(x, df_resources['CPU_Hours'], width, label='Local CPU', alpha=0.8)\n",
    "ax2.bar(x + width, df_resources['API_Hours'], width, label='API Service', alpha=0.8)\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(df_resources['Project'], rotation=45, ha='right')\n",
    "ax2.set_ylabel('Processing Time (Hours)')\n",
    "ax2.set_title('Processing Time by Method')\n",
    "ax2.set_yscale('log')\n",
    "ax2.legend()\n",
    "\n",
    "# Cost analysis\n",
    "# Assume: GPU compute $1/hour, CPU $0.1/hour, API $0.02/1K tokens\n",
    "df_resources['GPU_Cost'] = df_resources['GPU_Hours'] * 1.0\n",
    "df_resources['CPU_Cost'] = df_resources['CPU_Hours'] * 0.1\n",
    "df_resources['API_Cost'] = df_resources['Documents'] * 100 / 1000 * 0.00002  # Simplified\n",
    "\n",
    "ax3.bar(x - width, df_resources['GPU_Cost'], width, label='GPU Cost', alpha=0.8)\n",
    "ax3.bar(x, df_resources['CPU_Cost'], width, label='CPU Cost', alpha=0.8)\n",
    "ax3.bar(x + width, df_resources['API_Cost'], width, label='API Cost', alpha=0.8)\n",
    "\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(df_resources['Project'], rotation=45, ha='right')\n",
    "ax3.set_ylabel('Cost ($)')\n",
    "ax3.set_title('Processing Cost by Method')\n",
    "ax3.set_yscale('log')\n",
    "ax3.legend()\n",
    "\n",
    "# Scalability analysis\n",
    "ax4.loglog(df_resources['Documents'], df_resources['Storage_GB'], 'o-', label='Storage (GB)', linewidth=2)\n",
    "ax4.loglog(df_resources['Documents'], df_resources['GPU_Hours'], 's-', label='GPU Time (Hours)', linewidth=2)\n",
    "ax4.set_xlabel('Number of Documents')\n",
    "ax4.set_ylabel('Resources')\n",
    "ax4.set_title('Resource Scaling')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Planning Insights:\")\n",
    "print(\"   ‚Ä¢ Storage scales linearly with document count\")\n",
    "print(\"   ‚Ä¢ GPU processing provides best time/cost ratio\")\n",
    "print(\"   ‚Ä¢ API services excel for smaller projects\")\n",
    "print(\"   ‚Ä¢ Plan for 2-3x storage overhead for indices\")\n",
    "print(\"   ‚Ä¢ Consider batch processing for large datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Clean Up and Summary\n",
    "\n",
    "Let's clean up our connections and summarize what we've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up connections\n",
    "if weaviate_available and client:\n",
    "    try:\n",
    "        client.close()\n",
    "        print(\"‚úÖ Weaviate connection closed\")\n",
    "    except:\n",
    "        print(\"‚ÑπÔ∏è Connection cleanup completed\")\n",
    "\n",
    "print(\"\\nüßπ Cleanup completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Journey Complete: From Characters to Understanding\n",
    "\n",
    "Congratulations! You've completed a comprehensive journey through the evolution of text embeddings. Let's recap what you've learned:\n",
    "\n",
    "### üî§ Chapter 1: Word Embeddings (Word2Vec Era)\n",
    "**The Breakthrough**: Computers learned that words with similar meanings appear in similar contexts.\n",
    "\n",
    "**Key Insights**:\n",
    "- Vector arithmetic captures semantic relationships (`king - man + woman = queen`)\n",
    "- Semantic similarity becomes spatial proximity in high-dimensional space\n",
    "- Enabled the first practical applications of AI to language understanding\n",
    "\n",
    "**Limitations**: Fixed vocabulary, no context sensitivity, composition challenges\n",
    "\n",
    "### üìù Chapter 2: Sentence Embeddings (Transformer Era)\n",
    "**The Revolution**: Context determines meaning, and entire sentences can be understood as unified concepts.\n",
    "\n",
    "**Key Innovations**:\n",
    "- **Attention mechanisms** allow words to \"see\" all other words in context\n",
    "- **Bidirectional processing** understands text from both directions\n",
    "- **Multilingual capabilities** break down language barriers\n",
    "- **Context sensitivity** gives different meanings to the same word\n",
    "\n",
    "### üåê Chapter 3: Modern Text Embeddings (Production Era)\n",
    "**The Maturation**: AI systems that understand meaning at human-level accuracy and scale.\n",
    "\n",
    "**Production Capabilities**:\n",
    "- **99.55% precision** in real-world entity resolution\n",
    "- **Cross-language understanding** for global research\n",
    "- **Billions of documents** processed efficiently\n",
    "- **Domain adaptation** for specialized applications\n",
    "\n",
    "### üèõÔ∏è Chapter 4: Yale's Entity Resolution Case Study\n",
    "**Real-World Impact**: How embeddings solve actual research problems.\n",
    "\n",
    "**Demonstrated Solutions**:\n",
    "- **Distinguish between Franz Schubert the composer (1797-1828) and Franz Schubert the archaeologist (1978)**\n",
    "- **Match \"Garc√≠a M√°rquez, Gabriel\" with \"Gabriel Garcia Marquez\" across languages**\n",
    "- **Process 17.6 million catalog records** with 99.23% computational efficiency\n",
    "- **Integrate temporal, domain, and collaborative evidence** for robust decisions\n",
    "\n",
    "### üîç Chapter 5: Implementation Guidance\n",
    "**Practical Deployment**: How to choose and implement the right solution.\n",
    "\n",
    "**Decision Framework**:\n",
    "- **Small projects** (< 10K docs): API services or local models\n",
    "- **Medium projects** (10K-100K docs): Hybrid approaches\n",
    "- **Large systems** (100K+ docs): Production infrastructure with vector databases\n",
    "- **Budget considerations**: $0.02-$0.13 per 1K tokens vs. local compute costs\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Where to Go from Here\n",
    "\n",
    "### For Immediate Implementation:\n",
    "1. **Start Small**: Use Sentence Transformers for a pilot project\n",
    "2. **Test with Your Data**: Adapt the templates provided to your specific research questions\n",
    "3. **Measure Performance**: Use similarity scores and manual validation to assess quality\n",
    "4. **Scale Gradually**: Move from local processing to production infrastructure as needed\n",
    "\n",
    "### For Advanced Applications:\n",
    "1. **Fine-tuning**: Adapt models for your specific domain using SetFit or similar approaches\n",
    "2. **Vector Databases**: Deploy Weaviate, Pinecone, or similar for production search\n",
    "3. **Multi-modal Integration**: Combine text with images, audio, or structured data\n",
    "4. **Real-time Processing**: Build streaming pipelines for continuous data ingestion\n",
    "\n",
    "### Key Resources:\n",
    "- **Hugging Face Sentence Transformers**: [https://huggingface.co/sentence-transformers](https://huggingface.co/sentence-transformers)\n",
    "- **Weaviate Vector Database**: [https://weaviate.io](https://weaviate.io)\n",
    "- **OpenAI Embeddings API**: [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)\n",
    "- **SetFit Few-Shot Learning**: [https://huggingface.co/blog/setfit](https://huggingface.co/blog/setfit)\n",
    "\n",
    "---\n",
    "\n",
    "## üí´ The Transformation is Complete\n",
    "\n",
    "You began this journey with ASCII codes‚Äîarbitrary numbers that told computers nothing about meaning. You now understand how modern AI systems:\n",
    "\n",
    "‚ú® **Convert text to vectors that capture semantic relationships**  \n",
    "üß† **Understand context and meaning across languages and cultures**  \n",
    "üîç **Enable semantic search that finds meaning, not just keywords**  \n",
    "üè≠ **Scale to billions of documents with production-grade accuracy**  \n",
    "üåç **Bridge language barriers for truly global research**  \n",
    "\n",
    "The technology that seemed like magic at the start of this notebook is now a practical tool in your research toolkit. The only question remaining is: **What will you discover with it?**\n",
    "\n",
    "---\n",
    "\n",
    "*\"The best way to predict the future is to create it.\"* - Alan Kay\n",
    "\n",
    "Go forth and build amazing things! üöÄ\n",
    "\n",
    "---"
   ]
  }\n",
    "        