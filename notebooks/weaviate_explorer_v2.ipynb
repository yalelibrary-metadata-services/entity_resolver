{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Databases with Weaviate: Indexing and Retrieval at Scale\n",
    "\n",
    "**Yale AI Research Techniques Workshop**  \n",
    "*Text Embeddings and Classification for Entity Resolution*\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand why vector databases are essential for large-scale similarity search\n",
    "- Set up Weaviate with OpenAI embeddings integration\n",
    "- Index library metadata using proper schema design\n",
    "- Perform efficient similarity queries for entity resolution\n",
    "- Apply these techniques to real bibliographic data challenges\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction: Why Vector Databases?\n",
    "\n",
    "In our previous notebook, we explored Word2Vec and learned that words become vectors. But what happens when you have millions of these vectors and need to find similar ones quickly?\n",
    "\n",
    "**The Challenge**: Yale Library has 17.6 million catalog records. If each record becomes a vector with 1,536 dimensions (like OpenAI's embeddings), that's over 120 billion numbers to search through. Finding similar records would take hours using basic similarity calculations.\n",
    "\n",
    "**The Solution**: Vector databases like Weaviate use clever algorithms (like HNSW - Hierarchical Navigable Small World) to organize vectors in a way that makes similarity search lightning-fast. Instead of checking every single vector, these algorithms can find the most similar ones in milliseconds.\n",
    "\n",
    "Think of it like the difference between searching through a messy pile of papers versus using a well-organized library catalog system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up Our Environment\n",
    "\n",
    "We'll work with both OpenAI (for generating embeddings) and Weaviate (for storing and searching them). It's crucial to understand that these are separate services working together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install weaviate-client openai python-dotenv numpy pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T23:35:18.535435Z",
     "iopub.status.busy": "2025-06-30T23:35:18.535103Z",
     "iopub.status.idle": "2025-06-30T23:35:18.545239Z",
     "shell.execute_reply": "2025-06-30T23:35:18.544818Z",
     "shell.execute_reply.started": "2025-06-30T23:35:18.535399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "Weaviate client version: 4.8.0\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType, VectorDistances\n",
    "from weaviate.classes.query import Filter, MetadataQuery\n",
    "from weaviate.util import generate_uuid5\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"Weaviate client version: {weaviate.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding the Two-Client Architecture\n",
    "\n",
    "**Critical Concept**: We use TWO separate clients:\n",
    "1. **OpenAI Client**: Converts text into embedding vectors\n",
    "2. **Weaviate Client**: Stores vectors and performs similarity search\n",
    "\n",
    "This separation allows us to use OpenAI's powerful embedding models while leveraging Weaviate's efficient vector search capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T23:35:23.538086Z",
     "iopub.status.busy": "2025-06-30T23:35:23.537597Z",
     "iopub.status.idle": "2025-06-30T23:35:23.546107Z",
     "shell.execute_reply": "2025-06-30T23:35:23.545322Z",
     "shell.execute_reply.started": "2025-06-30T23:35:23.538051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— OpenAI Client: Handles text â†’ vector conversion\n",
      "   â€¢ Sends text to OpenAI's text-embedding-3-small model\n",
      "   â€¢ Receives 1,536-dimensional vectors back\n",
      "   â€¢ Cost: $0.02 per 1 million tokens\n",
      "ðŸ—„ï¸  Weaviate Client: Handles vector storage and similarity search\n",
      "   â€¢ Stores vectors with metadata in collections\n",
      "   â€¢ Performs fast similarity search using HNSW algorithm\n",
      "   â€¢ Can integrate directly with OpenAI for automatic embedding\n",
      "\n",
      "ðŸ’¡ Key Insight: These two clients work together but serve different purposes!\n"
     ]
    }
   ],
   "source": [
    "# Configuration - in practice, use environment variables for API keys\n",
    "OPENAI_API_KEY = \"your-openai-api-key-here\"  # Replace with your actual key\n",
    "WEAVIATE_URL = \"http://localhost:8080\"  # Local Weaviate instance\n",
    "\n",
    "# For this workshop, we'll use mock data to demonstrate concepts\n",
    "# In production, you'd have real API keys\n",
    "\n",
    "def setup_openai_client():\n",
    "    \"\"\"Initialize OpenAI client for generating embeddings.\"\"\"\n",
    "    # In a real scenario, you'd use: OpenAI(api_key=OPENAI_API_KEY)\n",
    "    # For demo purposes, we'll simulate this\n",
    "    print(\"ðŸ”— OpenAI Client: Handles text â†’ vector conversion\")\n",
    "    print(\"   â€¢ Sends text to OpenAI's text-embedding-3-small model\")\n",
    "    print(\"   â€¢ Receives 1,536-dimensional vectors back\")\n",
    "    print(\"   â€¢ Cost: $0.02 per 1 million tokens\")\n",
    "    return \"openai_client_placeholder\"\n",
    "\n",
    "def setup_weaviate_client():\n",
    "    \"\"\"Initialize Weaviate client for vector storage and search.\"\"\"\n",
    "    try:\n",
    "        # In production, this would connect to actual Weaviate instance\n",
    "        print(\"ðŸ—„ï¸  Weaviate Client: Handles vector storage and similarity search\")\n",
    "        print(\"   â€¢ Stores vectors with metadata in collections\")\n",
    "        print(\"   â€¢ Performs fast similarity search using HNSW algorithm\")\n",
    "        print(\"   â€¢ Can integrate directly with OpenAI for automatic embedding\")\n",
    "        \n",
    "        # For demo, we'll simulate the connection\n",
    "        # Real code would be:\n",
    "        # client = weaviate.WeaviateClient(\n",
    "        #     connection_params=weaviate.connect.ConnectionParams.from_url(WEAVIATE_URL)\n",
    "        # )\n",
    "        # client.connect()\n",
    "        \n",
    "        return \"weaviate_client_placeholder\"\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not connect to Weaviate (expected in demo): {e}\")\n",
    "        print(\"   This is normal - we'll demonstrate with mock data\")\n",
    "        return None\n",
    "\n",
    "# Initialize both clients\n",
    "openai_client = setup_openai_client()\n",
    "weaviate_client = setup_weaviate_client()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: These two clients work together but serve different purposes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Designing Our Schema for Library Metadata\n",
    "\n",
    "Before we can store vectors, we need to design a schema that captures the structure of our library metadata. This is like designing the blueprint for our vector database tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T23:35:29.177671Z",
     "iopub.status.busy": "2025-06-30T23:35:29.177206Z",
     "iopub.status.idle": "2025-06-30T23:35:29.186870Z",
     "shell.execute_reply": "2025-06-30T23:35:29.186402Z",
     "shell.execute_reply.started": "2025-06-30T23:35:29.177643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Schema Design for EntityString Collection\n",
      "==================================================\n",
      "Collection: EntityString\n",
      "Purpose: Collection for entity string values with their embeddings\n",
      "\n",
      "Properties (metadata we store with each vector):\n",
      "  â€¢ original_string (TEXT): The actual text (e.g., 'Schubert, Franz')\n",
      "  â€¢ hash_value (TEXT): Unique identifier for deduplication\n",
      "  â€¢ field_type (TEXT): What kind of field: 'person', 'title', 'composite'\n",
      "  â€¢ frequency (INT): How many times this string appears in the catalog\n",
      "\n",
      "Vector Configuration:\n",
      "  â€¢ model: text-embedding-3-small\n",
      "  â€¢ dimensions: 1536\n",
      "  â€¢ distance_metric: COSINE\n",
      "\n",
      "ðŸ’¡ Why This Design?\n",
      "â€¢ original_string: The actual text we want to search\n",
      "â€¢ hash_value: Prevents storing duplicate strings\n",
      "â€¢ field_type: Allows filtering by person names, titles, etc.\n",
      "â€¢ frequency: Helps with relevance ranking\n",
      "â€¢ Cosine distance: Best for text embeddings (as we learned earlier)\n"
     ]
    }
   ],
   "source": [
    "def design_entity_schema():\n",
    "    \"\"\"Design the schema for our EntityString collection.\n",
    "    \n",
    "    This schema is based on Yale's actual entity resolution pipeline.\n",
    "    Each record stores a text string along with its embedding vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    schema_design = {\n",
    "        \"collection_name\": \"EntityString\",\n",
    "        \"description\": \"Collection for entity string values with their embeddings\",\n",
    "        \"properties\": [\n",
    "            {\n",
    "                \"name\": \"original_string\",\n",
    "                \"type\": \"TEXT\",\n",
    "                \"description\": \"The actual text (e.g., 'Schubert, Franz')\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"hash_value\", \n",
    "                \"type\": \"TEXT\",\n",
    "                \"description\": \"Unique identifier for deduplication\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"field_type\",\n",
    "                \"type\": \"TEXT\", \n",
    "                \"description\": \"What kind of field: 'person', 'title', 'composite'\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"frequency\",\n",
    "                \"type\": \"INT\",\n",
    "                \"description\": \"How many times this string appears in the catalog\"\n",
    "            }\n",
    "        ],\n",
    "        \"vector_config\": {\n",
    "            \"model\": \"text-embedding-3-small\",\n",
    "            \"dimensions\": 1536,\n",
    "            \"distance_metric\": \"COSINE\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return schema_design\n",
    "\n",
    "# Display our schema design\n",
    "schema = design_entity_schema()\n",
    "print(\"ðŸ“‹ Schema Design for EntityString Collection\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Collection: {schema['collection_name']}\")\n",
    "print(f\"Purpose: {schema['description']}\")\n",
    "print(\"\\nProperties (metadata we store with each vector):\")\n",
    "\n",
    "for prop in schema['properties']:\n",
    "    print(f\"  â€¢ {prop['name']} ({prop['type']}): {prop['description']}\")\n",
    "\n",
    "print(\"\\nVector Configuration:\")\n",
    "for key, value in schema['vector_config'].items():\n",
    "    print(f\"  â€¢ {key}: {value}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Why This Design?\")\n",
    "print(\"â€¢ original_string: The actual text we want to search\")\n",
    "print(\"â€¢ hash_value: Prevents storing duplicate strings\")\n",
    "print(\"â€¢ field_type: Allows filtering by person names, titles, etc.\")\n",
    "print(\"â€¢ frequency: Helps with relevance ranking\")\n",
    "print(\"â€¢ Cosine distance: Best for text embeddings (as we learned earlier)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Creating the Collection with Integrated OpenAI Embeddings\n",
    "\n",
    "Weaviate can automatically generate embeddings using OpenAI's API. This means we only send text to Weaviate, and it handles the embedding generation behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T23:35:34.297804Z",
     "iopub.status.busy": "2025-06-30T23:35:34.297352Z",
     "iopub.status.idle": "2025-06-30T23:35:34.306241Z",
     "shell.execute_reply": "2025-06-30T23:35:34.305151Z",
     "shell.execute_reply.started": "2025-06-30T23:35:34.297777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—ï¸  Collection Creation Process\n",
      "========================================\n",
      "\n",
      "# Real Weaviate v4 collection creation code:\n",
      "collection = weaviate_client.collections.create(\n",
      "    name=\"EntityString\",\n",
      "    description=\"Collection for entity string values with their embeddings\",\n",
      "    \n",
      "    # Configure automatic OpenAI embedding generation\n",
      "    vectorizer_config=Configure.Vectorizer.text2vec_openai(\n",
      "        model=\"text-embedding-3-small\",\n",
      "        dimensions=1536\n",
      "    ),\n",
      "    \n",
      "    # Configure the vector index for fast similarity search\n",
      "    vector_index_config=Configure.VectorIndex.hnsw(\n",
      "        ef=128,                    # Higher = more accurate, slower\n",
      "        max_connections=64,        # Higher = more memory, faster search\n",
      "        ef_construction=128,       # Higher = better quality index\n",
      "        distance_metric=VectorDistances.COSINE\n",
      "    ),\n",
      "    \n",
      "    # Define the metadata properties\n",
      "    properties=[\n",
      "        Property(name=\"original_string\", data_type=DataType.TEXT),\n",
      "        Property(name=\"hash_value\", data_type=DataType.TEXT),\n",
      "        Property(name=\"field_type\", data_type=DataType.TEXT),\n",
      "        Property(name=\"frequency\", data_type=DataType.INT)\n",
      "    ]\n",
      ")\n",
      "\n",
      "\n",
      "ðŸ”§ Configuration Explained:\n",
      "\n",
      "1. Vectorizer Configuration:\n",
      "   â€¢ Tells Weaviate to use OpenAI's text-embedding-3-small\n",
      "   â€¢ Automatically converts text to 1,536-dimensional vectors\n",
      "   â€¢ No need to call OpenAI API manually!\n",
      "\n",
      "2. Vector Index Configuration (HNSW):\n",
      "   â€¢ ef=128: Search quality parameter (higher = more accurate)\n",
      "   â€¢ max_connections=64: Memory vs speed tradeoff\n",
      "   â€¢ ef_construction=128: Index building quality\n",
      "   â€¢ COSINE distance: Best for text embeddings\n",
      "\n",
      "3. Properties:\n",
      "   â€¢ Metadata that gets stored alongside each vector\n",
      "   â€¢ Allows filtering and retrieval of original information\n",
      "\n",
      "âœ… Collection design complete!\n",
      "In a real scenario, Weaviate would now be ready to accept data.\n"
     ]
    }
   ],
   "source": [
    "def create_collection_with_openai_integration():\n",
    "    \"\"\"Create a Weaviate collection that automatically generates OpenAI embeddings.\n",
    "    \n",
    "    This demonstrates the v4 Weaviate Python client syntax.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This is the actual code you'd use with a real Weaviate instance\n",
    "    creation_code = '''\n",
    "# Real Weaviate v4 collection creation code:\n",
    "collection = weaviate_client.collections.create(\n",
    "    name=\"EntityString\",\n",
    "    description=\"Collection for entity string values with their embeddings\",\n",
    "    \n",
    "    # Configure automatic OpenAI embedding generation\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_openai(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        dimensions=1536\n",
    "    ),\n",
    "    \n",
    "    # Configure the vector index for fast similarity search\n",
    "    vector_index_config=Configure.VectorIndex.hnsw(\n",
    "        ef=128,                    # Higher = more accurate, slower\n",
    "        max_connections=64,        # Higher = more memory, faster search\n",
    "        ef_construction=128,       # Higher = better quality index\n",
    "        distance_metric=VectorDistances.COSINE\n",
    "    ),\n",
    "    \n",
    "    # Define the metadata properties\n",
    "    properties=[\n",
    "        Property(name=\"original_string\", data_type=DataType.TEXT),\n",
    "        Property(name=\"hash_value\", data_type=DataType.TEXT),\n",
    "        Property(name=\"field_type\", data_type=DataType.TEXT),\n",
    "        Property(name=\"frequency\", data_type=DataType.INT)\n",
    "    ]\n",
    ")\n",
    "'''\n",
    "    \n",
    "    print(\"ðŸ—ï¸  Collection Creation Process\")\n",
    "    print(\"=\"*40)\n",
    "    print(creation_code)\n",
    "    \n",
    "    print(\"\\nðŸ”§ Configuration Explained:\")\n",
    "    print(\"\\n1. Vectorizer Configuration:\")\n",
    "    print(\"   â€¢ Tells Weaviate to use OpenAI's text-embedding-3-small\")\n",
    "    print(\"   â€¢ Automatically converts text to 1,536-dimensional vectors\")\n",
    "    print(\"   â€¢ No need to call OpenAI API manually!\")\n",
    "    \n",
    "    print(\"\\n2. Vector Index Configuration (HNSW):\")\n",
    "    print(\"   â€¢ ef=128: Search quality parameter (higher = more accurate)\")\n",
    "    print(\"   â€¢ max_connections=64: Memory vs speed tradeoff\")\n",
    "    print(\"   â€¢ ef_construction=128: Index building quality\")\n",
    "    print(\"   â€¢ COSINE distance: Best for text embeddings\")\n",
    "    \n",
    "    print(\"\\n3. Properties:\")\n",
    "    print(\"   â€¢ Metadata that gets stored alongside each vector\")\n",
    "    print(\"   â€¢ Allows filtering and retrieval of original information\")\n",
    "    \n",
    "    return \"collection_created_successfully\"\n",
    "\n",
    "# Demonstrate the collection creation process\n",
    "collection_status = create_collection_with_openai_integration()\n",
    "\n",
    "print(\"\\nâœ… Collection design complete!\")\n",
    "print(\"In a real scenario, Weaviate would now be ready to accept data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Preparing Library Metadata for Indexing\n",
    "\n",
    "Let's work with realistic examples from your entity resolution dataset. We'll create sample records that represent the Franz Schubert disambiguation challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T23:35:41.523178Z",
     "iopub.status.busy": "2025-06-30T23:35:41.522622Z",
     "iopub.status.idle": "2025-06-30T23:35:41.535219Z",
     "shell.execute_reply": "2025-06-30T23:35:41.534485Z",
     "shell.execute_reply.started": "2025-06-30T23:35:41.523142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Sample Library Records Created\n",
      "========================================\n",
      "Total bibliographic records: 4\n",
      "Total indexing items: 12\n",
      "Items per record: 3.0 (person + title + composite)\n",
      "\n",
      "ðŸ“‹ Sample Indexing Item:\n",
      "  original_string: Schubert, Franz\n",
      "  hash_value: hash_-7776589006278226991_person\n",
      "  field_type: person\n",
      "  frequency: 1\n",
      "  source_record: 9.1\n",
      "  actual_person: Franz August Schubert (1805-1893), German artist\n",
      "\n",
      "ðŸ’¡ This shows how one bibliographic record becomes multiple searchable items!\n"
     ]
    }
   ],
   "source": [
    "def create_sample_library_data():\n",
    "    \"\"\"Create sample library metadata following the actual entity resolution format.\n",
    "    \n",
    "    This represents the kind of data that would be indexed in Weaviate.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample records based on your actual entity resolution dataset\n",
    "    sample_records = [\n",
    "        {\n",
    "            \"identity\": \"9.1\",\n",
    "            \"composite\": \"Title: ArchÃ¤ologie und Photographie: fÃ¼nfzig Beispiele zur Geschichte und Methode Subjects: Photography in archaeology Provision information: Mainz: P. von Zabern, 1978\",\n",
    "            \"person\": \"Schubert, Franz\",\n",
    "            \"title\": \"ArchÃ¤ologie und Photographie: fÃ¼nfzig Beispiele zur Geschichte und Methode\",\n",
    "            \"subjects\": \"Photography in archaeology\",\n",
    "            \"attribution\": \"ausgewÃ¤hlt von Franz Schubert und Susanne Grunauer-von Hoerschelmann\",\n",
    "            \"provision\": \"Mainz: P. von Zabern, 1978\",\n",
    "            \"actual_person\": \"Franz August Schubert (1805-1893), German artist\"\n",
    "        },\n",
    "        {\n",
    "            \"identity\": \"12.7\",\n",
    "            \"composite\": \"Title: Symphony No. 8 in B minor 'Unfinished' Subjects: Symphonies, Classical music Person: Austrian composer 1797-1828\",\n",
    "            \"person\": \"Schubert, Franz, 1797-1828\",\n",
    "            \"title\": \"Symphony No. 8 in B minor 'Unfinished'\", \n",
    "            \"subjects\": \"Symphonies--19th century, Classical music\",\n",
    "            \"attribution\": \"composed by Franz Schubert\",\n",
    "            \"provision\": \"Vienna, 1822\",\n",
    "            \"actual_person\": \"Franz Schubert (1797-1828), Austrian composer\"\n",
    "        },\n",
    "        {\n",
    "            \"identity\": \"15.3\",\n",
    "            \"composite\": \"Title: Die schÃ¶ne MÃ¼llerin Subjects: Songs, German Lieder, Poetry settings Person: Austrian composer\",\n",
    "            \"person\": \"Schubert, Franz, 1797-1828\",\n",
    "            \"title\": \"Die schÃ¶ne MÃ¼llerin\",\n",
    "            \"subjects\": \"Songs, German--19th century, Lieder\",\n",
    "            \"attribution\": \"music by Franz Schubert; texts by Wilhelm MÃ¼ller\",\n",
    "            \"provision\": \"Vienna, 1823\",\n",
    "            \"actual_person\": \"Franz Schubert (1797-1828), Austrian composer\"\n",
    "        },\n",
    "        {\n",
    "            \"identity\": \"18.9\",\n",
    "            \"composite\": \"Title: Medieval manuscripts and illumination Subjects: Art history, Manuscripts Person: Franz Schubert German scholar\",\n",
    "            \"person\": \"Schubert, Franz\",\n",
    "            \"title\": \"Medieval manuscripts and illumination techniques\",\n",
    "            \"subjects\": \"Art history, Manuscript illumination, Medieval art\",\n",
    "            \"attribution\": \"by Franz Schubert\",\n",
    "            \"provision\": \"Munich: Beck, 1881\",\n",
    "            \"actual_person\": \"Franz August Schubert (1805-1893), German artist\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return sample_records\n",
    "\n",
    "def prepare_indexing_data(records):\n",
    "    \"\"\"Transform library records into the format needed for Weaviate indexing.\n",
    "    \n",
    "    This mimics the preprocessing step in your actual pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    indexing_items = []\n",
    "    \n",
    "    for record in records:\n",
    "        # For each record, we create multiple indexing items for different field types\n",
    "        # This allows us to search by person, title, or composite fields\n",
    "        \n",
    "        fields_to_index = {\n",
    "            'person': record['person'],\n",
    "            'title': record['title'], \n",
    "            'composite': record['composite']\n",
    "        }\n",
    "        \n",
    "        for field_type, text_value in fields_to_index.items():\n",
    "            if text_value and text_value.strip():  # Only index non-empty values\n",
    "                \n",
    "                # Create a hash for deduplication (simplified version)\n",
    "                hash_value = f\"hash_{hash(text_value)}_{field_type}\"\n",
    "                \n",
    "                indexing_items.append({\n",
    "                    'original_string': text_value,\n",
    "                    'hash_value': hash_value,\n",
    "                    'field_type': field_type,\n",
    "                    'frequency': 1,  # In real pipeline, this would be calculated\n",
    "                    'source_record': record['identity'],  # For tracking\n",
    "                    'actual_person': record['actual_person']  # For our analysis\n",
    "                })\n",
    "    \n",
    "    return indexing_items\n",
    "\n",
    "# Create and prepare our sample data\n",
    "library_records = create_sample_library_data()\n",
    "indexing_data = prepare_indexing_data(library_records)\n",
    "\n",
    "print(\"ðŸ“š Sample Library Records Created\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Total bibliographic records: {len(library_records)}\")\n",
    "print(f\"Total indexing items: {len(indexing_data)}\")\n",
    "print(f\"Items per record: {len(indexing_data) / len(library_records):.1f} (person + title + composite)\")\n",
    "\n",
    "# Show a sample indexing item\n",
    "print(\"\\nðŸ“‹ Sample Indexing Item:\")\n",
    "sample_item = indexing_data[0]\n",
    "for key, value in sample_item.items():\n",
    "    if key == 'original_string' and len(str(value)) > 60:\n",
    "        print(f\"  {key}: {str(value)[:60]}...\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ This shows how one bibliographic record becomes multiple searchable items!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: The Indexing Process\n",
    "\n",
    "Now let's understand how data gets indexed into Weaviate. In your actual pipeline, this happens automatically, but let's break down each step for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T23:35:57.291766Z",
     "iopub.status.busy": "2025-06-30T23:35:57.291152Z",
     "iopub.status.idle": "2025-06-30T23:35:57.300614Z",
     "shell.execute_reply": "2025-06-30T23:35:57.300112Z",
     "shell.execute_reply.started": "2025-06-30T23:35:57.291737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Batch Indexing Process\n",
      "==============================\n",
      "\n",
      "# Real Weaviate v4 batch indexing code:\n",
      "collection = weaviate_client.collections.get(\"EntityString\")\n",
      "\n",
      "# Use batch processing for efficiency\n",
      "with collection.batch.fixed_size(batch_size=100) as batch_writer:\n",
      "    for item in indexing_data:\n",
      "        # Generate consistent UUID for deduplication\n",
      "        uuid_input = f\"{item['hash_value']}_{item['field_type']}\"\n",
      "        uuid = generate_uuid5(uuid_input)\n",
      "        \n",
      "        # Add object to batch (Weaviate will auto-generate embedding)\n",
      "        batch_writer.add_object(\n",
      "            properties={\n",
      "                \"original_string\": item['original_string'],\n",
      "                \"hash_value\": item['hash_value'],\n",
      "                \"field_type\": item['field_type'],\n",
      "                \"frequency\": item['frequency']\n",
      "            },\n",
      "            uuid=uuid\n",
      "        )\n",
      "        # Note: No vector needed! OpenAI integration handles it automatically\n",
      "\n",
      "\n",
      "ðŸ”„ What Happens During Indexing:\n",
      "\n",
      "1. Text Processing:\n",
      "   â€¢ Weaviate receives the original_string\n",
      "   â€¢ Sends text to OpenAI's embedding API\n",
      "   â€¢ Receives back 1,536-dimensional vector\n",
      "\n",
      "2. Vector Storage:\n",
      "   â€¢ Vector gets stored in HNSW index structure\n",
      "   â€¢ Metadata (properties) stored alongside\n",
      "   â€¢ UUID ensures no duplicates\n",
      "\n",
      "3. Index Building:\n",
      "   â€¢ HNSW algorithm creates navigation structure\n",
      "   â€¢ Similar vectors get connected in graph\n",
      "   â€¢ Enables fast similarity search later\n",
      "\n",
      "ðŸ’° Indexing Cost Estimate:\n",
      "  â€¢ Total characters: 765\n",
      "  â€¢ Estimated tokens: 191\n",
      "  â€¢ Estimated cost: $0.0000\n",
      "  â€¢ Cost per item: $0.000000\n",
      "\n",
      "âœ… Indexing process complete!\n",
      "In production, Weaviate would now contain searchable vectors for all our library metadata.\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_batch_indexing_process(indexing_data):\n",
    "    \"\"\"Show how batch indexing works in Weaviate v4.\n",
    "    \n",
    "    This demonstrates the actual code pattern from your pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    indexing_code = '''\n",
    "# Real Weaviate v4 batch indexing code:\n",
    "collection = weaviate_client.collections.get(\"EntityString\")\n",
    "\n",
    "# Use batch processing for efficiency\n",
    "with collection.batch.fixed_size(batch_size=100) as batch_writer:\n",
    "    for item in indexing_data:\n",
    "        # Generate consistent UUID for deduplication\n",
    "        uuid_input = f\"{item['hash_value']}_{item['field_type']}\"\n",
    "        uuid = generate_uuid5(uuid_input)\n",
    "        \n",
    "        # Add object to batch (Weaviate will auto-generate embedding)\n",
    "        batch_writer.add_object(\n",
    "            properties={\n",
    "                \"original_string\": item['original_string'],\n",
    "                \"hash_value\": item['hash_value'],\n",
    "                \"field_type\": item['field_type'],\n",
    "                \"frequency\": item['frequency']\n",
    "            },\n",
    "            uuid=uuid\n",
    "        )\n",
    "        # Note: No vector needed! OpenAI integration handles it automatically\n",
    "'''\n",
    "    \n",
    "    print(\"âš¡ Batch Indexing Process\")\n",
    "    print(\"=\"*30)\n",
    "    print(indexing_code)\n",
    "    \n",
    "    print(\"\\nðŸ”„ What Happens During Indexing:\")\n",
    "    print(\"\\n1. Text Processing:\")\n",
    "    print(\"   â€¢ Weaviate receives the original_string\")\n",
    "    print(\"   â€¢ Sends text to OpenAI's embedding API\")\n",
    "    print(\"   â€¢ Receives back 1,536-dimensional vector\")\n",
    "    \n",
    "    print(\"\\n2. Vector Storage:\")\n",
    "    print(\"   â€¢ Vector gets stored in HNSW index structure\")\n",
    "    print(\"   â€¢ Metadata (properties) stored alongside\")\n",
    "    print(\"   â€¢ UUID ensures no duplicates\")\n",
    "    \n",
    "    print(\"\\n3. Index Building:\")\n",
    "    print(\"   â€¢ HNSW algorithm creates navigation structure\")\n",
    "    print(\"   â€¢ Similar vectors get connected in graph\")\n",
    "    print(\"   â€¢ Enables fast similarity search later\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def simulate_indexing_costs(indexing_data):\n",
    "    \"\"\"Calculate the cost of indexing our sample data.\"\"\"\n",
    "    \n",
    "    # Estimate tokens for cost calculation\n",
    "    total_chars = sum(len(item['original_string']) for item in indexing_data)\n",
    "    estimated_tokens = total_chars // 4  # Rough estimation: 4 chars per token\n",
    "    \n",
    "    # OpenAI pricing\n",
    "    cost_per_million_tokens = 0.02\n",
    "    estimated_cost = (estimated_tokens / 1_000_000) * cost_per_million_tokens\n",
    "    \n",
    "    print(f\"\\nðŸ’° Indexing Cost Estimate:\")\n",
    "    print(f\"  â€¢ Total characters: {total_chars:,}\")\n",
    "    print(f\"  â€¢ Estimated tokens: {estimated_tokens:,}\")\n",
    "    print(f\"  â€¢ Estimated cost: ${estimated_cost:.4f}\")\n",
    "    print(f\"  â€¢ Cost per item: ${estimated_cost/len(indexing_data):.6f}\")\n",
    "    \n",
    "    return estimated_cost\n",
    "\n",
    "# Demonstrate the indexing process\n",
    "demonstrate_batch_indexing_process(indexing_data)\n",
    "simulate_indexing_costs(indexing_data)\n",
    "\n",
    "print(\"\\nâœ… Indexing process complete!\")\n",
    "print(\"In production, Weaviate would now contain searchable vectors for all our library metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Similarity Search and Retrieval\n",
    "\n",
    "Now comes the exciting part: finding similar entities! This is where the magic of vector search really shines. Let's explore how to query our indexed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_similarity_search():\n",
    "    \"\"\"Show how to perform similarity search in Weaviate v4.\n",
    "    \n",
    "    This demonstrates the querying patterns from your actual pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    search_code = '''\n",
    "# Real Weaviate v4 similarity search code:\n",
    "collection = weaviate_client.collections.get(\"EntityString\")\n",
    "\n",
    "# Method 1: Search by text (let Weaviate handle embedding)\n",
    "result = collection.query.near_text(\n",
    "    query=\"Schubert composer Austrian music\",\n",
    "    filters=Filter.by_property(\"field_type\").equal(\"person\"),\n",
    "    limit=5,\n",
    "    return_properties=[\"original_string\", \"field_type\", \"frequency\"],\n",
    "    return_metadata=MetadataQuery(distance=True)\n",
    ")\n",
    "\n",
    "# Method 2: Search by vector (if you have pre-computed embedding)\n",
    "result = collection.query.near_vector(\n",
    "    near_vector=query_vector,  # 1536-dimensional vector\n",
    "    filters=Filter.by_property(\"field_type\").equal(\"composite\"),\n",
    "    limit=10,\n",
    "    return_properties=[\"original_string\", \"hash_value\"],\n",
    "    return_metadata=MetadataQuery(distance=True, score=True)\n",
    ")\n",
    "'''\n",
    "    \n",
    "    print(\"ðŸ” Similarity Search Methods\")\n",
    "    print(\"=\"*35)\n",
    "    print(search_code)\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ Search Method Comparison:\")\n",
    "    print(\"\\n1. near_text():\")\n",
    "    print(\"   â€¢ Send plain text query\")\n",
    "    print(\"   â€¢ Weaviate converts to vector automatically\")\n",
    "    print(\"   â€¢ Good for exploring and testing\")\n",
    "    \n",
    "    print(\"\\n2. near_vector():\")\n",
    "    print(\"   â€¢ Send pre-computed vector\")\n",
    "    print(\"   â€¢ Faster (no embedding API call)\")\n",
    "    print(\"   â€¢ Used in your production pipeline\")\n",
    "    \n",
    "    print(\"\\nðŸ”§ Query Components:\")\n",
    "    print(\"   â€¢ filters: Restrict search to specific field types\")\n",
    "    print(\"   â€¢ limit: Control number of results returned\")\n",
    "    print(\"   â€¢ return_properties: Which metadata to include\")\n",
    "    print(\"   â€¢ return_metadata: Include distance/similarity scores\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def simulate_entity_resolution_query(query_text, target_field_type=\"person\"):\n",
    "    \"\"\"Simulate how entity resolution queries work.\n",
    "    \n",
    "    This shows what would happen in your actual pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸ”Ž Entity Resolution Query: '{query_text}'\")\n",
    "    print(f\"Target field type: {target_field_type}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Simulate finding similar items from our sample data\n",
    "    # In reality, this would use vector similarity\n",
    "    relevant_items = []\n",
    "    \n",
    "    for item in indexing_data:\n",
    "        if item['field_type'] == target_field_type:\n",
    "            # Simple keyword matching for simulation\n",
    "            text_to_check = item['original_string'].lower()\n",
    "            query_words = query_text.lower().split()\n",
    "            \n",
    "            # Calculate simple similarity score\n",
    "            matches = sum(1 for word in query_words if word in text_to_check)\n",
    "            if matches > 0:\n",
    "                # Simulate cosine similarity score\n",
    "                simulated_similarity = min(0.95, matches / len(query_words) + 0.1)\n",
    "                \n",
    "                relevant_items.append({\n",
    "                    'text': item['original_string'],\n",
    "                    'source': item['source_record'],\n",
    "                    'person': item['actual_person'],\n",
    "                    'similarity': simulated_similarity,\n",
    "                    'distance': 1.0 - simulated_similarity\n",
    "                })\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    relevant_items.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Search Results:\")\n",
    "    for i, item in enumerate(relevant_items[:5], 1):\n",
    "        print(f\"\\n{i}. Similarity: {item['similarity']:.3f} (distance: {item['distance']:.3f})\")\n",
    "        print(f\"   Text: {item['text'][:80]}{'...' if len(item['text']) > 80 else ''}\")\n",
    "        print(f\"   Source: Record {item['source']}\")\n",
    "        print(f\"   Actual Person: {item['person']}\")\n",
    "    \n",
    "    if not relevant_items:\n",
    "        print(\"   No similar items found in sample data.\")\n",
    "    \n",
    "    return relevant_items\n",
    "\n",
    "# Demonstrate search methods\n",
    "demonstrate_similarity_search()\n",
    "\n",
    "# Simulate some actual entity resolution queries\n",
    "test_queries = [\n",
    "    \"Schubert Franz composer\",\n",
    "    \"Schubert photography archaeology\",\n",
    "    \"Austrian composer symphony\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = simulate_entity_resolution_query(query, \"person\")\n",
    "    time.sleep(1)  # Brief pause for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Advanced Querying Techniques\n",
    "\n",
    "Let's explore more sophisticated querying patterns that are essential for entity resolution at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_advanced_filtering():\n",
    "    \"\"\"Show advanced filtering and aggregation capabilities.\"\"\"\n",
    "    \n",
    "    advanced_code = '''\n",
    "# Advanced Weaviate v4 query patterns:\n",
    "\n",
    "# 1. Multi-field filtering\n",
    "result = collection.query.near_text(\n",
    "    query=\"Franz Schubert\",\n",
    "    filters=(\n",
    "        Filter.by_property(\"field_type\").equal(\"person\") &\n",
    "        Filter.by_property(\"frequency\").greater_than(1)\n",
    "    ),\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "# 2. Hybrid search (keyword + vector)\n",
    "result = collection.query.hybrid(\n",
    "    query=\"Austrian composer\",\n",
    "    alpha=0.7,  # 0.7 vector + 0.3 keyword search\n",
    "    filters=Filter.by_property(\"field_type\").equal(\"composite\")\n",
    ")\n",
    "\n",
    "# 3. Aggregation queries\n",
    "aggregation = collection.aggregate.over_all(\n",
    "    group_by=\"field_type\",\n",
    "    total_count=True\n",
    ")\n",
    "\n",
    "# 4. Batch similarity search\n",
    "queries = [\"composer\", \"artist\", \"photographer\"]\n",
    "for query_text in queries:\n",
    "    result = collection.query.near_text(\n",
    "        query=query_text,\n",
    "        limit=3\n",
    "    )\n",
    "'''\n",
    "    \n",
    "    print(\"ðŸš€ Advanced Query Patterns\")\n",
    "    print(\"=\"*30)\n",
    "    print(advanced_code)\n",
    "    \n",
    "    print(\"\\nðŸ”§ Advanced Techniques Explained:\")\n",
    "    print(\"\\n1. Multi-field Filtering:\")\n",
    "    print(\"   â€¢ Combine multiple conditions with & (AND) or | (OR)\")\n",
    "    print(\"   â€¢ Filter by field_type AND frequency threshold\")\n",
    "    print(\"   â€¢ Helps narrow down results to most relevant items\")\n",
    "    \n",
    "    print(\"\\n2. Hybrid Search:\")\n",
    "    print(\"   â€¢ Combines vector similarity with keyword matching\")\n",
    "    print(\"   â€¢ Alpha parameter controls the balance (0=keywords, 1=vectors)\")\n",
    "    print(\"   â€¢ Best of both worlds for complex queries\")\n",
    "    \n",
    "    print(\"\\n3. Aggregation:\")\n",
    "    print(\"   â€¢ Get statistics about your data\")\n",
    "    print(\"   â€¢ Count items by field type, frequency, etc.\")\n",
    "    print(\"   â€¢ Useful for understanding data distribution\")\n",
    "    \n",
    "    print(\"\\n4. Batch Processing:\")\n",
    "    print(\"   â€¢ Process multiple queries efficiently\")\n",
    "    print(\"   â€¢ Essential for large-scale entity resolution\")\n",
    "    print(\"   â€¢ Reduces API overhead\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def analyze_query_performance():\n",
    "    \"\"\"Analyze the performance characteristics of different query types.\"\"\"\n",
    "    \n",
    "    performance_data = {\n",
    "        'Query Type': [\n",
    "            'near_text()',\n",
    "            'near_vector()',\n",
    "            'hybrid()',\n",
    "            'filtered near_text()',\n",
    "            'aggregation()'\n",
    "        ],\n",
    "        'Speed': [\n",
    "            'Medium',\n",
    "            'Fast',\n",
    "            'Medium-Slow',\n",
    "            'Medium',\n",
    "            'Fast'\n",
    "        ],\n",
    "        'Accuracy': [\n",
    "            'High',\n",
    "            'High',\n",
    "            'Very High',\n",
    "            'High',\n",
    "            'N/A'\n",
    "        ],\n",
    "        'Use Case': [\n",
    "            'Exploration, prototyping',\n",
    "            'Production similarity search',\n",
    "            'Complex entity matching',\n",
    "            'Filtered entity resolution',\n",
    "            'Data analysis, monitoring'\n",
    "        ],\n",
    "        'API Calls': [\n",
    "            '2 (Weaviate + OpenAI)',\n",
    "            '1 (Weaviate only)',\n",
    "            '2 (Weaviate + OpenAI)', \n",
    "            '2 (Weaviate + OpenAI)',\n",
    "            '1 (Weaviate only)'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(performance_data)\n",
    "    print(\"\\nâš¡ Query Performance Analysis\")\n",
    "    print(\"=\"*40)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Performance Tips:\")\n",
    "    print(\"â€¢ Use near_vector() in production for best speed\")\n",
    "    print(\"â€¢ Cache frequently-used embedding vectors\")\n",
    "    print(\"â€¢ Apply filters to reduce search space\")\n",
    "    print(\"â€¢ Use batch operations when processing many queries\")\n",
    "    print(\"â€¢ Monitor query latency and adjust limits as needed\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Demonstrate advanced techniques\n",
    "demonstrate_advanced_filtering()\n",
    "performance_df = analyze_query_performance()\n",
    "\n",
    "print(\"\\nâœ… Advanced querying concepts covered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Entity Resolution Pipeline Integration\n",
    "\n",
    "Let's put it all together and show how Weaviate integrates into your complete entity resolution pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_complete_pipeline():\n",
    "    \"\"\"Show how Weaviate fits into the complete entity resolution pipeline.\"\"\"\n",
    "    \n",
    "    pipeline_stages = [\n",
    "        {\n",
    "            'stage': '1. Data Preparation',\n",
    "            'description': 'Extract and clean metadata from library catalog',\n",
    "            'tools': 'Pandas, custom preprocessing',\n",
    "            'output': 'Structured records with person/title/composite fields'\n",
    "        },\n",
    "        {\n",
    "            'stage': '2. Embedding Generation', \n",
    "            'description': 'Convert text fields to vectors',\n",
    "            'tools': 'OpenAI text-embedding-3-small API',\n",
    "            'output': '1,536-dimensional vectors for each text field'\n",
    "        },\n",
    "        {\n",
    "            'stage': '3. Vector Indexing',\n",
    "            'description': 'Store vectors in searchable index',\n",
    "            'tools': 'Weaviate with HNSW algorithm',\n",
    "            'output': 'Fast similarity search capability'\n",
    "        },\n",
    "        {\n",
    "            'stage': '4. Candidate Generation',\n",
    "            'description': 'Find potentially matching entities',\n",
    "            'tools': 'Weaviate near_vector queries',\n",
    "            'output': 'Candidate pairs for detailed comparison'\n",
    "        },\n",
    "        {\n",
    "            'stage': '5. Feature Engineering',\n",
    "            'description': 'Calculate similarity features',\n",
    "            'tools': 'Cosine similarity, birth/death matching, taxonomy',\n",
    "            'output': 'Feature vectors for classification'\n",
    "        },\n",
    "        {\n",
    "            'stage': '6. Classification',\n",
    "            'description': 'Decide if candidates are same entity',\n",
    "            'tools': 'Logistic regression or SetFit classifier',\n",
    "            'output': 'Match/no-match decisions with confidence scores'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸ”„ Complete Entity Resolution Pipeline\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    for stage in pipeline_stages:\n",
    "        print(f\"\\n{stage['stage']}\")\n",
    "        print(f\"  Purpose: {stage['description']}\")\n",
    "        print(f\"  Tools: {stage['tools']}\")\n",
    "        print(f\"  Output: {stage['output']}\")\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ Weaviate's Role:\")\n",
    "    print(\"â€¢ Stages 2-4: Core vector storage and similarity search\")\n",
    "    print(\"â€¢ Enables efficient candidate generation at scale\")\n",
    "    print(\"â€¢ Reduces O(nÂ²) problem to O(kÃ—m) where k << n\")\n",
    "    print(\"â€¢ Critical for processing millions of catalog records\")\n",
    "    \n",
    "    return pipeline_stages\n",
    "\n",
    "def calculate_scalability_benefits():\n",
    "    \"\"\"Show the scalability benefits of using Weaviate vs naive approaches.\"\"\"\n",
    "    \n",
    "    # Yale Library catalog size\n",
    "    catalog_size = 17_590_104\n",
    "    \n",
    "    # Naive approach: compare every pair\n",
    "    naive_comparisons = catalog_size * (catalog_size - 1) // 2\n",
    "    \n",
    "    # Weaviate approach: only compare similar candidates\n",
    "    avg_candidates_per_query = 100  # Typical from your pipeline\n",
    "    weaviate_comparisons = catalog_size * avg_candidates_per_query\n",
    "    \n",
    "    reduction_factor = naive_comparisons / weaviate_comparisons\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Scalability Analysis\")\n",
    "    print(\"=\"*25)\n",
    "    print(f\"Catalog size: {catalog_size:,} records\")\n",
    "    print(f\"\\nNaive approach (compare all pairs):\")\n",
    "    print(f\"  Comparisons needed: {naive_comparisons:,}\")\n",
    "    print(f\"  Time estimate: ~{naive_comparisons/1000000:.0f} million seconds\")\n",
    "    \n",
    "    print(f\"\\nWeaviate approach (similarity search):\")\n",
    "    print(f\"  Comparisons needed: {weaviate_comparisons:,}\")\n",
    "    print(f\"  Time estimate: ~{weaviate_comparisons/1000:.0f} thousand seconds\")\n",
    "    \n",
    "    print(f\"\\nðŸš€ Improvement: {reduction_factor:.0f}x fewer comparisons!\")\n",
    "    print(f\"This turns months of computation into hours.\")\n",
    "    \n",
    "    return reduction_factor\n",
    "\n",
    "# Demonstrate complete pipeline\n",
    "pipeline = demonstrate_complete_pipeline()\n",
    "reduction = calculate_scalability_benefits()\n",
    "\n",
    "print(\"\\nðŸŽ“ Key Takeaways:\")\n",
    "print(\"â€¢ Vector databases are essential for large-scale similarity search\")\n",
    "print(\"â€¢ Weaviate handles the complexity of vector indexing and search\")\n",
    "print(\"â€¢ Integration with OpenAI embeddings provides state-of-the-art accuracy\")\n",
    "print(\"â€¢ Proper schema design enables flexible querying and filtering\")\n",
    "print(f\"â€¢ Scalability improvements make real-world applications feasible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Production Considerations and Best Practices\n",
    "\n",
    "As we wrap up, let's discuss the practical considerations for deploying Weaviate in production environments like Yale Library's infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discuss_production_deployment():\n",
    "    \"\"\"Cover key considerations for production Weaviate deployment.\"\"\"\n",
    "    \n",
    "    considerations = {\n",
    "        'Infrastructure': {\n",
    "            'Hardware': [\n",
    "                'Memory: 2-4x your vector data size for optimal performance',\n",
    "                'Storage: Fast SSDs for vector index files', \n",
    "                'CPU: Multiple cores for concurrent query processing',\n",
    "                'Network: Low latency for real-time applications'\n",
    "            ],\n",
    "            'Deployment': [\n",
    "                'Docker containers for easy scaling',\n",
    "                'Kubernetes for orchestration and auto-scaling',\n",
    "                'Load balancers for high availability',\n",
    "                'Backup strategies for data persistence'\n",
    "            ]\n",
    "        },\n",
    "        'Data Management': {\n",
    "            'Schema Design': [\n",
    "                'Plan property types carefully (cannot change easily)',\n",
    "                'Use appropriate data types for filtering',\n",
    "                'Consider multi-tenant schemas if needed',\n",
    "                'Version your schema changes'\n",
    "            ],\n",
    "            'Indexing Strategy': [\n",
    "                'Batch operations for bulk loading',\n",
    "                'Monitor indexing performance and memory usage',\n",
    "                'Use appropriate vector compression if needed',\n",
    "                'Plan for index rebuilding procedures'\n",
    "            ]\n",
    "        },\n",
    "        'Performance': {\n",
    "            'Query Optimization': [\n",
    "                'Use filters to reduce search space',\n",
    "                'Cache frequently-used embeddings',\n",
    "                'Optimize ef and efConstruction parameters',\n",
    "                'Monitor query latency and throughput'\n",
    "            ],\n",
    "            'Scaling': [\n",
    "                'Horizontal scaling with sharding',\n",
    "                'Read replicas for query-heavy workloads',\n",
    "                'Connection pooling for multiple clients',\n",
    "                'Rate limiting to protect against overload'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ—ï¸  Production Deployment Considerations\")\n",
    "    print(\"=\"*42)\n",
    "    \n",
    "    for category, subcategories in considerations.items():\n",
    "        print(f\"\\nðŸ“‹ {category}:\")\n",
    "        for subcat, items in subcategories.items():\n",
    "            print(f\"\\n  {subcat}:\")\n",
    "            for item in items:\n",
    "                print(f\"    â€¢ {item}\")\n",
    "    \n",
    "    return considerations\n",
    "\n",
    "def estimate_resource_requirements():\n",
    "    \"\"\"Estimate resource requirements for Yale Library's use case.\"\"\"\n",
    "    \n",
    "    # Based on Yale's 17.6M catalog records\n",
    "    records = 17_590_104\n",
    "    \n",
    "    # Assume 3 vector types per record (person, title, composite)\n",
    "    vectors_per_record = 3\n",
    "    total_vectors = records * vectors_per_record\n",
    "    \n",
    "    # Vector specifications\n",
    "    dimensions = 1536\n",
    "    bytes_per_float = 4  # float32\n",
    "    vector_size_bytes = dimensions * bytes_per_float\n",
    "    \n",
    "    # Calculate storage requirements\n",
    "    raw_vector_storage = total_vectors * vector_size_bytes\n",
    "    metadata_overhead = raw_vector_storage * 0.3  # Rough estimate\n",
    "    index_overhead = raw_vector_storage * 0.5  # HNSW index overhead\n",
    "    total_storage = raw_vector_storage + metadata_overhead + index_overhead\n",
    "    \n",
    "    # Memory requirements (should be 2-4x storage for optimal performance)\n",
    "    recommended_memory = total_storage * 3\n",
    "    \n",
    "    print(\"\\nðŸ’¾ Resource Requirements for Yale Library\")\n",
    "    print(\"=\"*45)\n",
    "    print(f\"Catalog records: {records:,}\")\n",
    "    print(f\"Total vectors: {total_vectors:,}\")\n",
    "    print(f\"Vector dimensions: {dimensions:,}\")\n",
    "    \n",
    "    print(f\"\\nStorage Requirements:\")\n",
    "    print(f\"  Raw vectors: {raw_vector_storage/1024**3:.1f} GB\")\n",
    "    print(f\"  Metadata: {metadata_overhead/1024**3:.1f} GB\")\n",
    "    print(f\"  Index overhead: {index_overhead/1024**3:.1f} GB\")\n",
    "    print(f\"  Total storage: {total_storage/1024**3:.1f} GB\")\n",
    "    \n",
    "    print(f\"\\nRecommended Memory: {recommended_memory/1024**3:.1f} GB\")\n",
    "    \n",
    "    print(f\"\\nðŸ’° Estimated Monthly Costs:\")\n",
    "    # Rough AWS pricing estimates\n",
    "    storage_cost = (total_storage/1024**3) * 0.10  # $0.10/GB/month for SSD\n",
    "    memory_cost = (recommended_memory/1024**3) * 0.05  # Rough memory cost\n",
    "    compute_cost = 200  # Estimated compute instance cost\n",
    "    total_monthly = storage_cost + memory_cost + compute_cost\n",
    "    \n",
    "    print(f\"  Storage: ${storage_cost:.2f}/month\")\n",
    "    print(f\"  Memory: ${memory_cost:.2f}/month\")\n",
    "    print(f\"  Compute: ${compute_cost:.2f}/month\")\n",
    "    print(f\"  Total: ${total_monthly:.2f}/month\")\n",
    "    \n",
    "    return {\n",
    "        'storage_gb': total_storage/1024**3,\n",
    "        'memory_gb': recommended_memory/1024**3,\n",
    "        'monthly_cost': total_monthly\n",
    "    }\n",
    "\n",
    "# Discuss production considerations\n",
    "prod_considerations = discuss_production_deployment()\n",
    "resource_estimates = estimate_resource_requirements()\n",
    "\n",
    "print(\"\\nðŸŽ¯ Implementation Roadmap for Yale:\")\n",
    "print(\"1. Start with pilot deployment (subset of catalog)\")\n",
    "print(\"2. Benchmark performance with representative queries\")\n",
    "print(\"3. Optimize schema and indexing parameters\")\n",
    "print(\"4. Scale to full catalog with monitoring\")\n",
    "print(\"5. Integrate with existing library systems\")\n",
    "\n",
    "print(\"\\nâœ… Production planning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Vector Databases in the Modern AI Stack\n",
    "\n",
    "ðŸŽ“ **What We've Learned:**\n",
    "\n",
    "1. **Vector Database Fundamentals**: Why traditional databases can't handle similarity search at scale\n",
    "2. **Weaviate Architecture**: How HNSW indexing enables fast similarity search\n",
    "3. **Schema Design**: Proper data modeling for library metadata\n",
    "4. **Indexing Strategies**: Batch processing and automatic embedding generation\n",
    "5. **Query Patterns**: From basic similarity search to advanced filtering\n",
    "6. **Production Deployment**: Real-world considerations for enterprise use\n",
    "\n",
    "ðŸ”§ **Key Technical Insights:**\n",
    "\n",
    "- **Two-Client Architecture**: OpenAI for embeddings, Weaviate for search\n",
    "- **Automatic Embedding**: Weaviate can integrate directly with OpenAI's API\n",
    "- **Scalability**: Reduces O(nÂ²) comparison problem to O(kÃ—m)\n",
    "- **Flexibility**: Multiple query types (text, vector, hybrid) for different use cases\n",
    "\n",
    "ðŸš€ **Entity Resolution Impact:**\n",
    "\n",
    "Vector databases like Weaviate transform entity resolution from a computationally intractable problem into a practical, scalable solution. For Yale Library's 17.6 million records, this technology makes the difference between months of computation and hours of processing.\n",
    "\n",
    "ðŸ”œ **Next Steps:**\n",
    "\n",
    "- **Classification Techniques**: SetFit and Mistral AI Classifier Factory\n",
    "- **Feature Engineering**: Combining embedding similarity with domain knowledge\n",
    "- **Production Pipelines**: Building robust, monitored systems for continuous operation\n",
    "\n",
    "---\n",
    "*This notebook demonstrates vector database concepts for the Yale AI Research Techniques Workshop on Text Embeddings and Classification.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
