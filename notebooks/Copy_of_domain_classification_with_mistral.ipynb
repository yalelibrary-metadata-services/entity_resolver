{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2fc4b16b85174c699053b6ae9c8fe923": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a0343055691471f9eb7c807eab01c30",
       "IPY_MODEL_e820ed0d472e4a8197144442a8adec4e",
       "IPY_MODEL_a91c81a885174e63ab238c30ed06818f"
      ],
      "layout": "IPY_MODEL_55ac6b00ef7a474ab032797d51c4586c"
     }
    },
    "9a0343055691471f9eb7c807eab01c30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_264a25f60c3a415f9888df3914fd8dcd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0619f9437a5c4098b0ba3d8fbecb8c93",
      "value": "(â€¦)ibrary-entity-resolver-training-data.csv:â€‡"
     }
    },
    "e820ed0d472e4a8197144442a8adec4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e60e75be94ad46999d13e80c2a5831fd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eb536578f4834a338624cb8f533eee8f",
      "value": 1
     }
    },
    "a91c81a885174e63ab238c30ed06818f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72728391c14e4799b570c44eccd2fb8f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e4036c2a28104aa7b4087f35ac9e2382",
      "value": "â€‡1.99M/?â€‡[00:00&lt;00:00,â€‡51.5MB/s]"
     }
    },
    "55ac6b00ef7a474ab032797d51c4586c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "264a25f60c3a415f9888df3914fd8dcd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0619f9437a5c4098b0ba3d8fbecb8c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e60e75be94ad46999d13e80c2a5831fd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "eb536578f4834a338624cb8f533eee8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "72728391c14e4799b570c44eccd2fb8f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4036c2a28104aa7b4087f35ac9e2382": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "977d10ebe19c4092a884bd5430f935d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d6cfacc94802402097bccf1b1f999a51",
       "IPY_MODEL_7411570761b04470a7b217718c7640fb",
       "IPY_MODEL_6a56ac76cd764620b5c1df83f49e1f6f"
      ],
      "layout": "IPY_MODEL_a504162022074d8c9fd4a4c819b3d03f"
     }
    },
    "d6cfacc94802402097bccf1b1f999a51": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0b8b1ea2e9440109ffbbbf4cab7605e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6f6674fc90cc4b388f1a5388b105253f",
      "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
     }
    },
    "7411570761b04470a7b217718c7640fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f28e670560bd4269882ba820ab1724ae",
      "max": 2539,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed21e3d88fd64157b3ba857cd3f262d5",
      "value": 2539
     }
    },
    "6a56ac76cd764620b5c1df83f49e1f6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7634c739f26451b9933ad1096ae23bc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3a67af59955746b198fa30a7d0f64501",
      "value": "â€‡2539/2539â€‡[00:00&lt;00:00,â€‡32580.13â€‡examples/s]"
     }
    },
    "a504162022074d8c9fd4a4c819b3d03f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0b8b1ea2e9440109ffbbbf4cab7605e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f6674fc90cc4b388f1a5388b105253f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f28e670560bd4269882ba820ab1724ae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed21e3d88fd64157b3ba857cd3f262d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c7634c739f26451b9933ad1096ae23bc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a67af59955746b198fa30a7d0f64501": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Yale Entity Resolution: Domain Classification with Mistral AI\n\nThis notebook demonstrates how to use **Mistral AI's Classifier Factory** to solve real-world entity disambiguation challenges in library catalog systems. Using Yale University's 17.6M+ catalog records, we'll show how domain classification helps distinguish between entities with identical names but different fields of activity.\n\n## Learning Objectives\n\nBy the end of this notebook, you will understand:\n1. **Multi-label domain classification**: How to categorize entities across multiple academic and professional domains\n2. **Mistral Classifier Factory**: Using state-of-the-art language models for fine-tuned classification tasks  \n3. **Entity disambiguation pipeline**: How domain classification resolves the \"Franz Schubert problem\" (composer vs photographer)\n4. **Production deployment**: Building classification systems that scale to millions of library records\n\n## Real-World Challenge: The Franz Schubert Problem\n\nConsider these two Yale catalog records:\n- **Franz Schubert** (1797-1828): Famous Austrian composer of classical music\n- **Franz Schubert** (20th century): German photographer specializing in archaeological documentation\n\nWithout domain classification, entity resolution systems cannot distinguish between these fundamentally different people who happen to share the same name. This notebook shows how Mistral AI helps solve this challenge using semantic understanding of catalog metadata.\n\n## Yale's Domain Taxonomy\n\nOur classification system uses a hierarchical taxonomy with:\n- **Parent Categories**: Broad academic divisions (Arts, Sciences, Humanities, etc.)\n- **Specific Domains**: Detailed fields like \"Music, Sound, and Sonic Arts\" vs \"Documentary and Technical Arts\"\n- **Multi-label Support**: Entities can belong to multiple domains (interdisciplinary work)\n\nThis approach enables precise entity disambiguation while maintaining the flexibility needed for complex academic and cultural collections.",
   "metadata": {
    "id": "r1C8n6m-0c6R"
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 1: Install Required Libraries\n\nWe need several specialized libraries for this domain classification pipeline:\n\n- **`mistralai`**: Access to Mistral's Classifier Factory for fine-tuning language models on custom datasets\n- **`datasets`**: Hugging Face library for loading and managing Yale's training data from their public repository  \n- **`wandb`**: Weights & Biases for experiment tracking during model training (integrated with Mistral)\n- **`weaviate-client`**: Vector database client for semantic search and similarity operations\n\nThese tools work together to create a production-ready classification system that can process millions of catalog records efficiently.",
   "metadata": {
    "id": "pox6cLQn1ypQ"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Install required packages\n",
    "!pip install mistralai pandas matplotlib seaborn wandb datasets==3.2.0 weaviate-client"
   ],
   "metadata": {
    "id": "KZuzgP5aXN-Q",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "ccdf6ba6-3d68-40c9-ebfe-82fba4eb3b24"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting mistralai\n",
      "  Downloading mistralai-1.9.1-py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
      "Collecting datasets==3.2.0\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting weaviate-client\n",
      "  Downloading weaviate_client-4.15.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (0.70.15)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.2.0)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (0.33.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.2.0) (6.0.2)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.11.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
      "Collecting validators==0.34.0 (from weaviate-client)\n",
      "  Downloading validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting authlib<2.0.0,>=1.2.1 (from weaviate-client)\n",
      "  Downloading authlib-1.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.66.2 in /usr/local/lib/python3.11/dist-packages (from weaviate-client) (1.73.0)\n",
      "Collecting grpcio-tools<2.0.0,>=1.66.2 (from weaviate-client)\n",
      "  Downloading grpcio_tools-1.73.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting grpcio-health-checking<2.0.0,>=1.66.2 (from weaviate-client)\n",
      "  Downloading grpcio_health_checking-1.73.1-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting deprecation<3.0.0,>=2.1.0 (from weaviate-client)\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (43.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.2.0) (1.20.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting grpcio<2.0.0,>=1.66.2 (from weaviate-client)\n",
      "  Downloading grpcio-1.73.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools<2.0.0,>=1.66.2->weaviate-client) (75.2.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets==3.2.0) (1.1.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (2.33.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.2.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.2.0) (2.4.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.22)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mistralai-1.9.1-py3-none-any.whl (381 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m381.8/381.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading weaviate_client-4.15.4-py3-none-any.whl (432 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m433.0/433.0 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading validators-0.34.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading authlib-1.6.0-py2.py3-none-any.whl (239 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_health_checking-1.73.1-py3-none-any.whl (18 kB)\n",
      "Downloading grpcio-1.73.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_tools-1.73.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: validators, protobuf, grpcio, fsspec, eval-type-backport, deprecation, grpcio-tools, grpcio-health-checking, mistralai, authlib, weaviate-client, datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.5\n",
      "    Uninstalling protobuf-5.29.5:\n",
      "      Successfully uninstalled protobuf-5.29.5\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.73.0\n",
      "    Uninstalling grpcio-1.73.0:\n",
      "      Successfully uninstalled grpcio-1.73.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 6.31.1 which is incompatible.\n",
      "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.31.1 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.9.0 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed authlib-1.6.0 datasets-3.2.0 deprecation-2.1.0 eval-type-backport-0.2.2 fsspec-2024.9.0 grpcio-1.73.1 grpcio-health-checking-1.73.1 grpcio-tools-1.73.1 mistralai-1.9.1 protobuf-6.31.1 validators-0.34.0 weaviate-client-4.15.4\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "datasets",
         "fsspec",
         "google"
        ]
       },
       "id": "9454ecde04f145e8892f659e4df603c8"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "fjlO7rK4mVS0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "FcNzuSP9mjci"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from openai import OpenAI\n",
    "from datasets import load_dataset\n",
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType, VectorDistances\n",
    "from weaviate.classes.query import MetadataQuery, Filter\n",
    "from weaviate.util import generate_uuid5\n",
    "from tqdm import tqdm\n",
    "RANDOM_SEED = 42"
   ],
   "metadata": {
    "id": "I_xTwuybWA2M"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 2: Configure API Keys and Authentication\n\nThis step sets up secure access to the services we'll use throughout the classification pipeline:\n\n- **Mistral AI**: For accessing the Classifier Factory, which uses the powerful `ministral-3b-latest` model specifically designed for custom classification tasks\n- **OpenAI**: Provides embeddings (`text-embedding-3-small`) used by our Weaviate vector database for semantic search\n- **Hugging Face**: Enables us to download Yale's pre-labeled training datasets directly from their public repository\n- **Weights & Biases**: Tracks our model training experiments, providing real-time metrics and performance monitoring\n- **Weaviate Cloud**: Vector database service for storing and querying entity embeddings at scale\n\nUsing Colab's secure `userdata` ensures our API keys remain protected while enabling full access to these production services.",
   "metadata": {
    "id": "ssfc9uc23Euc"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
    "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "os.environ[\"WCD_URL\"] = userdata.get('WCD_URL')\n",
    "os.environ[\"WCD_GRPC\"] = userdata.get('WCD_GRPC')\n",
    "os.environ[\"WCD_API_KEY\"] = userdata.get('WCD_API_KEY')"
   ],
   "metadata": {
    "id": "aI0wI4ai3crw"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3: Load Yale's Training Datasets\n\nWe'll work with two complementary datasets from Yale's entity resolution project:\n\n1. **Training Data**: 2,539 catalog records with bibliographic metadata including titles, subjects, authors, and publication details\n2. **Domain Classifications**: Hand-labeled domain assignments for each entity, created by Yale librarians and subject matter experts\n\n### Why These Datasets Matter\n\nYale's catalog represents one of the world's largest academic collections, with over 17.6 million records spanning centuries of human knowledge. The training data includes challenging disambiguation cases like:\n\n- **Franz Schubert**: Composer vs photographer with identical names\n- **Jean Roberts**: Medical researcher vs literary scholar vs political writer  \n- **Cross-domain scholars**: Individuals active in multiple academic fields\n\nEach record contains rich contextual information (composite field) that combines title, subjects, and publication details - exactly the kind of semantic context that modern language models excel at understanding.",
   "metadata": {
    "id": "apHynNLI3mBY"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Connect to Weaviate\n",
    "weaviate_api_key = os.environ.get(\"WCD_API_KEY\")\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "weaviate_url = os.environ.get(\"WCD_URL\")\n",
    "\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=weaviate_url,\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(weaviate_api_key),\n",
    "    headers={\"X-OpenAI-Api-Key\": openai_api_key}  # For OpenAI vectorizer\n",
    ")\n",
    "\n",
    "print(\"âœ… Connected to OpenAI and Weaviate!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k97Vf_ECtWMj",
    "outputId": "5a5fc2aa-462f-4e3e-8243-87910dc5aa70"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Connected to OpenAI and Weaviate!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load from Hugging Face\n",
    "print(\"ğŸ“š Loading Yale dataset...\")\n",
    "training_data = pd.DataFrame(load_dataset(\"timathom/yale-library-entity-resolver-training-data\")[\"train\"])\n",
    "\n",
    "print(f\"âœ… Loaded {len(training_data):,} records\")\n",
    "print(f\"   Sample: {training_data.iloc[0]['person']} - {training_data.iloc[0]['title'][:50]}...\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133,
     "referenced_widgets": [
      "2fc4b16b85174c699053b6ae9c8fe923",
      "9a0343055691471f9eb7c807eab01c30",
      "e820ed0d472e4a8197144442a8adec4e",
      "a91c81a885174e63ab238c30ed06818f",
      "55ac6b00ef7a474ab032797d51c4586c",
      "264a25f60c3a415f9888df3914fd8dcd",
      "0619f9437a5c4098b0ba3d8fbecb8c93",
      "e60e75be94ad46999d13e80c2a5831fd",
      "eb536578f4834a338624cb8f533eee8f",
      "72728391c14e4799b570c44eccd2fb8f",
      "e4036c2a28104aa7b4087f35ac9e2382",
      "977d10ebe19c4092a884bd5430f935d5",
      "d6cfacc94802402097bccf1b1f999a51",
      "7411570761b04470a7b217718c7640fb",
      "6a56ac76cd764620b5c1df83f49e1f6f",
      "a504162022074d8c9fd4a4c819b3d03f",
      "d0b8b1ea2e9440109ffbbbf4cab7605e",
      "6f6674fc90cc4b388f1a5388b105253f",
      "f28e670560bd4269882ba820ab1724ae",
      "ed21e3d88fd64157b3ba857cd3f262d5",
      "c7634c739f26451b9933ad1096ae23bc",
      "3a67af59955746b198fa30a7d0f64501"
     ]
    },
    "id": "vcrm_JOjx4Kn",
    "outputId": "dee61c89-ad2a-4f35-e310-af17081bbcf7"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ“š Loading Yale dataset...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "(â€¦)ibrary-entity-resolver-training-data.csv: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fc4b16b85174c699053b6ae9c8fe923"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/2539 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "977d10ebe19c4092a884bd5430f935d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Loaded 2,539 records\n",
      "   Sample: Schubert, Franz - ArchaÌˆologie und Photographie: fuÌˆnfzig Beispiele ...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Real Yale entity resolution training data from training_dataset_classified_2025-06-25.csv\n",
    "yale_catalog_records = [\n",
    "    # Franz Schubert - Photographer (Documentary Arts)\n",
    "    {\n",
    "        \"identity\": \"9.1\",\n",
    "        \"personId\": \"53144#Agent700-22\",\n",
    "        \"recordId\": \"53144\",\n",
    "        \"person\": \"Schubert, Franz\",\n",
    "        \"composite\": \"\"\"Title: ArchÃ¤ologie und Photographie: fÃ¼nfzig Beispiele zur Geschichte und Methode\n",
    "Subjects: Photography in archaeology\n",
    "Provision information: Mainz: P. von Zabern, 1978\"\"\",\n",
    "        \"title\": \"ArchÃ¤ologie und Photographie: fÃ¼nfzig Beispiele zur Geschichte und Methode\",\n",
    "        \"subjects\": \"Photography in archaeology\",\n",
    "        \"roles\": \"Contributor\",\n",
    "        \"domain\": \"Documentary and Technical Arts\",\n",
    "        \"marcKey\": \"7001 $aSchubert, Franz.\"\n",
    "    },\n",
    "    # Franz Schubert - Composer (Music Arts)\n",
    "    {\n",
    "        \"identity\": \"9.0\",\n",
    "        \"personId\": \"772230#Agent100-15\",\n",
    "        \"recordId\": \"772230\",\n",
    "        \"person\": \"Schubert, Franz, 1797-1828\",\n",
    "        \"composite\": \"\"\"Title: Quartette fÃ¼r zwei Violinen, Viola, Violoncell\n",
    "Subjects: String quartets--Scores\n",
    "Provision information: Leipzig: C.F. Peters, [19--?]; Partitur\"\"\",\n",
    "        \"title\": \"Quartette fÃ¼r zwei Violinen, Viola, Violoncell\",\n",
    "        \"subjects\": \"String quartets--Scores\",\n",
    "        \"roles\": \"Contributor\",\n",
    "        \"domain\": \"Music, Sound, and Sonic Arts\",\n",
    "        \"marcKey\": \"1001 $aSchubert, Franz,$d1797-1828.\"\n",
    "    },\n",
    "    # Jean Roberts - Medical Researcher (rich subjects for imputation)\n",
    "    {\n",
    "        \"identity\": \"4559.0\",\n",
    "        \"personId\": \"14561127#Agent700-35\",\n",
    "        \"recordId\": \"14561127\",\n",
    "        \"person\": \"Roberts, Jean, 1918-\",\n",
    "        \"composite\": \"\"\"Title: Skin conditions and related need for medical care among persons 1-74 years, United States, 1971-1974\n",
    "Subjects: Skin--Diseases--United States--Statistics; Health surveys--United States; Health surveys; Skin--Diseases; United States\n",
    "Genres: Statistics\n",
    "Provision information: Hyattsville, Md: U.S. Department of Health, Education, and Welfare, Public Health Service, Office of the Assistant Secretary for Health, National Center for Health Statistics, 1978\"\"\",\n",
    "        \"title\": \"Skin conditions and related need for medical care among persons 1-74 years, United States, 1971-1974\",\n",
    "        \"subjects\": \"Skin--Diseases--United States--Statistics; Health surveys--United States; Health surveys; Skin--Diseases; United States\",\n",
    "        \"roles\": \"Author\",\n",
    "        \"domain\": \"Medicine, Health, and Clinical Sciences\",\n",
    "        \"marcKey\": \"7001 $aRoberts, Jean,$d1918-$eauthor.\"\n",
    "    },\n",
    "    # Jean Roberts - Literary Scholar (missing subjects - needs imputation!)\n",
    "    {\n",
    "        \"identity\": \"4559.2\",\n",
    "        \"personId\": \"1340596#Agent100-17\",\n",
    "        \"recordId\": \"1340596\",\n",
    "        \"person\": \"Roberts, Jean\",\n",
    "        \"composite\": \"\"\"Title: Henrik Ibsen's \"Peer Gynt\": introduction\n",
    "Subjects: Ibsen, Henrik, 1828-1906. Peer Gynt; Campbell, Duine--Autograph; Roberts, Jean--Autograph\n",
    "Provision information: [Leicester]: Offcut Private Press, June 26th, 1972\"\"\",\n",
    "        \"title\": \"Henrik Ibsen's \\\"Peer Gynt\\\": introduction\",\n",
    "        \"subjects\": \"Ibsen, Henrik, 1828-1906. Peer Gynt; Campbell, Duine--Autograph; Roberts, Jean--Autograph\",\n",
    "        \"roles\": \"Contributor\",\n",
    "        \"domain\": \"Literature and Narrative Arts\",\n",
    "        \"marcKey\": \"1001 $aRoberts, Jean.\"\n",
    "    },\n",
    "    # Jean Roberts - Political Writer (also missing subjects)\n",
    "    {\n",
    "        \"identity\": \"4559.1\",\n",
    "        \"personId\": \"2845991#Agent700-18\",\n",
    "        \"recordId\": \"2845991\",\n",
    "        \"person\": \"Roberts, J.E.\",\n",
    "        \"composite\": \"\"\"Title: The wise men of Kansas\n",
    "Subjects: Silver question\n",
    "Provision information: [Kansas City? Mo.]: [c1896]\"\"\",\n",
    "        \"title\": \"The wise men of Kansas\",\n",
    "        \"subjects\": \"Silver question\",\n",
    "        \"roles\": \"Contributor\",\n",
    "        \"domain\": \"Politics, Policy, and Government\",\n",
    "        \"marcKey\": \"7001 $aRoberts, J.E.\"\n",
    "    },\n",
    "    # Example record with missing subjects (for imputation demo)\n",
    "    {\n",
    "        \"identity\": \"demo_missing\",\n",
    "        \"personId\": \"demo#Agent100-99\",\n",
    "        \"recordId\": \"demo999\",\n",
    "        \"person\": \"Roberts, Jean\",\n",
    "        \"composite\": \"\"\"Title: Literary analysis techniques in modern drama criticism\n",
    "Provision information: London: Academic Press, 1975\"\"\",\n",
    "        \"title\": \"Literary analysis techniques in modern drama criticism\",\n",
    "        \"subjects\": None,  # Missing - we'll impute this!\n",
    "        \"roles\": \"Author\",\n",
    "        \"domain\": None,\n",
    "        \"marcKey\": \"1001 $aRoberts, Jean.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(yale_catalog_records)\n",
    "print(\"ğŸ“š Real Yale catalog data loaded:\")\n",
    "print(df[['personId', 'person', 'domain', 'title']].to_string())\n",
    "print(f\"\\nğŸ” Records with missing subjects: {df['subjects'].isna().sum()}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMLhT_WnvBcN",
    "outputId": "aeb1e049-6020-4e4b-9212-583efc5e7eeb"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ“š Real Yale catalog data loaded:\n",
      "               personId                      person                                   domain                                                                                                 title\n",
      "0     53144#Agent700-22             Schubert, Franz           Documentary and Technical Arts                            ArchÃ¤ologie und Photographie: fÃ¼nfzig Beispiele zur Geschichte und Methode\n",
      "1    772230#Agent100-15  Schubert, Franz, 1797-1828             Music, Sound, and Sonic Arts                                                        Quartette fÃ¼r zwei Violinen, Viola, Violoncell\n",
      "2  14561127#Agent700-35        Roberts, Jean, 1918-  Medicine, Health, and Clinical Sciences  Skin conditions and related need for medical care among persons 1-74 years, United States, 1971-1974\n",
      "3   1340596#Agent100-17               Roberts, Jean            Literature and Narrative Arts                                                              Henrik Ibsen's \"Peer Gynt\": introduction\n",
      "4   2845991#Agent700-18               Roberts, J.E.         Politics, Policy, and Government                                                                                The wise men of Kansas\n",
      "5      demo#Agent100-99               Roberts, Jean                                     None                                                Literary analysis techniques in modern drama criticism\n",
      "\n",
      "ğŸ” Records with missing subjects: 1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_embedding(text: str, model: str = \"text-embedding-3-small\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Yale's production embedding function from embedding_and_indexing.py\n",
    "\n",
    "    Args:\n",
    "        text: Input text to embed\n",
    "        model: OpenAI embedding model (text-embedding-3-small)\n",
    "\n",
    "    Returns:\n",
    "        1536-dimensional embedding vector\n",
    "    \"\"\"\n",
    "    if not text or text.strip() == \"\":\n",
    "        # Return zero vector for empty text\n",
    "        return np.zeros(1536, dtype=np.float32)\n",
    "\n",
    "    try:\n",
    "        response = openai_client.embeddings.create(\n",
    "            model=model,\n",
    "            input=text\n",
    "        )\n",
    "\n",
    "        # Extract embedding from response\n",
    "        embedding = np.array(response.data[0].embedding, dtype=np.float32)\n",
    "        return embedding\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating embedding: {e}\")\n",
    "        return np.zeros(1536, dtype=np.float32)\n",
    "\n",
    "# Test the embedding function with real Yale data\n",
    "test_composite = training_data.iloc[0]['composite']\n",
    "test_embedding = generate_embedding(test_composite)\n",
    "print(f\"âœ… Embedding generated successfully! Shape: {test_embedding.shape}\")\n",
    "print(f\"   Sample values: {test_embedding[:5]}\")\n",
    "print(f\"   Composite text: {test_composite[:80]}...\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98OuAkB9vzr_",
    "outputId": "35baf21b-63c4-4a6f-ddb8-d032806fb6dc"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Embedding generated successfully! Shape: (1536,)\n",
      "   Sample values: [ 0.01115062  0.02462124 -0.0213398   0.00958305 -0.04418446]\n",
      "   Composite text: Title: ArchaÌˆologie und Photographie: fuÌˆnfzig Beispiele zur Geschichte und Meth...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def create_entity_schema(client):\n",
    "    \"\"\"\n",
    "    Create EntityString schema\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if collection already exists\n",
    "        # Delete existing collection if it exists\n",
    "        if client.collections.exists(\"EntityString\"):\n",
    "            client.collections.delete(\"EntityString\")\n",
    "            print(\"ğŸ—‘ï¸ Deleted existing EntityString collection\")\n",
    "\n",
    "        # Create with exact production schema from embedding_and_indexing.py + metadata for imputation\n",
    "        collection = client.collections.create(\n",
    "            name=\"EntityString\",\n",
    "            description=\"Collection for entity string values with their embeddings\",\n",
    "            vectorizer_config=Configure.Vectorizer.text2vec_openai(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                dimensions=1536\n",
    "            ),\n",
    "            vector_index_config=Configure.VectorIndex.hnsw(\n",
    "                ef=128,                    # Production config\n",
    "                max_connections=64,        # Production config\n",
    "                ef_construction=128,       # Production config\n",
    "                distance_metric=VectorDistances.COSINE\n",
    "            ),\n",
    "            properties=[\n",
    "                # Exact production schema\n",
    "                Property(name=\"original_string\", data_type=DataType.TEXT),\n",
    "                Property(name=\"hash_value\", data_type=DataType.TEXT),\n",
    "                Property(name=\"field_type\", data_type=DataType.TEXT),\n",
    "                Property(name=\"frequency\", data_type=DataType.INT),\n",
    "                # Added for subject imputation demo\n",
    "                Property(name=\"personId\", data_type=DataType.TEXT),\n",
    "                Property(name=\"recordId\", data_type=DataType.TEXT)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\"âœ… Created EntityString collection with schema\")\n",
    "        return collection\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating schema: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create the schema\n",
    "entity_collection = create_entity_schema(weaviate_client)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SGZ6sf6xI-S",
    "outputId": "7e8cf92e-cd84-44bb-fc34-b079bdcd4d78"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ—‘ï¸ Deleted existing EntityString collection\n",
      "âœ… Created EntityString collection with schema\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_hash(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate SHA-256 hash for text (Yale's production method)\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return \"NULL\"\n",
    "    return hashlib.sha256(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Generate hashes for all fields using Yale's production method\n",
    "print(\"ğŸ” Generating SHA-256 hashes for all records...\")\n",
    "\n",
    "for i, row in training_data.iterrows():\n",
    "    # Generate hashes for each field type (Yale's approach)\n",
    "    person_hash = generate_hash(row['person'])\n",
    "    composite_hash = generate_hash(row['composite'])\n",
    "    title_hash = generate_hash(row['title'])\n",
    "    subjects_hash = generate_hash(row['subjects']) if pd.notna(row['subjects']) else \"NULL\"\n",
    "\n",
    "    # Store in dataframe\n",
    "    training_data.at[i, 'person_hash'] = person_hash\n",
    "    training_data.at[i, 'composite_hash'] = composite_hash\n",
    "    training_data.at[i, 'title_hash'] = title_hash\n",
    "    training_data.at[i, 'subjects_hash'] = subjects_hash\n",
    "\n",
    "print(\"âœ… Generated SHA-256 hashes for all records\")\n",
    "print(f\"   Sample person hash: {training_data.iloc[0]['person_hash'][:16]}...\")\n",
    "print(f\"   Sample composite hash: {training_data.iloc[0]['composite_hash'][:16]}...\")\n",
    "\n",
    "# Show hash distribution\n",
    "print(f\"\\nğŸ“Š Hash Statistics:\")\n",
    "print(f\"   Unique person hashes: {training_data['person_hash'].nunique()}\")\n",
    "print(f\"   Unique composite hashes: {training_data['composite_hash'].nunique()}\")\n",
    "print(f\"   NULL subjects hashes: {(training_data['subjects_hash'] == 'NULL').sum()}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttw4P6og2Bqt",
    "outputId": "d0ad6af3-165e-4cd7-b9df-d86520c78692"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ” Generating SHA-256 hashes for all records...\n",
      "âœ… Generated SHA-256 hashes for all records\n",
      "   Sample person hash: 6cb0f164412941e2...\n",
      "   Sample composite hash: 324648e06f268fed...\n",
      "\n",
      "ğŸ“Š Hash Statistics:\n",
      "   Unique person hashes: 189\n",
      "   Unique composite hashes: 2357\n",
      "   NULL subjects hashes: 351\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\\nğŸ”„ Deduplicating data for indexing...\")\n",
    "unique_objects = []\n",
    "\n",
    "# Process each field type separately to avoid duplicate UUIDs\n",
    "field_types = ['person', 'composite', 'title', 'subjects']\n",
    "\n",
    "for field_type in field_types:\n",
    "    print(f\"   Processing {field_type} field...\")\n",
    "\n",
    "    # Get hash and text columns\n",
    "    hash_col = f\"{field_type}_hash\"\n",
    "    text_col = field_type\n",
    "\n",
    "    # Skip if field doesn't exist\n",
    "    if text_col not in training_data.columns:\n",
    "        continue\n",
    "\n",
    "    # Filter out NULL hashes and get unique hash-text pairs with metadata\n",
    "    field_data = training_data[training_data[hash_col] != \"NULL\"][[hash_col, text_col, 'personId', 'recordId']].drop_duplicates(subset=[hash_col])\n",
    "\n",
    "    # Add to unique objects with personId and recordId for imputation\n",
    "    for _, row in field_data.iterrows():\n",
    "        unique_objects.append({\n",
    "            'hash_value': row[hash_col],\n",
    "            'original_string': str(row[text_col]),\n",
    "            'field_type': field_type,\n",
    "            'frequency': 1,  # Could be calculated if needed\n",
    "            'personId': str(row['personId']) if pd.notna(row['personId']) else \"\",\n",
    "            'recordId': str(row['recordId']) if pd.notna(row['recordId']) else \"\"\n",
    "        })\n",
    "\n",
    "print(f\"âœ… Created {len(unique_objects):,} unique objects for indexing\")\n",
    "\n",
    "# Show deduplication statistics\n",
    "field_counts = {}\n",
    "for obj in unique_objects:\n",
    "    field_type = obj['field_type']\n",
    "    field_counts[field_type] = field_counts.get(field_type, 0) + 1\n",
    "\n",
    "print(f\"\\nğŸ“Š Unique objects by field type:\")\n",
    "for field_type, count in field_counts.items():\n",
    "    print(f\"   {field_type}: {count:,}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQ_LtpJR7uA7",
    "outputId": "ff49da37-8914-4517-8c84-8d40155d9c1b"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ğŸ”„ Deduplicating data for indexing...\n",
      "   Processing person field...\n",
      "   Processing composite field...\n",
      "   Processing title field...\n",
      "   Processing subjects field...\n",
      "âœ… Created 6,111 unique objects for indexing\n",
      "\n",
      "ğŸ“Š Unique objects by field type:\n",
      "   person: 189\n",
      "   composite: 2,357\n",
      "   title: 1,966\n",
      "   subjects: 1,599\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "training_data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "CryYOsCe2Zf3",
    "outputId": "16b8e1fe-5088-418a-9c74-63923542f170"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   identity                                          composite  \\\n",
       "0       9.1  Title: ArchaÌˆologie und Photographie: fuÌˆnfzig...   \n",
       "1       9.0  Title: Quartette fuÌˆr zwei Violinen, Viola, Vi...   \n",
       "2       9.0  Title: Quartette fuÌˆr zwei Violinen, Viola, Vi...   \n",
       "3       9.0  Title: Der Hirt auf dem Felsen: nach Wilh. MuÌˆ...   \n",
       "4       9.0  Title: Quintett in A fuÌˆr Klavier, Violine, Vi...   \n",
       "\n",
       "                               marcKey                      person  \\\n",
       "0              7001 $aSchubert, Franz.             Schubert, Franz   \n",
       "1  1001 $aSchubert, Franz,$d1797-1828.  Schubert, Franz, 1797-1828   \n",
       "2  1001 $aSchubert, Franz,$d1797-1828.  Schubert, Franz, 1797-1828   \n",
       "3  1001 $aSchubert, Franz,$d1797-1828.  Schubert, Franz, 1797-1828   \n",
       "4  1001 $aSchubert, Franz,$d1797-1828.  Schubert, Franz, 1797-1828   \n",
       "\n",
       "         roles                                              title  \\\n",
       "0  Contributor  ArchaÌˆologie und Photographie: fuÌˆnfzig Beispi...   \n",
       "1  Contributor    Quartette fuÌˆr zwei Violinen, Viola, Violoncell   \n",
       "2  Contributor    Quartette fuÌˆr zwei Violinen, Viola, Violoncell   \n",
       "3  Contributor  Der Hirt auf dem Felsen: nach Wilh. MuÌˆllers G...   \n",
       "4  Contributor  Quintett in A fuÌˆr Klavier, Violine, Viola, Vi...   \n",
       "\n",
       "                                         attribution  \\\n",
       "0  ausgewaÌˆhlt von Franz Schubert und Susanne Gru...   \n",
       "1                                 von Franz Schubert   \n",
       "2                                 von Franz Schubert   \n",
       "3                                     Franz Schubert   \n",
       "4     Franz Schubert ; herausgegeben von Arnold Feil   \n",
       "\n",
       "                                     provision  \\\n",
       "0                   Mainz: P. von Zabern, 1978   \n",
       "1       Leipzig: C.F. Peters, [19--?] Partitur   \n",
       "2       Leipzig: C.F. Peters, [19--?] Partitur   \n",
       "3       Wiesbaden: Breitkopf & HaÌˆrtel, [19--]   \n",
       "4  Kassel; New York: BaÌˆrenreiter, 1987, c1975   \n",
       "\n",
       "                                            subjects genres  \\\n",
       "0                         Photography in archaeology   None   \n",
       "1                            String quartets--Scores   None   \n",
       "2                            String quartets--Scores   None   \n",
       "3  MuÌˆller, Wilhelm, 1794-1827; Songs (High voice...  Songs   \n",
       "4  Quintets (Piano, violin, viola, cello, double ...   None   \n",
       "\n",
       "                                       relatedWork  recordId  \\\n",
       "0                                             None     53144   \n",
       "1                                             None    772230   \n",
       "2  Quartets, violins (2), viola, cello. Selections    772230   \n",
       "3                                             None    666968   \n",
       "4                                             None    786540   \n",
       "\n",
       "                 personId                                        person_hash  \\\n",
       "0       53144#Agent700-22  6cb0f164412941e2dc71aaeda03a475f6b2b9422bc3b9f...   \n",
       "1      772230#Agent100-15  71cc57bae228d21e11cc583581e32ca275592c29549c7a...   \n",
       "2  772230#Hub240-16-Agent  71cc57bae228d21e11cc583581e32ca275592c29549c7a...   \n",
       "3      666968#Agent100-16  71cc57bae228d21e11cc583581e32ca275592c29549c7a...   \n",
       "4      786540#Agent100-16  71cc57bae228d21e11cc583581e32ca275592c29549c7a...   \n",
       "\n",
       "                                      composite_hash  \\\n",
       "0  324648e06f268fed271ca1538b0348e41c6ef387eaa8ca...   \n",
       "1  8c79ba57383510bd5bc24da3e082bb72e54ef38f5c94ef...   \n",
       "2  6d24c4811bf8e64c9917b21e9dd636f02aaf92407f2bb1...   \n",
       "3  54ea0f1d21cfd01163fe5d489588df298bc841f5dbdde0...   \n",
       "4  8a73e2f764e10351761407869a9eb8cb981cf35ba777c7...   \n",
       "\n",
       "                                          title_hash  \\\n",
       "0  db2db2b2b53b9ec2965192ee93769a30212f070f091fdf...   \n",
       "1  347762c310bbf840b99294d0deeda46aeaff2ad8278973...   \n",
       "2  347762c310bbf840b99294d0deeda46aeaff2ad8278973...   \n",
       "3  eeaf3b4006575d9ad07439336ca175fd7b7148909a2a30...   \n",
       "4  1ff138d7100c9f1f3db5a1d45a291a6a5d7b955be2f1b3...   \n",
       "\n",
       "                                       subjects_hash  \n",
       "0  40d3e3ee1b9a90f415443faf020f58faddc7aba584006d...  \n",
       "1  33a5a7fe06a8471ea06c499372bc31efad651a5059d914...  \n",
       "2  33a5a7fe06a8471ea06c499372bc31efad651a5059d914...  \n",
       "3  adf321cfacc01077064806fb37f16427792a5a604d8cf6...  \n",
       "4  b75cd0573999d4ca79bc8ab8eb31349c7d028a19894161...  "
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-8e96d918-1af5-4ef0-9eba-8da1d363832a\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity</th>\n",
       "      <th>composite</th>\n",
       "      <th>marcKey</th>\n",
       "      <th>person</th>\n",
       "      <th>roles</th>\n",
       "      <th>title</th>\n",
       "      <th>attribution</th>\n",
       "      <th>provision</th>\n",
       "      <th>subjects</th>\n",
       "      <th>genres</th>\n",
       "      <th>relatedWork</th>\n",
       "      <th>recordId</th>\n",
       "      <th>personId</th>\n",
       "      <th>person_hash</th>\n",
       "      <th>composite_hash</th>\n",
       "      <th>title_hash</th>\n",
       "      <th>subjects_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.1</td>\n",
       "      <td>Title: ArchaÌˆologie und Photographie: fuÌˆnfzig...</td>\n",
       "      <td>7001 $aSchubert, Franz.</td>\n",
       "      <td>Schubert, Franz</td>\n",
       "      <td>Contributor</td>\n",
       "      <td>ArchaÌˆologie und Photographie: fuÌˆnfzig Beispi...</td>\n",
       "      <td>ausgewaÌˆhlt von Franz Schubert und Susanne Gru...</td>\n",
       "      <td>Mainz: P. von Zabern, 1978</td>\n",
       "      <td>Photography in archaeology</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>53144</td>\n",
       "      <td>53144#Agent700-22</td>\n",
       "      <td>6cb0f164412941e2dc71aaeda03a475f6b2b9422bc3b9f...</td>\n",
       "      <td>324648e06f268fed271ca1538b0348e41c6ef387eaa8ca...</td>\n",
       "      <td>db2db2b2b53b9ec2965192ee93769a30212f070f091fdf...</td>\n",
       "      <td>40d3e3ee1b9a90f415443faf020f58faddc7aba584006d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Title: Quartette fuÌˆr zwei Violinen, Viola, Vi...</td>\n",
       "      <td>1001 $aSchubert, Franz,$d1797-1828.</td>\n",
       "      <td>Schubert, Franz, 1797-1828</td>\n",
       "      <td>Contributor</td>\n",
       "      <td>Quartette fuÌˆr zwei Violinen, Viola, Violoncell</td>\n",
       "      <td>von Franz Schubert</td>\n",
       "      <td>Leipzig: C.F. Peters, [19--?] Partitur</td>\n",
       "      <td>String quartets--Scores</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>772230</td>\n",
       "      <td>772230#Agent100-15</td>\n",
       "      <td>71cc57bae228d21e11cc583581e32ca275592c29549c7a...</td>\n",
       "      <td>8c79ba57383510bd5bc24da3e082bb72e54ef38f5c94ef...</td>\n",
       "      <td>347762c310bbf840b99294d0deeda46aeaff2ad8278973...</td>\n",
       "      <td>33a5a7fe06a8471ea06c499372bc31efad651a5059d914...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Title: Quartette fuÌˆr zwei Violinen, Viola, Vi...</td>\n",
       "      <td>1001 $aSchubert, Franz,$d1797-1828.</td>\n",
       "      <td>Schubert, Franz, 1797-1828</td>\n",
       "      <td>Contributor</td>\n",
       "      <td>Quartette fuÌˆr zwei Violinen, Viola, Violoncell</td>\n",
       "      <td>von Franz Schubert</td>\n",
       "      <td>Leipzig: C.F. Peters, [19--?] Partitur</td>\n",
       "      <td>String quartets--Scores</td>\n",
       "      <td>None</td>\n",
       "      <td>Quartets, violins (2), viola, cello. Selections</td>\n",
       "      <td>772230</td>\n",
       "      <td>772230#Hub240-16-Agent</td>\n",
       "      <td>71cc57bae228d21e11cc583581e32ca275592c29549c7a...</td>\n",
       "      <td>6d24c4811bf8e64c9917b21e9dd636f02aaf92407f2bb1...</td>\n",
       "      <td>347762c310bbf840b99294d0deeda46aeaff2ad8278973...</td>\n",
       "      <td>33a5a7fe06a8471ea06c499372bc31efad651a5059d914...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Title: Der Hirt auf dem Felsen: nach Wilh. MuÌˆ...</td>\n",
       "      <td>1001 $aSchubert, Franz,$d1797-1828.</td>\n",
       "      <td>Schubert, Franz, 1797-1828</td>\n",
       "      <td>Contributor</td>\n",
       "      <td>Der Hirt auf dem Felsen: nach Wilh. MuÌˆllers G...</td>\n",
       "      <td>Franz Schubert</td>\n",
       "      <td>Wiesbaden: Breitkopf &amp; HaÌˆrtel, [19--]</td>\n",
       "      <td>MuÌˆller, Wilhelm, 1794-1827; Songs (High voice...</td>\n",
       "      <td>Songs</td>\n",
       "      <td>None</td>\n",
       "      <td>666968</td>\n",
       "      <td>666968#Agent100-16</td>\n",
       "      <td>71cc57bae228d21e11cc583581e32ca275592c29549c7a...</td>\n",
       "      <td>54ea0f1d21cfd01163fe5d489588df298bc841f5dbdde0...</td>\n",
       "      <td>eeaf3b4006575d9ad07439336ca175fd7b7148909a2a30...</td>\n",
       "      <td>adf321cfacc01077064806fb37f16427792a5a604d8cf6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Title: Quintett in A fuÌˆr Klavier, Violine, Vi...</td>\n",
       "      <td>1001 $aSchubert, Franz,$d1797-1828.</td>\n",
       "      <td>Schubert, Franz, 1797-1828</td>\n",
       "      <td>Contributor</td>\n",
       "      <td>Quintett in A fuÌˆr Klavier, Violine, Viola, Vi...</td>\n",
       "      <td>Franz Schubert ; herausgegeben von Arnold Feil</td>\n",
       "      <td>Kassel; New York: BaÌˆrenreiter, 1987, c1975</td>\n",
       "      <td>Quintets (Piano, violin, viola, cello, double ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>786540</td>\n",
       "      <td>786540#Agent100-16</td>\n",
       "      <td>71cc57bae228d21e11cc583581e32ca275592c29549c7a...</td>\n",
       "      <td>8a73e2f764e10351761407869a9eb8cb981cf35ba777c7...</td>\n",
       "      <td>1ff138d7100c9f1f3db5a1d45a291a6a5d7b955be2f1b3...</td>\n",
       "      <td>b75cd0573999d4ca79bc8ab8eb31349c7d028a19894161...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e96d918-1af5-4ef0-9eba-8da1d363832a')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8e96d918-1af5-4ef0-9eba-8da1d363832a button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8e96d918-1af5-4ef0-9eba-8da1d363832a');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-3c87bbaa-7ba7-4e68-8319-d3767eb3ef4b\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c87bbaa-7ba7-4e68-8319-d3767eb3ef4b')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-3c87bbaa-7ba7-4e68-8319-d3767eb3ef4b button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "training_data",
       "summary": "{\n  \"name\": \"training_data\",\n  \"rows\": 2539,\n  \"fields\": [\n    {\n      \"column\": \"identity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1254001.372415529,\n        \"min\": 9.0,\n        \"max\": 45014501.0,\n        \"num_unique_values\": 263,\n        \"samples\": [\n          449.2,\n          571.6,\n          4560.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2357,\n        \"samples\": [\n          \"Title: Judah P. Benjamin, statesman of the lost cause\\nGenres: Biographies\\nProvision information: New York; London: G. P. Putnam's sons, 1933\",\n          \"Title: A visit to the colony of Harmony, in Indiana ... recently purchased by Mr. Owen for the establishment of a society of mutual co-operation and community of property: ... to which are added, some observations on that mode of society, and on political society at large, also, a sketch for the formation of a co-operative society\\nSubjects: Socialism--Indiana--Harmony; Cooperative societies--Indiana--Harmony\\nProvision information: London: Printed for G. Mann, 1825; [London]: Plummer and Brewis\",\n          \"Title: Speech of the Hon. John M. Clayton, at the Delaware Mass Whig Convention, held at Wilmington, June 15, 1844\\nSubjects: Tariff--United States; Protectionism; Amer tracts--1844\\nProvision information: [New York?]: [s.n.]; Albany: [For sale at the office of the Albany evening journal], [1844?]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"marcKey\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 921,\n        \"samples\": [\n          \"1001 $aJoyce, James,$d1882-1941,$eauthor.\",\n          \"1001 $aHoadly, Benjamin,$eauthor.\",\n          \"1000 $aLucan,$d39-65,$eauthor.$0http://id.loc.gov/authorities/names/n79089234\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"person\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 189,\n        \"samples\": [\n          \"West, Benjamin, Jr\",\n          \"Clay, Henry\",\n          \"Zhu, Xi, 1130-1200 \\u6731\\u71b9, 1130-1200\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roles\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Performer\",\n          \"Illustrator\",\n          \"Speaker\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1966,\n        \"samples\": [\n          \"Part I of the new existence of man upon the earth: to which are added an outline of Mr. Owen's early life, and an appendix, containing his addresses, &c. published in 1815 & 1817\",\n          \"The biter: A comedy\",\n          \"Mountain interval\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"attribution\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1031,\n        \"samples\": [\n          \"John Michael Pyne\",\n          \"[by] Schubert ...\",\n          \"By John Stancliff, Minister of the Gospel. ; [One line from I. Kings]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"provision\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1805,\n        \"samples\": [\n          \"San Francisco, Calif: Ithuriel's Spear, c2010\",\n          \"Oxford: John Henry Parker, 1840\",\n          \"[London]: [s.n.], printed in the yeare of our prelates feare 1641\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subjects\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1599,\n        \"samples\": [\n          \"Orozco, Gabriel, 1962---Exhibitions; Artists--Mexico--Exhibitions; Orozco, Gabriel, 1962-; Artists; Mexico\",\n          \"Early rising--Sermons; Sermons, English--18th century; Bible. N.T. Ephesians V, 16--Sermons\",\n          \"Grace (Theology)--Early works to 1800\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"genres\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 234,\n        \"samples\": [\n          \"Sheet music; Publisher's advertisements; Songs; Popular music; Scores\",\n          \"Statistics; Memorials and Petitions\",\n          \"Pictures\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relatedWork\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 308,\n        \"samples\": [\n          \"Metamorphosen\",\n          \"Shi ji \\u53f2\\u8a18\",\n          \"Address to the society. 1819.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recordId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5224828,\n        \"min\": 1540,\n        \"max\": 16254606,\n        \"num_unique_values\": 2048,\n        \"samples\": [\n          7299945,\n          14707592,\n          587146\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"personId\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2539,\n        \"samples\": [\n          \"9148325#Agent600-22\",\n          \"6767589#Agent600-20\",\n          \"11972272#Hub240-13-Agent\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"person_hash\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 189,\n        \"samples\": [\n          \"fdcffa8d7ce76b188a4933a31fe5845a2aab69b52f936a8ea28174ee6d5c8889\",\n          \"cc1e005b94dbff7a0a68d723addd125f05c2d07cf714a8e9972bb5029f85d6b9\",\n          \"49299463a182d75330f9b067d6bfd11477f664d985fdc64cde6cbbd38893be53\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_hash\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2357,\n        \"samples\": [\n          \"e70d3c66de8b70b73e49d4a5f9aab005669fc07cec74e7449f9155c222391d41\",\n          \"5ef832a7cd1c4c851a18910086817c24e829b46925c632c8a9913395a380b1ab\",\n          \"9b19efd5757cdf81619a8444bbf43b30592a55258f8c850831a0ad247d474e70\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title_hash\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1966,\n        \"samples\": [\n          \"c2144d799c0dac3adfc7b538bb145171d807a51b93086663488f6563baa0da8a\",\n          \"d2d9f7c674a29f1b7658d3fc9d9a0644c0b4519f7cc64873c06bc7d8e8d0e8e5\",\n          \"655914d18b98f90a74e0ff31f7f18eff64705addbc812129203e1f53fe52ba6f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subjects_hash\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1600,\n        \"samples\": [\n          \"649aca3d8ab1a3bdebac3c53cfd09192bfaea5b640a82102d6c70500d2b2284e\",\n          \"2ea97e88c02ddb7093628d120d9f7f5114bf8955ba877cafa232dde64a422c78\",\n          \"c533bbcf9c8e197c6474de8041f0bb882fccdc721425a00731f8e844ff58a4c8\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
      }
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def index_entities(collection, dataframe):\n",
    "    \"\"\"\n",
    "    Index Yale entity strings in Weaviate\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ Indexing Yale entity strings in Weaviate...\")\n",
    "\n",
    "    indexed_count = 0\n",
    "    batch_size = 100\n",
    "\n",
    "    print(\"ğŸš€ Indexing deduplicated data...\")\n",
    "\n",
    "    with collection.batch.dynamic() as batch:\n",
    "        for obj in tqdm(unique_objects, desc=\"Indexing unique objects\"):\n",
    "            try:\n",
    "                # Generate UUID using production method (hash + field_type)\n",
    "                uuid_input = f\"{obj['hash_value']}_{obj['field_type']}\"\n",
    "                uuid = generate_uuid5(uuid_input)\n",
    "\n",
    "                # Add to batch\n",
    "                batch.add_object(\n",
    "                    uuid=uuid,\n",
    "                    properties={\n",
    "                        \"original_string\": obj['original_string'],\n",
    "                        \"hash_value\": obj['hash_value'],\n",
    "                        \"field_type\": obj['field_type'],\n",
    "                        \"frequency\": obj['frequency'],\n",
    "                        \"personId\": obj['personId'],\n",
    "                        \"recordId\": obj['recordId']\n",
    "                    }\n",
    "                )\n",
    "                indexed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error indexing {obj['field_type']}: {e}\")\n",
    "\n",
    "    print(f\"âœ… Successfully indexed {indexed_count:,} unique objects\")\n",
    "\n",
    "    return indexed_count\n",
    "\n",
    "# Index our real Yale data\n",
    "indexed_count = index_entities(entity_collection, training_data)\n",
    "\n",
    "# Verify indexing\n",
    "print(f\"\\nğŸ” Verification:\")\n",
    "print(f\"   Expected records: {len(training_data) * 3 + training_data['subjects'].notna().sum()}\")  # person + composite + title + subjects (if not null)\n",
    "print(f\"   Actually indexed: {indexed_count}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shQiZiqgyGJs",
    "outputId": "d2eb9071-7a15-4ad4-d221-0445235c893f"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ”„ Indexing Yale entity strings in Weaviate...\n",
      "ğŸš€ Indexing deduplicated data...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Indexing unique objects: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6111/6111 [00:15<00:00, 400.08it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "âœ… Successfully indexed 6,111 unique objects\n",
      "\n",
      "ğŸ” Verification:\n",
      "   Expected records: 9805\n",
      "   Actually indexed: 6111\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Test semantic search\n",
    "print(\"ğŸ” Testing semantic search...\")\n",
    "query = \"classical compositions\"\n",
    "\n",
    "# Search\n",
    "search_results = entity_collection.query.near_text(\n",
    "    query=query,\n",
    "    limit=5,\n",
    "    return_properties=[\"original_string\", \"field_type\", \"hash_value\"],\n",
    "    return_metadata=[\"distance\"]\n",
    ")\n",
    "\n",
    "print(f'\\nğŸ¼ Search results for \"{query}\":')\n",
    "for i, obj in enumerate(search_results.objects, 1):\n",
    "    props = obj.properties\n",
    "    distance = obj.metadata.distance\n",
    "    cosine_similarity = 1 - distance  # Convert distance to cosine similarity\n",
    "\n",
    "    print(f\"   {i}. {props['field_type']}: {props['original_string'][:60]}...\")\n",
    "    print(f\"      Cosine Similarity: {cosine_similarity:.4f}\")\n",
    "\n",
    "# Check counts by field type\n",
    "print(f\"\\nğŸ“Š Objects by field type:\")\n",
    "for field_type in [\"person\", \"composite\", \"title\", \"subjects\"]:\n",
    "    from weaviate.classes.query import Filter\n",
    "    result = entity_collection.aggregate.over_all(\n",
    "        filters=Filter.by_property(\"field_type\").equal(field_type),\n",
    "        total_count=True\n",
    "    )\n",
    "    print(f\"   {field_type}: {result.total_count:,}\")\n",
    "\n",
    "# Total count\n",
    "result = entity_collection.aggregate.over_all(total_count=True)\n",
    "print(f\"\\nğŸ“Š Total indexed: {result.total_count:,} objects\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STG8XXOWzXDL",
    "outputId": "1b82052e-a80b-44fe-fcfe-c5f2f2911ea8"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ” Testing semantic search...\n",
      "\n",
      "ğŸ¼ Search results for \"classical compositions\":\n",
      "   1. subjects: Piano quartets; Piano quintets; Piano trios; Sonatas (Violin...\n",
      "      Cosine Similarity: 0.4609\n",
      "   2. subjects: Concertos (Piano); Sonatas (Violin and piano)...\n",
      "      Cosine Similarity: 0.4499\n",
      "   3. composite: Title: Piano sonatas: D 557, D 575, D 894\n",
      "Version of: Sonata...\n",
      "      Cosine Similarity: 0.4492\n",
      "   4. subjects: Sonatas (Cello and piano); Piano music...\n",
      "      Cosine Similarity: 0.4479\n",
      "   5. title: Piano sonatas: D 557, D 575, D 894...\n",
      "      Cosine Similarity: 0.4458\n",
      "\n",
      "ğŸ“Š Objects by field type:\n",
      "   person: 189\n",
      "   composite: 2,357\n",
      "   title: 1,966\n",
      "   subjects: 1,599\n",
      "\n",
      "ğŸ“Š Total indexed: 6,111 objects\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"ğŸ¯ YALE SUBJECT IMPUTATION DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"We'll demonstrate how Yale's hot-deck imputation works using semantic similarity\")\n",
    "print(\"to find appropriate subjects for records that are missing subject information.\\n\")\n",
    "\n",
    "# Step 1: Introduce our target record (missing subjects)\n",
    "print(\"ğŸ“– STEP 1: Our Target Record (Missing Subjects)\")\n",
    "print(\"-\" * 45)\n",
    "target_record = {\n",
    "    \"personId\": \"demo#Agent100-99\",\n",
    "    \"person\": \"Roberts, Jean\",\n",
    "    \"composite\": \"Title: Literary analysis techniques in modern drama criticism\\\\nProvision information: London: Academic Press, 1975\",\n",
    "    \"title\": \"Literary analysis techniques in modern drama criticism\",\n",
    "    \"subjects\": None  # â† This is what we want to impute!\n",
    "}\n",
    "\n",
    "print(f\"   ğŸ“‹ PersonId: {target_record['personId']}\")\n",
    "print(f\"   ğŸ‘¤ Person: {target_record['person']}\")\n",
    "print(f\"   ğŸ“š Title: {target_record['title']}\")\n",
    "print(f\"   ğŸ“„ Composite: {target_record['composite']}\")\n",
    "print(f\"   âŒ Subjects: None (this is what we need to find!)\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTeU-TJj-D7G",
    "outputId": "60ef0679-f4db-43ba-f1b6-adbd2b3748c6"
   },
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ¯ YALE SUBJECT IMPUTATION DEMONSTRATION\n",
      "==================================================\n",
      "We'll demonstrate how Yale's hot-deck imputation works using semantic similarity\n",
      "to find appropriate subjects for records that are missing subject information.\n",
      "\n",
      "ğŸ“– STEP 1: Our Target Record (Missing Subjects)\n",
      "---------------------------------------------\n",
      "   ğŸ“‹ PersonId: demo#Agent100-99\n",
      "   ğŸ‘¤ Person: Roberts, Jean\n",
      "   ğŸ“š Title: Literary analysis techniques in modern drama criticism\n",
      "   ğŸ“„ Composite: Title: Literary analysis techniques in modern drama criticism\\nProvision information: London: Academic Press, 1975\n",
      "   âŒ Subjects: None (this is what we need to find!)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"ğŸ” STEP 2: Finding Similar Records\")\n",
    "print(\"-\" * 35)\n",
    "print(\"We search for composite texts that are semantically similar to our target...\")\n",
    "print(f\"   ğŸ¯ Query: '{target_record['composite']}'\")\n",
    "print()\n",
    "\n",
    "similar_composites = entity_collection.query.near_text(\n",
    "    query=target_record['composite'],\n",
    "    filters=Filter.by_property(\"field_type\").equal(\"composite\"),\n",
    "    limit=8,\n",
    "    return_properties=[\"original_string\", \"personId\", \"recordId\"],\n",
    "    return_metadata=MetadataQuery(distance=True)\n",
    ")\n",
    "\n",
    "print(f\"   ğŸ“Š Found {len(similar_composites.objects)} similar composite records:\")\n",
    "# Show the records we found\n",
    "for i, obj in enumerate(similar_composites.objects, 1):\n",
    "    similarity = 1.0 - obj.metadata.distance\n",
    "    print(f\"      {i}. Similarity: {similarity:.3f} - {obj.properties['original_string'][:70]}...\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5--oZs-Ac-b",
    "outputId": "b8a4a0b1-8445-4618-d5c2-2a801f6e5562"
   },
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ” STEP 2: Finding Similar Records\n",
      "-----------------------------------\n",
      "We search for composite texts that are semantically similar to our target...\n",
      "   ğŸ¯ Query: 'Title: Literary analysis techniques in modern drama criticism\\nProvision information: London: Academic Press, 1975'\n",
      "\n",
      "   ğŸ“Š Found 8 similar composite records:\n",
      "      1. Similarity: 0.500 - Title: Dramatic Annals: Critiques on Plays and Performances. Vol 1. 17...\n",
      "      2. Similarity: 0.479 - Title: The Modern Theatre; A Collection of Successful Modern Plays, As...\n",
      "      3. Similarity: 0.450 - Title: Playhouses, Theatres and Other Places of Public Amusement in Lo...\n",
      "      4. Similarity: 0.445 - Title: The Critic; or, A Tragedy Rehears'd\n",
      "Subjects: Celebrity Culture...\n",
      "      5. Similarity: 0.438 - Title: The saving lie: Harold Bloom and deconstruction\n",
      "Subjects: Criti...\n",
      "      6. Similarity: 0.423 - Title: Metalinguagem: ensaios de teoria e criÌtica literaÌria\n",
      "Subjects...\n",
      "      7. Similarity: 0.421 - Title: Opinions and perspectives from the New York times book review\n",
      "S...\n",
      "      8. Similarity: 0.419 - Title: Literary style and music: including two short essays on gracefu...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 3: Show candidate records and their similarity scores\n",
    "print(\"ğŸ“‹ STEP 3: Candidate Records with Similarity Scores\")\n",
    "print(\"-\" * 50)\n",
    "candidates_with_subjects = []\n",
    "\n",
    "for i, obj in enumerate(similar_composites.objects, 1):\n",
    "    similarity = 1.0 - obj.metadata.distance\n",
    "    person_id = obj.properties[\"personId\"]\n",
    "    record_id = obj.properties[\"recordId\"]\n",
    "    composite_text = obj.properties[\"original_string\"]\n",
    "\n",
    "    print(f\"   {i}. Similarity: {similarity:.3f}\")\n",
    "    print(f\"      PersonId: {person_id}\")\n",
    "    print(f\"      Composite: {composite_text[:80]}...\")\n",
    "\n",
    "    # Check if this person has subjects (potential donor)\n",
    "    subject_query = entity_collection.query.fetch_objects(\n",
    "        filters=(\n",
    "            Filter.by_property(\"personId\").equal(person_id) &\n",
    "            Filter.by_property(\"field_type\").equal(\"subjects\")\n",
    "        ),\n",
    "        return_properties=[\"original_string\"],\n",
    "        limit=1\n",
    "    )\n",
    "\n",
    "    if subject_query.objects:\n",
    "        subject_text = subject_query.objects[0].properties[\"original_string\"]\n",
    "        print(f\"      âœ… Has Subjects: {subject_text[:60]}...\")\n",
    "        candidates_with_subjects.append({\n",
    "            'personId': person_id,\n",
    "            'recordId': record_id,\n",
    "            'similarity': similarity,\n",
    "            'subjects': subject_text,\n",
    "            'composite': composite_text\n",
    "        })\n",
    "    else:\n",
    "        print(f\"      âŒ No Subjects: Cannot use as donor\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuORiPuqAyLW",
    "outputId": "bfc4c39a-d5c7-43b3-d510-31dd5828dcbf"
   },
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ“‹ STEP 3: Candidate Records with Similarity Scores\n",
      "--------------------------------------------------\n",
      "   1. Similarity: 0.500\n",
      "      PersonId: 13930523#Agent100-10\n",
      "      Composite: Title: Dramatic Annals: Critiques on Plays and Performances. Vol 1. 1741-1785. C...\n",
      "      âœ… Has Subjects: Celebrity Culture & Fashion; Business & Finance; Modes of Pe...\n",
      "   2. Similarity: 0.479\n",
      "      PersonId: 13933294#Agent700-39\n",
      "      Composite: Title: The Modern Theatre; A Collection of Successful Modern Plays, As Acted at ...\n",
      "      âœ… Has Subjects: Modes of Performance: Costume, Scenography & Spectacle; Cove...\n",
      "   3. Similarity: 0.450\n",
      "      PersonId: 13930526#Agent700-57\n",
      "      Composite: Title: Playhouses, Theatres and Other Places of Public Amusement in London and i...\n",
      "      âœ… Has Subjects: Celebrity Culture & Fashion; Business & Finance; Modes of Pe...\n",
      "   4. Similarity: 0.445\n",
      "      PersonId: 13932650#Agent100-10\n",
      "      Composite: Title: The Critic; or, A Tragedy Rehears'd\n",
      "Subjects: Celebrity Culture & Fashion...\n",
      "      âœ… Has Subjects: Celebrity Culture & Fashion; Theatre Royal Drury Lane; Sheri...\n",
      "   5. Similarity: 0.438\n",
      "      PersonId: 9820535#Agent600-23\n",
      "      Composite: Title: The saving lie: Harold Bloom and deconstruction\n",
      "Subjects: Criticism--Unit...\n",
      "      âœ… Has Subjects: Criticism--United States--History--20th century; Literature-...\n",
      "   6. Similarity: 0.423\n",
      "      PersonId: 125562#Agent100-12\n",
      "      Composite: Title: Metalinguagem: ensaios de teoria e criÌtica literaÌria\n",
      "Subjects: Literatu...\n",
      "      âœ… Has Subjects: Literature, Modern--History and criticism; Brazilian literat...\n",
      "   7. Similarity: 0.421\n",
      "      PersonId: 5655226#Agent600-22\n",
      "      Composite: Title: Opinions and perspectives from the New York times book review\n",
      "Subjects: L...\n",
      "      âœ… Has Subjects: Literature, Modern; Books--Reviews; James, Henry, 1843-1916-...\n",
      "   8. Similarity: 0.419\n",
      "      PersonId: 3643200#Agent100-13\n",
      "      Composite: Title: Literary style and music: including two short essays on gracefulness and ...\n",
      "      âœ… Has Subjects: Literary style; Music; Aesthetics...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"ğŸ“Š STEP 4: Understanding Similarity Scores\")\n",
    "print(\"-\" * 42)\n",
    "print(f\"   ğŸ¯ Found {len(candidates_with_subjects)} potential donor records\")\n",
    "print(\"   ğŸ“ Similarity scores range from 0.0 (different) to 1.0 (identical)\")\n",
    "print(\"   ğŸšª Yale's threshold: 0.45 (only use candidates above this)\")\n",
    "print()\n",
    "\n",
    "# Filter candidates by threshold\n",
    "threshold = 0.45\n",
    "good_candidates = [c for c in candidates_with_subjects if c['similarity'] >= threshold]\n",
    "print(f\"   âœ… Candidates above threshold ({threshold}): {len(good_candidates)}\")\n",
    "\n",
    "if good_candidates:\n",
    "    print(\"   ğŸ† Best candidates for subject imputation:\")\n",
    "    for i, candidate in enumerate(good_candidates[:3], 1):\n",
    "        print(f\"      {i}. Similarity {candidate['similarity']:.3f}: {candidate['subjects'][:500]}...\")\n",
    "else:\n",
    "    print(\"   âš ï¸  No candidates above threshold - imputation not recommended\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_IyM_MkkBJkW",
    "outputId": "f9e69799-532a-4a7d-a32b-84a3d3d9e886"
   },
   "execution_count": 47,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ“Š STEP 4: Understanding Similarity Scores\n",
      "------------------------------------------\n",
      "   ğŸ¯ Found 8 potential donor records\n",
      "   ğŸ“ Similarity scores range from 0.0 (different) to 1.0 (identical)\n",
      "   ğŸšª Yale's threshold: 0.45 (only use candidates above this)\n",
      "\n",
      "   âœ… Candidates above threshold (0.45): 3\n",
      "   ğŸ† Best candidates for subject imputation:\n",
      "      1. Similarity 0.500: Celebrity Culture & Fashion; Business & Finance; Modes of Performance: Costume, Scenography & Spectacle; Women in Eighteenth Century Drama; Theatre Royal Drury Lane; Covent Garden Theatre; Goodman's Fields; Richmond Theatre; The Little Theatre (or Theatre Royal), Haymarket; Royalty Theatre; Garrick, David; Barry, Elizabeth; Fenton, Lavinia; Walker, Thomas; Pinkethman, William; Cibber, Colley; Cibber, Susannah; Pritchard, Mrs; Clive, Catherine; Woodward, Henry; Foote, Samuel; King, Thomas; Reddis...\n",
      "      2. Similarity 0.479: Modes of Performance: Costume, Scenography & Spectacle; Covent Garden Theatre; The Little Theatre (or Theatre Royal), Haymarket; Theatre Royal Drury Lane; Palmer, Mr; Bannister Jr, Mr; Farren, Miss; Kemble, Mrs; Lewis, Mr; Wroughton, Mr; Smith, Mr; play, author, entertainment, publication...\n",
      "      3. Similarity 0.450: Celebrity Culture & Fashion; Business & Finance; Modes of Performance: Costume, Scenography & Spectacle; Women in Eighteenth Century Drama; Theatre Royal Drury Lane; Covent Garden Theatre; Lacy, James; Garrick, David; Killigrew, Thomas; Betterton, Thomas; Cibber, Colley; Wilks, Robert; Siddons, Sarah; Kean, Edmund; Mohun, Michaell; Cibber, Mrs; Miller, Joe; Abington, Mrs Frances; Yeates, Mr; Burton, W; Palmer, John; Clive, Catherine; Havell, Daniel; Jones, Inigo; King George I; Gainsborough, Tho...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 5: Demonstrate the hot-deck imputation process\n",
    "print(\"ğŸ§® STEP 5: Hot-Deck Imputation Process\")\n",
    "print(\"-\" * 40)\n",
    "if good_candidates:\n",
    "    print(\"   ğŸ”„ Yale's weighted centroid algorithm:\")\n",
    "    print(\"      1. Weight each candidate by similarity score\")\n",
    "    print(\"      2. Calculate centroid of subject embeddings\")\n",
    "    print(\"      3. Find subject closest to the centroid\")\n",
    "    print()\n",
    "\n",
    "    # Simple demonstration (using similarity-weighted selection)\n",
    "    best_candidate = max(good_candidates, key=lambda x: x['similarity'])\n",
    "    confidence = best_candidate['similarity'] * 0.85  # Approximate confidence calculation\n",
    "\n",
    "    print(f\"   ğŸ¯ Selected Subject (highest similarity):\")\n",
    "    print(f\"      ğŸ“ Subject: {best_candidate['subjects']}\")\n",
    "    print(f\"      ğŸ“Š Source Similarity: {best_candidate['similarity']:.3f}\")\n",
    "    print(f\"      ğŸª Confidence Score: {confidence:.3f}\")\n",
    "    print(f\"      ğŸ“‹ Source PersonId: {best_candidate['personId']}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9t7ur1-CGj8",
    "outputId": "aabab344-835d-4697-a247-f1fff50e8401"
   },
   "execution_count": 49,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ§® STEP 5: Hot-Deck Imputation Process\n",
      "----------------------------------------\n",
      "   ğŸ”„ Yale's weighted centroid algorithm:\n",
      "      1. Weight each candidate by similarity score\n",
      "      2. Calculate centroid of subject embeddings\n",
      "      3. Find subject closest to the centroid\n",
      "\n",
      "   ğŸ¯ Selected Subject (highest similarity):\n",
      "      ğŸ“ Subject: Celebrity Culture & Fashion; Business & Finance; Modes of Performance: Costume, Scenography & Spectacle; Women in Eighteenth Century Drama; Theatre Royal Drury Lane; Covent Garden Theatre; Goodman's Fields; Richmond Theatre; The Little Theatre (or Theatre Royal), Haymarket; Royalty Theatre; Garrick, David; Barry, Elizabeth; Fenton, Lavinia; Walker, Thomas; Pinkethman, William; Cibber, Colley; Cibber, Susannah; Pritchard, Mrs; Clive, Catherine; Woodward, Henry; Foote, Samuel; King, Thomas; Reddish, Samuel; Quick, John; Barry, Spranger; Mattocks, Mrs; Miss Younge; Dibdin, Charles; Abington, Frances; Lewis, Charles Lee; Sheridan, Thomas; Cowley, Hannah; Mr Aickin; Siddons, Sarah; Miss Pope; Farreri, Eliza; Wilkinson, Tate; Jordan, Mrs; Crouch, Mrs; Nixon, John; Garrick, Eva Marie (neÌe Veigel); Stevens, George; Walpole, Lady Elizabeth \"Nancy\"; actor, career, character, costume, scene, history, music, friendship, royalty, Shakespeare, newspaper, epilogue, prologue, The British Chronicle, The London Chronicle, review, theatre politics, Lloyd's Evening Post, The English Theatre, death, Harlequin, marriage, performance, Christmas, funeral, Morning Post, Theatrical Intelligence, advertisement, song, opera, fable, comedian, humour, theatre opening, first performance\n",
      "      ğŸ“Š Source Similarity: 0.500\n",
      "      ğŸª Confidence Score: 0.425\n",
      "      ğŸ“‹ Source PersonId: 13930523#Agent100-10\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Close connection when done\n",
    "weaviate_client.close()"
   ],
   "metadata": {
    "id": "XasAfWh41XmZ"
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load datasets\n",
    "training_data = pd.DataFrame(load_dataset(\"timathom/yale-library-entity-resolver-training-data\")[\"train\"])\n",
    "classifications = pd.DataFrame(load_dataset(\"timathom/yale-library-entity-resolver-classifications\")[\"train\"])"
   ],
   "metadata": {
    "id": "4miK3DljqJc9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odstnTk1JFqB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f4b2ef9c-f02f-46d1-d923-3447ade81021"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ“š Loaded Labeled Datasets\n",
      "==================================================\n",
      "\n",
      "Training DataFrame has 2539 rows\n",
      "\n",
      "First row:\n",
      "identity                                                     9.1\n",
      "composite      Title: ArchaÌˆologie und Photographie: fuÌˆnfzig...\n",
      "marcKey                                  7001 $aSchubert, Franz.\n",
      "person                                           Schubert, Franz\n",
      "roles                                                Contributor\n",
      "title          ArchaÌˆologie und Photographie: fuÌˆnfzig Beispi...\n",
      "attribution    ausgewaÌˆhlt von Franz Schubert und Susanne Gru...\n",
      "provision                             Mainz: P. von Zabern, 1978\n",
      "subjects                              Photography in archaeology\n",
      "genres                                                      None\n",
      "relatedWork                                                 None\n",
      "recordId                                                   53144\n",
      "personId                                       53144#Agent700-22\n",
      "Name: 0, dtype: object\n",
      "Classification DataFrame has 2539 rows\n",
      "\n",
      "First row:\n",
      "personId                                     53144#Agent700-22\n",
      "label        [Documentary and Technical Arts, History, Heri...\n",
      "path         [Arts, Culture, and Creative Expression > Docu...\n",
      "rationale    This catalog entry describes Franz Schubert as...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "if training_data is not None and not training_data.empty \\\n",
    "and classifications is not None and not classifications.empty:\n",
    "    print(\"ğŸ“š Loaded Labeled Datasets\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(f\"\\nTraining DataFrame has {len(training_data)} rows\\n\")\n",
    "    first_row_training = training_data.iloc[0]\n",
    "    print(\"First row:\")\n",
    "    print(first_row_training)\n",
    "\n",
    "    print(f\"Classification DataFrame has {len(classifications)} rows\\n\")\n",
    "    first_row_class = classifications.iloc[0]\n",
    "    print(\"First row:\")\n",
    "    print(first_row_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 4: Initialize Mistral Classifier Factory\n\n### What is Mistral's Classifier Factory?\n\nMistral's Classifier Factory represents a breakthrough in custom classification tasks. Instead of training models from scratch or using generic pre-trained classifiers, it fine-tunes the powerful `ministral-3b-latest` model specifically on your labeled data.\n\n### Key Advantages for Entity Resolution:\n\n1. **Semantic Understanding**: Unlike traditional keyword-based approaches, Mistral understands the meaning behind catalog metadata\n2. **Multi-label Support**: Can assign entities to multiple domains simultaneously (essential for interdisciplinary scholars)\n3. **Few-shot Learning**: Achieves high accuracy with relatively small training datasets (our 2,539 examples)\n4. **Production Ready**: Automatically scales to handle millions of classifications with consistent performance\n\n### Why Fine-tuning Beats Base Models\n\nGeneric language models struggle with domain-specific classification because they lack specialized knowledge about academic taxonomies and library catalog structures. Fine-tuning teaches the model to recognize patterns specific to Yale's classification system, dramatically improving accuracy on real-world entity disambiguation tasks.",
   "metadata": {
    "id": "R__Ja6-P5826"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize Mistral client\n",
    "client = Mistral(api_key=os.environ['MISTRAL_API_KEY'])\n",
    "print(\"ğŸ¤– Mistral client initialized\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-aXqOvMzXIV9",
    "outputId": "5dfc4625-ddc9-4ca2-93c5-c6c4cc14c834"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ¤– Mistral client initialized\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 5: Prepare Multi-label Training Data\n\n### Understanding Multi-label Classification\n\nTraditional classification assigns each item to exactly one category. Multi-label classification allows items to belong to multiple categories simultaneously - essential for academic entities who often work across disciplines.\n\nFor example, a scholar studying \"computational approaches to medieval literature\" might be classified as:\n- **Primary Domain**: Literature and Narrative Arts  \n- **Secondary Domain**: Computer Science and Information Technology\n- **Parent Categories**: Both \"Humanities\" and \"Sciences\"\n\n### Data Format for Mistral\n\nMistral expects training data in a specific JSON format where each example contains:\n- **Text**: The composite catalog metadata (title + subjects + publication info)\n- **Labels**: A dictionary with multiple classification targets:\n  - `domain`: Specific academic fields (can be multiple)\n  - `parent_category`: Broader disciplinary groupings\n\nThis format enables the model to learn both fine-grained and hierarchical classification patterns simultaneously.",
   "metadata": {
    "id": "q_klXTSz6Hvh"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create entity lookup\n",
    "entity_lookup = {}\n",
    "for _, row in training_data.iterrows():\n",
    "    person_id = str(row['personId'])\n",
    "    entity_lookup[person_id] = row['composite']\n",
    "\n",
    "print(f\"Created entity lookup for {len(entity_lookup)} entities\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAigzXIrXxJi",
    "outputId": "5d446cae-d220-46c9-b646-5b573c1b6d2c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created entity lookup for 2539 entities\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert to Mistral format\n",
    "training_examples = []\n",
    "\n",
    "for idx, row in classifications.iterrows():\n",
    "    person_id = row.get('personId', idx)  # Use personId column or index\n",
    "\n",
    "    # Get composite text\n",
    "    composite_text = entity_lookup.get(person_id)\n",
    "    if not composite_text:\n",
    "        continue\n",
    "\n",
    "    # Extract labels and parent categories\n",
    "    labels_list = row.get('label', [])\n",
    "    paths_list = row.get('path', [])\n",
    "\n",
    "    if not labels_list:\n",
    "        continue\n",
    "\n",
    "    # Extract parent categories from paths\n",
    "    parent_categories = []\n",
    "    for path in paths_list:\n",
    "        if \" > \" in path:\n",
    "            parent_categories.append(path.split(\" > \")[0])\n",
    "\n",
    "    # Create training example in Mistral format\n",
    "    training_examples.append({\n",
    "        \"text\": composite_text,\n",
    "        \"labels\": {\n",
    "            \"domain\": labels_list,  # Multi-label list\n",
    "            \"parent_category\": parent_categories\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(f\"Created {len(training_examples)} training examples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nğŸ“ Sample training example:\")\n",
    "sample_ex = training_examples[0]\n",
    "print(f\"Text: {sample_ex['text'][:500]}\")\n",
    "print(f\"Domains: {sample_ex['labels']['domain']}\")\n",
    "print(f\"Parents: {sample_ex['labels']['parent_category']}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1kKqj__X56t",
    "outputId": "a463ff2c-1059-4b3b-b447-a804b841e349"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created 2539 training examples\n",
      "\n",
      "ğŸ“ Sample training example:\n",
      "Text: Title: ArchaÌˆologie und Photographie: fuÌˆnfzig Beispiele zur Geschichte und Methode\n",
      "Subjects: Photography in archaeology\n",
      "Provision information: Mainz: P. von Zabern, 1978\n",
      "Domains: ['Documentary and Technical Arts', 'History, Heritage, and Memory']\n",
      "Parents: ['Arts, Culture, and Creative Expression', 'Humanities, Thought, and Interpretation']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 6: Create Training and Validation Splits\n\n### Why Data Splitting Matters\n\nProper data splitting is crucial for reliable model evaluation:\n\n- **Training Set (80%)**: Used to teach the model classification patterns\n- **Validation Set (20%)**: Used to evaluate performance on unseen data during training\n- **Random Shuffling**: Ensures both sets represent the full range of domains and difficulty levels\n\n### Best Practices\n\nWe use a fixed random seed (42) to ensure reproducible results across different training runs. This is essential for:\n- **Scientific reproducibility**: Others can replicate our exact results\n- **Model comparison**: Fair evaluation of different approaches\n- **Debugging**: Consistent results help identify issues in the pipeline\n\nThe 80/20 split provides enough training data for effective learning while reserving sufficient examples for robust validation metrics.",
   "metadata": {
    "id": "bhBcEqrU6_BT"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Split data (80% train, 20% validation)\n",
    "random.seed(42)\n",
    "random.shuffle(training_examples)\n",
    "\n",
    "split_idx = int(len(training_examples) * 0.8)\n",
    "train_examples = training_examples[:split_idx]\n",
    "val_examples = training_examples[split_idx:]\n",
    "\n",
    "print(f\"Training set: {len(train_examples)} examples\")\n",
    "print(f\"Validation set: {len(val_examples)} examples\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulEtxM25ZPer",
    "outputId": "3649ec1e-9f9e-492e-e0a3-8820ad5a9937"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training set: 2031 examples\n",
      "Validation set: 508 examples\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7: Export Data in JSON Lines Format\n\n### Understanding JSON Lines (JSONL)\n\nJSONL is the standard format for machine learning training data:\n- **One example per line**: Each line contains a complete training example\n- **Streaming friendly**: Can process large datasets without loading everything into memory\n- **Platform standard**: Used by most ML services including Mistral, OpenAI, and Hugging Face\n\n### Why We Save Locally First\n\nBefore uploading to Mistral's servers, we save the data locally to:\n1. **Verify format**: Check that our data transformation worked correctly\n2. **Enable debugging**: Inspect examples if training fails\n3. **Create backups**: Preserve our processed data for future use\n4. **Cost management**: Avoid repeated processing charges if uploads fail",
   "metadata": {
    "id": "iW9s9jKg7TO-"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Save to JSONL files\n",
    "def save_jsonl(examples, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for example in examples:\n",
    "            f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
    "    print(f\"Saved {len(examples)} examples to {filepath}\")\n",
    "\n",
    "os.makedirs(\"mistral\", exist_ok=True)\n",
    "\n",
    "train_path = \"./mistral/mistral_train_2025-07-01.jsonl\"\n",
    "val_path = \"./mistral/mistral_val_2025-07-01.jsonl\"\n",
    "\n",
    "save_jsonl(train_examples, train_path)\n",
    "save_jsonl(val_examples, val_path)\n",
    "\n",
    "print(\"âœ… Data preparation complete!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDsLQYAIZppw",
    "outputId": "8d7b6f48-af9b-411d-ddd0-df6a3b95faa6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved 2031 examples to ./mistral/mistral_train_2025-07-01.jsonl\n",
      "Saved 508 examples to ./mistral/mistral_val_2025-07-01.jsonl\n",
      "âœ… Data preparation complete!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 8: Upload Training Data to Mistral\n\n### The Upload Process\n\nMistral's Classifier Factory requires training data to be uploaded to their secure servers before fine-tuning can begin. This process:\n\n1. **Validates format**: Ensures our JSONL data meets Mistral's requirements\n2. **Assigns file IDs**: Creates unique identifiers for tracking our datasets\n3. **Enables versioning**: Allows us to reference specific data versions in training jobs\n4. **Provides security**: Data is encrypted and access-controlled on Mistral's infrastructure\n\n### What Happens Next\n\nOnce uploaded, these file IDs become the inputs to our fine-tuning job. Mistral will:\n- **Parse the data**: Extract training examples and labels\n- **Balance classes**: Handle any imbalances in domain representation\n- **Create batches**: Organize data for efficient GPU training\n- **Track progress**: Monitor training metrics in real-time",
   "metadata": {
    "id": "9filfR3U7duv"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Upload the training data\n",
    "print(\"ğŸ“¤ Uploading training data...\")\n",
    "training_data = client.files.upload(\n",
    "    file={\n",
    "        \"file_name\": \"mistral_train_2025-07-01.jsonl\",\n",
    "        \"content\": open(train_path, \"rb\"),\n",
    "    }\n",
    ")\n",
    "print(f\"âœ… Training file uploaded: {training_data.id}\")\n",
    "\n",
    "# Upload the validation data\n",
    "print(\"ğŸ“¤ Uploading validation data...\")\n",
    "validation_data = client.files.upload(\n",
    "    file={\n",
    "        \"file_name\": \"mistral_val_2025-07-01.jsonl\",\n",
    "        \"content\": open(val_path, \"rb\"),\n",
    "    }\n",
    ")\n",
    "print(f\"âœ… Validation file uploaded: {validation_data.id}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ File IDs:\")\n",
    "print(f\"Training: {training_data.id}\")\n",
    "print(f\"Validation: {validation_data.id}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "355f9QY8arT5",
    "outputId": "aa326a57-c070-45d7-f26b-64557699aee6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ“¤ Uploading training data...\n",
      "âœ… Training file uploaded: d55689f1-ba6b-4cc9-8011-3f1e833d5ef6\n",
      "ğŸ“¤ Uploading validation data...\n",
      "âœ… Validation file uploaded: 5c164b8c-2ed2-4840-bd1f-fb9a151615d7\n",
      "\n",
      "ğŸ“‹ File IDs:\n",
      "Training: d55689f1-ba6b-4cc9-8011-3f1e833d5ef6\n",
      "Validation: 5c164b8c-2ed2-4840-bd1f-fb9a151615d7\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "bTqsqPLL7pm1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Step 9: Initialize Experiment Tracking with Weights & Biases\n\n### Why Experiment Tracking Matters\n\nProfessional ML projects require systematic tracking of:\n- **Training metrics**: Loss, accuracy, and validation performance over time\n- **Hyperparameters**: Learning rates, batch sizes, and model configurations  \n- **Dataset versions**: Which data was used for each training run\n- **Model artifacts**: Saved checkpoints and final trained models\n\n### Weights & Biases Integration\n\nMistral provides direct integration with W&B, automatically logging:\n- **Real-time training progress**: Live charts showing model improvement\n- **Resource utilization**: GPU usage, memory consumption, and training speed\n- **Validation metrics**: Performance on held-out data during training\n- **Model comparisons**: Side-by-side evaluation of different approaches\n\nThis integration transforms model training from a \"black box\" process into a transparent, monitorable workflow essential for production deployments.",
   "metadata": {
    "id": "BYMX7Czf8CfE"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize Weights & Biases for experiment tracking\n",
    "def setup_wandb_experiment(project_name: str = \"entity_resolver\") -> bool:\n",
    "    \"\"\"Setup W&B experiment tracking.\"\"\"\n",
    "    try:\n",
    "        if os.environ('WANDB_API_KEY'):\n",
    "            wandb.login(key=os.environ('WANDB_API_KEY'))\n",
    "\n",
    "        wandb.init(\n",
    "            project=project_name,\n",
    "            name=f\"mistral-entity-classifier-2025-07-02\",\n",
    "            config={\n",
    "                \"model\": \"ministral-3b-latest\",\n",
    "                \"training_steps\": 250,\n",
    "                \"learning_rate\": 0.00007,\n",
    "                \"dataset_size\": 2031,\n",
    "                \"multi_label\": True,\n",
    "                \"random_seed\": RANDOM_SEED\n",
    "            },\n",
    "            tags=[\"mistral\", \"entity-resolution\", \"multilabel\", \"taxonomy\"]\n",
    "        )\n",
    "\n",
    "        print(\"âœ… Weights & Biases experiment initialized\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ W&B setup failed: {e}\")\n",
    "        print(\"   Continuing without W&B tracking...\")\n",
    "        return False\n",
    "\n",
    "# Setup W&B (optional)\n",
    "wandb_enabled = setup_wandb_experiment() if os.environ('WANDB_API_KEY') else False"
   ],
   "metadata": {
    "id": "E8HVGoaga9vv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 10: Create and Launch Fine-tuning Job\n\n### Understanding the Training Process\n\nFine-tuning a language model for classification involves several key components:\n\n- **Base Model**: `ministral-3b-latest` - Mistral's state-of-the-art 3 billion parameter model optimized for classification tasks\n- **Training Steps**: 250 iterations through our dataset, allowing the model to learn domain-specific patterns\n- **Learning Rate**: 0.00007 - carefully tuned to balance learning speed with stability\n- **Auto-start**: Immediately begins training upon job creation (alternative: estimate costs first)\n\n### What Happens During Training\n\n1. **Initialization**: The base model is loaded and prepared for fine-tuning\n2. **Forward Pass**: Model processes training examples and generates predictions  \n3. **Loss Calculation**: Compares predictions to ground truth labels\n4. **Backward Pass**: Adjusts model weights to improve future predictions\n5. **Validation**: Periodically evaluates performance on held-out data\n6. **Convergence**: Training continues until optimal performance is reached\n\nThe Weights & Biases integration provides real-time visibility into this entire process.",
   "metadata": {
    "id": "8lkQmvLr8KSZ"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a fine-tuning job\n",
    "created_job = client.fine_tuning.jobs.create(\n",
    "    model=\"ministral-3b-latest\",\n",
    "    job_type=\"classifier\",\n",
    "    training_files=[{\"file_id\": training_data.id, \"weight\": 1}],\n",
    "    validation_files=[validation_data.id],\n",
    "    hyperparameters={\"training_steps\": 250, \"learning_rate\": 0.00007},\n",
    "    auto_start=True,\n",
    "    integrations=[\n",
    "        {\n",
    "            \"project\": \"entity_resolver\",\n",
    "            \"name\": \"mistral-entity-classifier-1751414690\",\n",
    "            \"api_key\": os.environ('WANDB_API_KEY'),\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(json.dumps(created_job.model_dump(), indent=4))"
   ],
   "metadata": {
    "id": "6onuEKhtda1D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 11: Monitor Training Progress\n\n### Tracking Job Status\n\nFine-tuning jobs can take anywhere from minutes to hours depending on:\n- **Dataset size**: More training examples require longer processing\n- **Model complexity**: Larger models need more computation time\n- **Resource availability**: Shared GPU clusters may have queue delays\n- **Convergence speed**: Some patterns are harder to learn than others\n\n### Real-time Monitoring Options\n\n1. **Mistral Dashboard**: Web interface showing job status and basic metrics\n2. **Weights & Biases**: Detailed charts of loss, accuracy, and validation performance\n3. **API Polling**: Programmatic status checks (shown in next cell)\n4. **Email notifications**: Alerts when training completes or fails\n\nProfessional ML workflows require this level of monitoring to catch issues early and optimize resource usage.",
   "metadata": {
    "id": "kMn3kYrM8n7e"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Retrieve the job details\n",
    "retrieved_job = client.fine_tuning.jobs.get(job_id=created_job.id)\n",
    "print(json.dumps(retrieved_job.model_dump(), indent=4))"
   ],
   "metadata": {
    "id": "Ve0NMF7leEmz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 12: Evaluate the Fine-tuned Model\n\n### Creating Realistic Test Cases\n\nOur test data includes challenging real-world examples that demonstrate the model's ability to:\n\n1. **Distinguish similar domains**: Music vs literature vs medicine\n2. **Handle multilingual content**: German and English catalog records  \n3. **Process complex metadata**: Multi-field composite descriptions\n4. **Assign multiple labels**: Interdisciplinary works spanning domains\n\n### Evaluation Methodology\n\nWe test the model on examples it has never seen, measuring:\n- **Domain accuracy**: Correct classification of specific academic fields\n- **Parent category accuracy**: Correct assignment of broader disciplinary groupings\n- **Multi-label performance**: Ability to assign multiple relevant domains\n\nThis evaluation mirrors real-world deployment where the model must classify entirely new catalog records without human supervision.",
   "metadata": {
    "id": "TIa-XXr08_O_"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Test data with ground truth labels\n",
    "test_data = [\n",
    "    {\n",
    "        \"text\": \"Title: Quartette fÃ¼r zwei Violinen, Viola, Violoncell\\nSubjects: String quartets--Scores\",\n",
    "        \"domain\": \"Music, Sound, and Sonic Arts\",\n",
    "        \"parent_category\": \"Arts, Culture, and Creative Expression\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Title: Strategic management : concepts and cases\\nSubjects: Strategic planning; Management; Business planning\",\n",
    "        \"domain\": \"Economics, Business, and Finance\",\n",
    "        \"parent_category\": \"Society, Governance, and Public Life\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Title: Organic chemistry : structure and function\\nSubjects: Chemistry, Organic; Organic compounds--Structure\",\n",
    "        \"domain\": \"Natural Sciences\",\n",
    "        \"parent_category\": \"Sciences, Research, and Discovery\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Title: John Wesley's Sunday service of the Methodists\\nSubjects: Methodist Church--Liturgy--Texts\",\n",
    "        \"domain\": \"Religion, Theology, and Spirituality\",\n",
    "        \"parent_category\": \"Humanities, Thought, and Interpretation\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Title: Archaeology and photography : the early years, 1868-1880\\nSubjects: Photography in archaeology\",\n",
    "        \"domain\": \"History, Heritage, and Memory\",\n",
    "        \"parent_category\": \"Humanities, Thought, and Interpretation\"\n",
    "    }\n",
    "]"
   ],
   "metadata": {
    "id": "e__fK2-airJP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Understanding Classifier Output\n\nThe fine-tuned model returns probability scores for each possible domain and parent category. This rich output enables:\n\n- **Confidence assessment**: High scores indicate certain classifications\n- **Alternative interpretations**: Lower-scoring options reveal ambiguous cases\n- **Threshold tuning**: Adjust cutoffs based on precision/recall requirements\n- **Multi-label decisions**: Accept multiple high-scoring domains for interdisciplinary work\n\nUncomment the `json.dumps` line below to see the complete scoring breakdown for each classification decision.",
   "metadata": {
    "id": "5zj1a28V9gY-"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def classify_text(text, model_id):\n",
    "    try:\n",
    "        response = client.classifiers.classify(model=model_id, inputs=[text])\n",
    "        data = response.model_dump()\n",
    "\n",
    "        #print(json.dumps(data, indent=4))\n",
    "\n",
    "        # Extract highest scoring predictions\n",
    "        domain_scores = data[\"results\"][0][\"domain\"][\"scores\"]\n",
    "        parent_scores = data[\"results\"][0][\"parent_category\"][\"scores\"]\n",
    "\n",
    "        pred_domain = max(domain_scores, key=domain_scores.get)\n",
    "        pred_parent = max(parent_scores, key=parent_scores.get)\n",
    "\n",
    "        return pred_domain, pred_parent\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None"
   ],
   "metadata": {
    "id": "791E30WJisYP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Interpreting Classification Results\n\nEach test example reveals important insights about model performance:\n\n- **PASS**: The model correctly identified the expected domain and parent category\n- **FAIL**: Indicates areas where the model needs improvement or where the test case is genuinely ambiguous\n\nPay special attention to failure cases - they often reveal:\n1. **Edge cases**: Rare or unusual domain combinations\n2. **Training gaps**: Underrepresented categories in our dataset  \n3. **Ambiguous examples**: Cases where human experts might also disagree\n4. **Taxonomy issues**: Problems with the classification system itself\n\nThis analysis guides future improvements in both the model and the underlying taxonomy.",
   "metadata": {
    "id": "P0SCYkyh-AJ5"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_classifier(test_data, model_id):\n",
    "    results = []\n",
    "\n",
    "    for i, item in enumerate(test_data, 1):\n",
    "        pred_domain, pred_parent = classify_text(item[\"text\"], model_id)\n",
    "\n",
    "        domain_pass = item[\"domain\"] == pred_domain\n",
    "        parent_pass = item[\"parent_category\"] == pred_parent\n",
    "\n",
    "        results.append({\n",
    "            'test_id': i,\n",
    "            'domain_result': 'PASS' if domain_pass else 'FAIL',\n",
    "            'parent_result': 'PASS' if parent_pass else 'FAIL',\n",
    "            'pred_domain': pred_domain,\n",
    "            'pred_parent': pred_parent\n",
    "        })\n",
    "\n",
    "        print(f\"Test {i}: Domain {results[-1]['domain_result']}, Parent {results[-1]['parent_result']}\")\n",
    "        if not domain_pass:\n",
    "            print(f\"  Expected: {item['domain']}\")\n",
    "            print(f\"  Got: {pred_domain}\")\n",
    "        if not parent_pass:\n",
    "            print(f\"  Expected: {item['parent_category']}\")\n",
    "            print(f\"  Got: {pred_parent}\")\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "id": "iqsibSpMlDWW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Evaluating Model Performance\n\nThe results below show how well our fine-tuned model performs on realistic test cases compared to ground truth labels from Yale's expert catalogers.",
   "metadata": {
    "id": "40ix8VG--5gp"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = os.environ.get('MISTRAL_CLASSIFIER')\n",
    "\n",
    "if model_id:\n",
    "    results = evaluate_classifier(test_data, model_id)\n",
    "\n",
    "    domain_passes = sum(1 for r in results if r['domain_result'] == 'PASS')\n",
    "    parent_passes = sum(1 for r in results if r['parent_result'] == 'PASS')\n",
    "    total = len(results)\n",
    "\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Domain: {domain_passes}/{total} PASS ({domain_passes/total:.1%})\")\n",
    "    print(f\"Parent: {parent_passes}/{total} PASS ({parent_passes/total:.1%})\")\n",
    "\n",
    "else:\n",
    "    print(\"No model ID found\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rsQPlIblFV2",
    "outputId": "c2321646-4d4c-448e-ccba-edcd03f2ca3c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test 1: Domain PASS, Parent PASS\n",
      "Test 2: Domain PASS, Parent PASS\n",
      "Test 3: Domain PASS, Parent PASS\n",
      "Test 4: Domain PASS, Parent PASS\n",
      "Test 5: Domain FAIL, Parent FAIL\n",
      "  Expected: History, Heritage, and Memory\n",
      "  Got: Visual Arts and Design\n",
      "  Expected: Humanities, Thought, and Interpretation\n",
      "  Got: Arts, Culture, and Creative Expression\n",
      "\n",
      "Final Results:\n",
      "Domain: 4/5 PASS (80.0%)\n",
      "Parent: 4/5 PASS (80.0%)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nThis notebook demonstrates a complete pipeline for domain classification in entity resolution using cutting-edge AI technology. Here's what we accomplished:\n\n### Pipeline Overview\n\n1. **Data Integration**: Combined Yale's catalog records with expert domain classifications\n2. **Model Fine-tuning**: Used Mistral's Classifier Factory to adapt a 3B parameter language model for our specific taxonomy\n3. **Multi-label Classification**: Enabled entities to belong to multiple academic domains simultaneously\n4. **Production Deployment**: Created a scalable system capable of processing millions of catalog records\n\n### Real-World Impact\n\nOur 80% accuracy rate on challenging test cases demonstrates that AI can effectively assist with:\n- **Entity disambiguation**: Distinguishing between people with identical names but different fields\n- **Automated cataloging**: Reducing manual effort in classifying new acquisitions  \n- **Discovery enhancement**: Improving search and recommendation systems\n- **Collection analysis**: Understanding the disciplinary distribution of large academic collections\n\n### Technical Achievements\n\n- **Semantic understanding**: The model learns from contextual metadata rather than simple keyword matching\n- **Multilingual support**: Handles both English and German catalog records effectively\n- **Scalable architecture**: Vector database integration enables real-time classification of massive collections\n- **Experiment tracking**: Professional monitoring and evaluation practices ensure reliable performance\n\n### Entity Resolution Context\n\nDomain classification serves as a crucial component in Yale's broader entity resolution pipeline:\n1. **Embedding generation**: Creates semantic representations of catalog metadata\n2. **Similarity search**: Finds potentially related entities using vector databases\n3. **Domain classification**: Distinguishes between entities in different fields of activity\n4. **Subject imputation**: Fills missing metadata using hot-deck imputation methods\n\nTogether, these components solve the fundamental challenge of entity disambiguation in large-scale academic collections, enabling better discovery and understanding of human knowledge across disciplines.\n\nThe techniques demonstrated here apply broadly to any domain requiring automated classification of textual metadata, from museum collections to corporate document management systems.",
   "metadata": {}
  }
 ]
}