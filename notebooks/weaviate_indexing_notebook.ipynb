{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Indexing and Retrieval with Weaviate v4\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Set up and configure Weaviate vector database\n",
    "- Create collections optimized for library metadata\n",
    "- Index library catalog records with modern embeddings\n",
    "- Perform semantic similarity searches\n",
    "- Execute complex queries combining vector and keyword search\n",
    "- Apply these techniques to real entity resolution challenges\n",
    "\n",
    "## Why Vector Databases for Libraries?\n",
    "\n",
    "Traditional library catalogs rely on exact keyword matching and controlled vocabularies. But what if a researcher searches for \"musical compositions\" and we have records cataloged under \"symphonies,\" \"sonatas,\" or \"lieder\"? \n",
    "\n",
    "Vector databases like Weaviate solve this by understanding semantic meaning. They can find relevant records even when the exact terms don't match, opening up new possibilities for discovery and research.\n",
    "\n",
    "**Real Example**: A search for \"Viennese classical music\" could automatically surface records for Mozart, Beethoven, and Schubert, even if those exact terms don't appear in the catalog records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Dependencies\n",
    "\n",
    "First, let's install the required packages and set up our environment. We'll use the latest Weaviate v4 Python client which provides significant improvements over previous versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T21:35:48.737737Z",
     "iopub.status.busy": "2025-06-30T21:35:48.737350Z",
     "iopub.status.idle": "2025-06-30T21:35:53.358017Z",
     "shell.execute_reply": "2025-06-30T21:35:53.357416Z",
     "shell.execute_reply.started": "2025-06-30T21:35:48.737706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (0.28.1)\n",
      "Collecting openai\n",
      "  Downloading openai-1.93.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from httpx) (4.0.0)\n",
      "Requirement already satisfied: certifi in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from httpx) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from httpx) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/tt434/.pyenv/versions/dfrnt/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Downloading openai-1.93.0-py3-none-any.whl (755 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m755.0/755.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai\n",
      "Successfully installed openai-1.93.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T21:35:58.127177Z",
     "iopub.status.busy": "2025-06-30T21:35:58.126439Z",
     "iopub.status.idle": "2025-06-30T21:35:59.476727Z",
     "shell.execute_reply": "2025-06-30T21:35:59.476220Z",
     "shell.execute_reply.started": "2025-06-30T21:35:58.127123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "httpcore                      1.0.9\n",
      "httpx                         0.28.1\n",
      "langchain-openai              0.2.2\n",
      "openai                        1.93.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -E \"(openai|httpx|httpcore)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T21:36:04.313955Z",
     "iopub.status.busy": "2025-06-30T21:36:04.313441Z",
     "iopub.status.idle": "2025-06-30T21:36:04.596677Z",
     "shell.execute_reply": "2025-06-30T21:36:04.595928Z",
     "shell.execute_reply.started": "2025-06-30T21:36:04.313917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 4.4.0 not found\n",
      "‚úÖ Environment setup complete!\n",
      "üì¶ Weaviate client version: 4.8.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for this workshop\n",
    "!pip install weaviate-client>=4.4.0 openai pandas numpy matplotlib seaborn python-dotenv\n",
    "\n",
    "# Import libraries\n",
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "from weaviate.classes.config import Property, DataType, Configure\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables (create a .env file with your API keys)\n",
    "load_dotenv()\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"üì¶ Weaviate client version: {weaviate.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Starting Weaviate and Client Connection\n",
    "\n",
    "We'll start with a local Weaviate instance using Docker. This gives us full control and eliminates API costs during development.\n",
    "\n",
    "**Note**: If you don't have Docker installed, you can use Weaviate Cloud Services (WCS) instead by modifying the connection parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T21:40:53.983069Z",
     "iopub.status.busy": "2025-06-30T21:40:53.982717Z",
     "iopub.status.idle": "2025-06-30T21:40:54.213449Z",
     "shell.execute_reply": "2025-06-30T21:40:54.212990Z",
     "shell.execute_reply.started": "2025-06-30T21:40:53.983027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to Weaviate at http://localhost:8080\n",
      "üìä Weaviate version: 1.30.0-rc.0\n",
      "\n",
      "üîç Current collections in Weaviate: 7\n",
      "  - ConceptScheme\n",
      "  - ClassificationScheme\n",
      "  - LibraryCatalog\n",
      "  - FineTunedConcepts\n",
      "  - Document\n",
      "  - MultilingualDocument\n",
      "  - EntityString\n"
     ]
    }
   ],
   "source": [
    "# First, let's start Weaviate using Docker\n",
    "# Run this in your terminal before executing this notebook:\n",
    "# docker run -d --name weaviate-workshop \\\n",
    "#   -p 8080:8080 -p 50051:50051 \\\n",
    "#   -e QUERY_DEFAULTS_LIMIT=25 \\\n",
    "#   -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true \\\n",
    "#   -e PERSISTENCE_DATA_PATH='/var/lib/weaviate' \\\n",
    "#   -e DEFAULT_VECTORIZER_MODULE='none' \\\n",
    "#   -e CLUSTER_HOSTNAME='node1' \\\n",
    "#   semitechnologies/weaviate:1.25.0\n",
    "\n",
    "# Connect to local Weaviate instance\n",
    "def connect_to_weaviate(host: str = \"localhost\", port: int = 8080, secure: bool = False) -> weaviate.WeaviateClient:\n",
    "    \"\"\"Connect to Weaviate instance with proper error handling.\"\"\"\n",
    "    try:\n",
    "        # Create connection URL\n",
    "        protocol = \"https\" if secure else \"http\"\n",
    "        url = f\"{protocol}://{host}:{port}\"\n",
    "        \n",
    "        # Connect using v4 client syntax\n",
    "        weaviate_client = weaviate.connect_to_local(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            grpc_port=50051,  # gRPC port for faster operations            \n",
    "        )\n",
    "        \n",
    "        # Test the connection\n",
    "        if weaviate_client.is_ready():\n",
    "            print(f\"‚úÖ Successfully connected to Weaviate at {url}\")\n",
    "            \n",
    "            # Get cluster information\n",
    "            meta = weaviate_client.get_meta()\n",
    "            print(f\"üìä Weaviate version: {meta.get('version', 'Unknown')}\")\n",
    "            \n",
    "            return weaviate_client\n",
    "        else:\n",
    "            raise ConnectionError(\"Weaviate is not ready\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to connect to Weaviate: {e}\")\n",
    "        print(\"\\nüí° Troubleshooting tips:\")\n",
    "        print(\"   1. Make sure Docker is running\")\n",
    "        print(\"   2. Start Weaviate with the Docker command provided above\")\n",
    "        print(\"   3. Wait 30-60 seconds for Weaviate to fully start\")\n",
    "        print(\"   4. Check that port 8080 is not in use by another service\")\n",
    "        raise\n",
    "\n",
    "# Connect to Weaviate\n",
    "weaviate_client = connect_to_weaviate()\n",
    "\n",
    "# Verify connection with some basic info\n",
    "print(f\"\\nüîç Current collections in Weaviate: {len(weaviate_client.collections.list_all())}\")\n",
    "for collection_name in weaviate_client.collections.list_all():\n",
    "    print(f\"  - {collection_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Designing Our Library Collection Schema\n",
    "\n",
    "Now we'll create a collection specifically designed for library catalog records. This schema will capture the essential metadata fields while optimizing for semantic search.\n",
    "\n",
    "Think of this as designing the \"database table\" structure, but optimized for vector similarity rather than traditional relational queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T21:40:57.772659Z",
     "iopub.status.busy": "2025-06-30T21:40:57.772147Z",
     "iopub.status.idle": "2025-06-30T21:40:57.986787Z",
     "shell.execute_reply": "2025-06-30T21:40:57.986024Z",
     "shell.execute_reply.started": "2025-06-30T21:40:57.772627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è  Deleting existing 'LibraryCatalog' collection\n",
      "‚úÖ Created collection 'LibraryCatalog' successfully\n",
      "üìã Properties defined: 9\n",
      "\n",
      "üìä Collection Schema:\n",
      "  ‚Ä¢ personId: Unique identifier for the person/entity\n",
      "  ‚Ä¢ person: Person name (e.g., 'Schubert, Franz, 1797-1828')\n",
      "  ‚Ä¢ title: Work title\n",
      "  ‚Ä¢ roles: Person's role (Composer, Contributor, etc.)\n",
      "  ‚Ä¢ subjects: Subject headings and topics\n",
      "  ‚Ä¢ provision: Publication information\n",
      "  ‚Ä¢ composite: Full composite record for embedding\n",
      "  ‚Ä¢ classification: Hierarchical classification (e.g., 'Music and Sound Arts')\n",
      "  ‚Ä¢ recordId: Original catalog record ID\n",
      "\n",
      "üéØ Total collections now: 7\n",
      "üìö Ready to work with 'LibraryCatalog' collection\n"
     ]
    }
   ],
   "source": [
    "# Define our library catalog collection schema\n",
    "COLLECTION_NAME = \"LibraryCatalog\"\n",
    "\n",
    "def create_library_collection(weaviate_client: weaviate.WeaviateClient) -> bool:\n",
    "    \"\"\"Create a collection optimized for library catalog records.\"\"\"\n",
    "    \n",
    "    # Delete existing collection if it exists (for clean slate)\n",
    "    if weaviate_client.collections.exists(COLLECTION_NAME):\n",
    "        print(f\"üóëÔ∏è  Deleting existing '{COLLECTION_NAME}' collection\")\n",
    "        weaviate_client.collections.delete(COLLECTION_NAME)\n",
    "    \n",
    "    try:\n",
    "        # Define properties for our library records\n",
    "        # Each property maps to a field in our MARC/catalog data\n",
    "        properties = [\n",
    "            Property(\n",
    "                name=\"personId\",\n",
    "                data_type=DataType.TEXT,\n",
    "                description=\"Unique identifier for the person/entity\"\n",
    "            ),\n",
    "            Property(\n",
    "                name=\"person\",\n",
    "                data_type=DataType.TEXT,\n",
    "                description=\"Person name (e.g., 'Schubert, Franz, 1797-1828')\"\n",
    "            ),\n",
    "            Property(\n",
    "                name=\"title\",\n",
    "                data_type=DataType.TEXT,\n",
    "                description=\"Work title\"\n",
    "            ),\n",
    "            Property(\n",
    "                name=\"roles\",\n",
    "                data_type=DataType.TEXT,\n",
    "                description=\"Person's role (Composer, Contributor, etc.)\"\n",
    "            ),\n",
    "            Property(\n",
    "                name=\"subjects\",\n",
    "                data_type=DataType.TEXT,\n",
    "                description=\"Subject headings and topics\"\n",
    "            ),\n",
    "            Property(\n",
    "                name=\"provision\",\n",
    "                data_type=DataType.TEXT,\n",
    "                description=\"Publication information\"\n",
    "            ),\n",
    "            Property(\n",
    "                name=\"composite\",\n",
    "                data_type=DataType.TEXT,\n",
    "                description=\"Full composite record for embedding\"\n",
    "            ),\n",
    "            Property(\n",
    "                name=\"classification\",\n",
    "                data_type=DataType.TEXT,\n",
    "                description=\"Hierarchical classification (e.g., 'Music and Sound Arts')\"\n",
    "            ),\n",
    "            Property(\n",
    "                name=\"recordId\",\n",
    "                data_type=DataType.TEXT,\n",
    "                description=\"Original catalog record ID\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Create the collection with manual vectorization\n",
    "        # We'll provide our own embeddings rather than using Weaviate's built-in vectorizers\n",
    "        collection = weaviate_client.collections.create(\n",
    "            name=COLLECTION_NAME,\n",
    "            properties=properties,\n",
    "            # Configure for manual vector management\n",
    "            vectorizer_config=Configure.Vectorizer.none(),\n",
    "            # Use HNSW index for fast approximate nearest neighbor search\n",
    "            vector_index_config=Configure.VectorIndex.hnsw(\n",
    "                distance_metric=wvc.config.VectorDistances.COSINE,  # Use cosine similarity\n",
    "                ef_construction=128,  # Higher = better quality, slower indexing\n",
    "                max_connections=64   # Higher = better search quality, more memory\n",
    "            ),\n",
    "            description=\"Yale Library catalog records with semantic embeddings\"\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Created collection '{COLLECTION_NAME}' successfully\")\n",
    "        print(f\"üìã Properties defined: {len(properties)}\")\n",
    "        \n",
    "        # Display the schema for verification\n",
    "        print(\"\\nüìä Collection Schema:\")\n",
    "        for prop in properties:\n",
    "            print(f\"  ‚Ä¢ {prop.name}: {prop.description}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create collection: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create our library collection\n",
    "success = create_library_collection(weaviate_client)\n",
    "\n",
    "if success:\n",
    "    # Verify the collection was created\n",
    "    collections = weaviate_client.collections.list_all()\n",
    "    print(f\"\\nüéØ Total collections now: {len(collections)}\")\n",
    "    \n",
    "    # Get a reference to our collection for future operations\n",
    "    library_collection = weaviate_client.collections.get(COLLECTION_NAME)\n",
    "    print(f\"üìö Ready to work with '{COLLECTION_NAME}' collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Preparing Sample Library Data\n",
    "\n",
    "Let's create sample library catalog records that represent the diversity of an academic library collection. We'll include our Franz Schubert examples plus records from various domains to demonstrate semantic search capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T21:41:12.956857Z",
     "iopub.status.busy": "2025-06-30T21:41:12.956171Z",
     "iopub.status.idle": "2025-06-30T21:41:12.970856Z",
     "shell.execute_reply": "2025-06-30T21:41:12.969818Z",
     "shell.execute_reply.started": "2025-06-30T21:41:12.956821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Created 9 sample library records\n",
      "\n",
      "üìä Records by classification:\n",
      "  ‚Ä¢ Music and Sound Arts: 4 records\n",
      "  ‚Ä¢ Documentary and Technical Arts: 1 records\n",
      "  ‚Ä¢ Visual Arts: 1 records\n",
      "  ‚Ä¢ History and Culture: 1 records\n",
      "  ‚Ä¢ Social Sciences: 1 records\n",
      "  ‚Ä¢ Literature and Narrative Arts: 1 records\n",
      "\n",
      "üîç Example record structure:\n",
      "  personId: 53001#Agent700-1\n",
      "  person: Schubert, Franz, 1797-1828\n",
      "  title: Symphony no. 8 in B minor, D. 759 \"Unfinished\"\n",
      "  roles: Composer\n",
      "  subjects: Symphonies; Classical music; Romantic period music\n",
      "  provision: Vienna: Universal Edition, 1978\n",
      "  composite: Title: Symphony no. 8 in B minor, D. 759 \"Unfinished\" Subjects: Symphonies; Classical music; Romantic period music Provision information: Vienna: Universal Edition, 1978\n",
      "  classification: Music and Sound Arts\n",
      "  recordId: 53001\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive sample library data\n",
    "def create_sample_library_data() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate sample library catalog records for demonstration.\"\"\"\n",
    "    \n",
    "    sample_records = [\n",
    "        # Franz Schubert - Composer (1797-1828)\n",
    "        {\n",
    "            \"personId\": \"53001#Agent700-1\",\n",
    "            \"person\": \"Schubert, Franz, 1797-1828\",\n",
    "            \"title\": \"Symphony no. 8 in B minor, D. 759 \\\"Unfinished\\\"\",\n",
    "            \"roles\": \"Composer\",\n",
    "            \"subjects\": \"Symphonies; Classical music; Romantic period music\",\n",
    "            \"provision\": \"Vienna: Universal Edition, 1978\",\n",
    "            \"composite\": \"Title: Symphony no. 8 in B minor, D. 759 \\\"Unfinished\\\" Subjects: Symphonies; Classical music; Romantic period music Provision information: Vienna: Universal Edition, 1978\",\n",
    "            \"classification\": \"Music and Sound Arts\",\n",
    "            \"recordId\": \"53001\"\n",
    "        },\n",
    "        {\n",
    "            \"personId\": \"53001#Agent700-2\",\n",
    "            \"person\": \"Schubert, Franz, 1797-1828\",\n",
    "            \"title\": \"Gretchen am Spinnrade, D. 118\",\n",
    "            \"roles\": \"Composer\",\n",
    "            \"subjects\": \"Songs; Lieder; German songs; Voice and piano music\",\n",
    "            \"provision\": \"Leipzig: C. F. Peters, 1885\",\n",
    "            \"composite\": \"Title: Gretchen am Spinnrade, D. 118 Subjects: Songs; Lieder; German songs; Voice and piano music Provision information: Leipzig: C. F. Peters, 1885\",\n",
    "            \"classification\": \"Music and Sound Arts\",\n",
    "            \"recordId\": \"53002\"\n",
    "        },\n",
    "        \n",
    "        # Franz August Schubert - Artist (1806-1893)\n",
    "        {\n",
    "            \"personId\": \"53144#Agent700-22\",\n",
    "            \"person\": \"Schubert, Franz August, 1806-1893\",\n",
    "            \"title\": \"Arch√§ologie und Photographie: f√ºnfzig Beispiele zur Geschichte und Methode\",\n",
    "            \"roles\": \"Contributor\",\n",
    "            \"subjects\": \"Photography in archaeology; Archaeological illustration; Scientific photography\",\n",
    "            \"provision\": \"Mainz: P. von Zabern, 1978\",\n",
    "            \"composite\": \"Title: Arch√§ologie und Photographie: f√ºnfzig Beispiele zur Geschichte und Methode Subjects: Photography in archaeology; Archaeological illustration; Scientific photography Provision information: Mainz: P. von Zabern, 1978\",\n",
    "            \"classification\": \"Documentary and Technical Arts\",\n",
    "            \"recordId\": \"53144\"\n",
    "        },\n",
    "        {\n",
    "            \"personId\": \"53144#Agent700-23\",\n",
    "            \"person\": \"Schubert, Franz August, 1806-1893\",\n",
    "            \"title\": \"Dessauer K√ºnstler des 19. Jahrhunderts\",\n",
    "            \"roles\": \"Artist\",\n",
    "            \"subjects\": \"German artists; 19th century art; Regional art history; Portrait painting\",\n",
    "            \"provision\": \"Dessau: Anhaltische Verlagsgesellschaft, 1925\",\n",
    "            \"composite\": \"Title: Dessauer K√ºnstler des 19. Jahrhunderts Subjects: German artists; 19th century art; Regional art history; Portrait painting Provision information: Dessau: Anhaltische Verlagsgesellschaft, 1925\",\n",
    "            \"classification\": \"Visual Arts\",\n",
    "            \"recordId\": \"53145\"\n",
    "        },\n",
    "        \n",
    "        # Other composers for context\n",
    "        {\n",
    "            \"personId\": \"12345#Agent700-5\",\n",
    "            \"person\": \"Mozart, Wolfgang Amadeus, 1756-1791\",\n",
    "            \"title\": \"Piano Sonata No. 11 in A major, K. 331\",\n",
    "            \"roles\": \"Composer\",\n",
    "            \"subjects\": \"Piano music; Classical period; Sonatas\",\n",
    "            \"provision\": \"Vienna: Artaria, 1784\",\n",
    "            \"composite\": \"Title: Piano Sonata No. 11 in A major, K. 331 Subjects: Piano music; Classical period; Sonatas Provision information: Vienna: Artaria, 1784\",\n",
    "            \"classification\": \"Music and Sound Arts\",\n",
    "            \"recordId\": \"12345\"\n",
    "        },\n",
    "        {\n",
    "            \"personId\": \"67890#Agent700-8\",\n",
    "            \"person\": \"Beethoven, Ludwig van, 1770-1827\",\n",
    "            \"title\": \"Symphony No. 9 in D minor, Op. 125 \\\"Choral\\\"\",\n",
    "            \"roles\": \"Composer\",\n",
    "            \"subjects\": \"Symphonies; Choral symphonies; Classical music; Romantic music\",\n",
    "            \"provision\": \"Mainz: B. Schott's S√∂hne, 1826\",\n",
    "            \"composite\": \"Title: Symphony No. 9 in D minor, Op. 125 \\\"Choral\\\" Subjects: Symphonies; Choral symphonies; Classical music; Romantic music Provision information: Mainz: B. Schott's S√∂hne, 1826\",\n",
    "            \"classification\": \"Music and Sound Arts\",\n",
    "            \"recordId\": \"67890\"\n",
    "        },\n",
    "        \n",
    "        # Archaeology and Art records for domain contrast\n",
    "        {\n",
    "            \"personId\": \"78901#Agent700-12\",\n",
    "            \"person\": \"Winkelmann, Johann Joachim, 1717-1768\",\n",
    "            \"title\": \"Geschichte der Kunst des Alterthums\",\n",
    "            \"roles\": \"Author\",\n",
    "            \"subjects\": \"Art history; Ancient art; Classical archaeology; Greek art; Roman art\",\n",
    "            \"provision\": \"Dresden: Walther, 1764\",\n",
    "            \"composite\": \"Title: Geschichte der Kunst des Alterthums Subjects: Art history; Ancient art; Classical archaeology; Greek art; Roman art Provision information: Dresden: Walther, 1764\",\n",
    "            \"classification\": \"History and Culture\",\n",
    "            \"recordId\": \"78901\"\n",
    "        },\n",
    "        {\n",
    "            \"personId\": \"89012#Agent700-15\",\n",
    "            \"person\": \"Wheeler, Mortimer, 1890-1976\",\n",
    "            \"title\": \"Archaeology from the Earth\",\n",
    "            \"roles\": \"Author\",\n",
    "            \"subjects\": \"Archaeological methods; Excavation techniques; Field archaeology; Stratigraphy\",\n",
    "            \"provision\": \"Oxford: Clarendon Press, 1954\",\n",
    "            \"composite\": \"Title: Archaeology from the Earth Subjects: Archaeological methods; Excavation techniques; Field archaeology; Stratigraphy Provision information: Oxford: Clarendon Press, 1954\",\n",
    "            \"classification\": \"Social Sciences\",\n",
    "            \"recordId\": \"89012\"\n",
    "        },\n",
    "        \n",
    "        # Literature for additional domain diversity\n",
    "        {\n",
    "            \"personId\": \"34567#Agent700-20\",\n",
    "            \"person\": \"Goethe, Johann Wolfgang von, 1749-1832\",\n",
    "            \"title\": \"Faust: eine Trag√∂die\",\n",
    "            \"roles\": \"Author\",\n",
    "            \"subjects\": \"German literature; Drama; Romanticism; Philosophy in literature\",\n",
    "            \"provision\": \"T√ºbingen: Cotta, 1808\",\n",
    "            \"composite\": \"Title: Faust: eine Trag√∂die Subjects: German literature; Drama; Romanticism; Philosophy in literature Provision information: T√ºbingen: Cotta, 1808\",\n",
    "            \"classification\": \"Literature and Narrative Arts\",\n",
    "            \"recordId\": \"34567\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return sample_records\n",
    "\n",
    "# Generate our sample data\n",
    "sample_data = create_sample_library_data()\n",
    "\n",
    "print(f\"üìö Created {len(sample_data)} sample library records\")\n",
    "print(\"\\nüìä Records by classification:\")\n",
    "\n",
    "# Count records by classification\n",
    "classification_counts = {}\n",
    "for record in sample_data:\n",
    "    classification = record[\"classification\"]\n",
    "    classification_counts[classification] = classification_counts.get(classification, 0) + 1\n",
    "\n",
    "for classification, count in classification_counts.items():\n",
    "    print(f\"  ‚Ä¢ {classification}: {count} records\")\n",
    "\n",
    "# Display first record as example\n",
    "print(\"\\nüîç Example record structure:\")\n",
    "for key, value in sample_data[0].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Generating Embeddings with OpenAI\n",
    "\n",
    "Now we need to convert our text records into vector embeddings. We'll use OpenAI's text-embedding-3-small model, which provides excellent performance for semantic search tasks.\n",
    "\n",
    "**Note**: You'll need an OpenAI API key for this section. Add it to your `.env` file as `OPENAI_API_KEY=your_key_here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T21:30:02.058415Z",
     "iopub.status.busy": "2025-06-30T21:30:02.057853Z",
     "iopub.status.idle": "2025-06-30T21:30:03.401985Z",
     "shell.execute_reply": "2025-06-30T21:30:03.401238Z",
     "shell.execute_reply.started": "2025-06-30T21:30:02.058376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: openai 1.12.0\n",
      "Uninstalling openai-1.12.0:\n",
      "  Successfully uninstalled openai-1.12.0\n",
      "Found existing installation: httpx 0.25.2\n",
      "Uninstalling httpx-0.25.2:\n",
      "  Successfully uninstalled httpx-0.25.2\n",
      "Found existing installation: httpcore 1.0.9\n",
      "Uninstalling httpcore-1.0.9:\n",
      "  Successfully uninstalled httpcore-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall openai httpx httpcore -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T21:30:12.000438Z",
     "iopub.status.busy": "2025-06-30T21:30:11.999920Z",
     "iopub.status.idle": "2025-06-30T21:30:15.289769Z",
     "shell.execute_reply": "2025-06-30T21:30:15.289109Z",
     "shell.execute_reply.started": "2025-06-30T21:30:12.000402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 1.3.0 not found\n"
     ]
    }
   ],
   "source": [
    "!pip install openai>=1.3.0 httpx httpcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T21:49:54.041638Z",
     "iopub.status.busy": "2025-06-30T21:49:54.040052Z",
     "iopub.status.idle": "2025-06-30T21:49:54.383622Z",
     "shell.execute_reply": "2025-06-30T21:49:54.362248Z",
     "shell.execute_reply.started": "2025-06-30T21:49:54.041598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client created successfully\n",
      "‚úÖ Generated 9 real embeddings using 384 tokens\n",
      "üîó Successfully attached vectors to 9 records\n",
      "üìö Ready to proceed with Weaviate indexing!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "\n",
    "# Simple, direct OpenAI client setup following your working pattern\n",
    "def setup_openai_client():\n",
    "    \"\"\"Create OpenAI client using the exact pattern from your working code.\"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"‚ö†Ô∏è  No OpenAI API key found\")\n",
    "        return None, False\n",
    "    \n",
    "    try:\n",
    "        # This is exactly how you do it in your working code\n",
    "        openai_client = OpenAI(api_key=api_key)\n",
    "        print(\"‚úÖ OpenAI client created successfully\")\n",
    "        return openai_client, True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Client creation failed: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def get_embeddings(texts: List[str], openai_client=None, model: str = \"text-embedding-3-small\") -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings using your proven API call pattern.\"\"\"\n",
    "    \n",
    "    if openai_client is None:\n",
    "        return create_dummy_embeddings(texts)\n",
    "    \n",
    "    try:\n",
    "        # This follows your exact working pattern from embedding_and_indexing.py\n",
    "        response = openai_client.embeddings.create(\n",
    "            model=model,\n",
    "            input=texts\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings exactly like your working code\n",
    "        embeddings = []\n",
    "        for embedding_data in response.data:\n",
    "            embedding = np.array(embedding_data.embedding, dtype=np.float32)\n",
    "            embeddings.append(embedding.tolist())  # Convert to list for consistency\n",
    "        \n",
    "        # Get token count like your working code\n",
    "        token_count = response.usage.total_tokens\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(embeddings)} real embeddings using {token_count} tokens\")\n",
    "        return embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  API call failed: {e}\")\n",
    "        print(\"Falling back to dummy embeddings...\")\n",
    "        return create_dummy_embeddings(texts)\n",
    "\n",
    "def create_dummy_embeddings(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Create educational dummy embeddings with realistic patterns.\"\"\"\n",
    "    print(\"üìù Creating dummy embeddings...\")\n",
    "    \n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        # Deterministic embeddings based on text content\n",
    "        np.random.seed(hash(text) % 2147483647)\n",
    "        embedding = np.random.normal(0, 0.1, 1536).tolist()\n",
    "        \n",
    "        # Add semantic clustering based on content\n",
    "        text_lower = text.lower()\n",
    "        if any(word in text_lower for word in [\"music\", \"symphony\", \"composer\"]):\n",
    "            for i in range(0, 100):\n",
    "                embedding[i] += 0.4\n",
    "        elif any(word in text_lower for word in [\"archaeology\", \"photography\", \"art\"]):\n",
    "            for i in range(200, 300):\n",
    "                embedding[i] += 0.4\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    print(f\"üìä Created {len(embeddings)} dummy embeddings\")\n",
    "    return embeddings\n",
    "\n",
    "# Execute the setup using your proven approach\n",
    "openai_client, has_real_api = setup_openai_client()\n",
    "\n",
    "# Generate embeddings for your data\n",
    "texts_to_embed = [record[\"composite\"] for record in sample_data]\n",
    "embeddings = get_embeddings(texts_to_embed, openai_client)\n",
    "\n",
    "# Attach to records\n",
    "for i, record in enumerate(sample_data):\n",
    "    record[\"vector\"] = embeddings[i]\n",
    "\n",
    "print(f\"üîó Successfully attached vectors to {len(sample_data)} records\")\n",
    "print(\"üìö Ready to proceed with Weaviate indexing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Indexing Records in Weaviate\n",
    "\n",
    "Now comes the exciting part - loading our embedded records into Weaviate! This is where the magic happens: Weaviate will build an efficient index structure that allows for fast similarity searches across our entire collection.\n",
    "\n",
    "The HNSW (Hierarchical Navigable Small World) algorithm creates a graph structure that can find similar vectors in logarithmic time, even with millions of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T21:49:59.633548Z",
     "iopub.status.busy": "2025-06-30T21:49:59.632456Z",
     "iopub.status.idle": "2025-06-30T21:49:59.683924Z",
     "shell.execute_reply": "2025-06-30T21:49:59.683393Z",
     "shell.execute_reply.started": "2025-06-30T21:49:59.633468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Starting to index 9 records...\n",
      "  üìã Prepared 2/9 records\n",
      "  üìã Prepared 4/9 records\n",
      "  üìã Prepared 6/9 records\n",
      "  üìã Prepared 8/9 records\n",
      "\n",
      "üöÄ Inserting 9 records into Weaviate...\n",
      "‚úÖ Successfully indexed 9/9 records\n",
      "üìä Total objects in collection: 9\n",
      "\n",
      "üéâ Indexing completed successfully!\n",
      "üìö Your library collection is now ready for semantic search\n",
      "\n",
      "üîç Sample indexed record:\n",
      "  UUID: 1099f26d-99f1-5080-8efd-ede20ae10695\n",
      "  Person: Winkelmann, Johann Joachim, 1717-1768\n",
      "  Title: Geschichte der Kunst des Alterthums\n",
      "  Classification: History and Culture\n",
      "  Vector dimension: N/A\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.data import DataObject\n",
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "def index_library_records(client: weaviate.WeaviateClient, records: List[Dict[str, Any]]) -> bool:\n",
    "    \"\"\"\n",
    "    Index library records into Weaviate with their embeddings.\n",
    "    \n",
    "    Args:\n",
    "        client: Weaviate client instance\n",
    "        records: List of library records with embeddings\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get our collection\n",
    "        collection = weaviate_client.collections.get(COLLECTION_NAME)\n",
    "        \n",
    "        print(f\"üì§ Starting to index {len(records)} records...\")\n",
    "        \n",
    "        # Prepare data objects for batch insertion\n",
    "        data_objects = []\n",
    "        \n",
    "        for i, record in enumerate(records):\n",
    "            # Extract the vector (embedding)\n",
    "            vector = record.pop(\"vector\")  # Remove vector from properties\n",
    "            \n",
    "            # Generate a consistent UUID based on the record ID\n",
    "            # This ensures we can update the same record later if needed\n",
    "            uuid = generate_uuid5(record[\"recordId\"], \"LibraryRecord\")\n",
    "            \n",
    "            # Create data object\n",
    "            data_object = DataObject(\n",
    "                properties=record,  # All the metadata fields\n",
    "                vector=vector,      # The embedding vector\n",
    "                uuid=uuid          # Consistent UUID\n",
    "            )\n",
    "            \n",
    "            data_objects.append(data_object)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 2 == 0:\n",
    "                print(f\"  üìã Prepared {i + 1}/{len(records)} records\")\n",
    "        \n",
    "        # Batch insert all records\n",
    "        print(f\"\\nüöÄ Inserting {len(data_objects)} records into Weaviate...\")\n",
    "        \n",
    "        # Use batch insertion for efficiency\n",
    "        response = collection.data.insert_many(data_objects)\n",
    "        \n",
    "        # Check for any errors\n",
    "        if response.has_errors:\n",
    "            print(f\"‚ö†Ô∏è  Some records had errors:\")\n",
    "            for i, error in enumerate(response.errors):\n",
    "                if error:\n",
    "                    print(f\"  Record {i}: {error}\")\n",
    "        \n",
    "        successful_inserts = len([r for r in response.uuids if r is not None])\n",
    "        print(f\"‚úÖ Successfully indexed {successful_inserts}/{len(records)} records\")\n",
    "        \n",
    "        # Verify the data was inserted\n",
    "        total_objects = collection.aggregate.over_all(total_count=True).total_count\n",
    "        print(f\"üìä Total objects in collection: {total_objects}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error indexing records: {e}\")\n",
    "        return False\n",
    "\n",
    "# Index our sample records\n",
    "success = index_library_records(weaviate_client, sample_data)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ Indexing completed successfully!\")\n",
    "    print(\"üìö Your library collection is now ready for semantic search\")\n",
    "    \n",
    "    # Quick verification: let's see what's in our collection\n",
    "    collection = weaviate_client.collections.get(COLLECTION_NAME)\n",
    "    \n",
    "    # Get a sample record to verify structure\n",
    "    sample_result = collection.query.fetch_objects(limit=1)\n",
    "    \n",
    "    if sample_result.objects:\n",
    "        sample_object = sample_result.objects[0]\n",
    "        print(f\"\\nüîç Sample indexed record:\")\n",
    "        print(f\"  UUID: {sample_object.uuid}\")\n",
    "        print(f\"  Person: {sample_object.properties.get('person', 'N/A')}\")\n",
    "        print(f\"  Title: {sample_object.properties.get('title', 'N/A')}\")\n",
    "        print(f\"  Classification: {sample_object.properties.get('classification', 'N/A')}\")\n",
    "        print(f\"  Vector dimension: {len(sample_object.vector) if sample_object.vector else 'N/A'}\")\n",
    "else:\n",
    "    print(\"‚ùå Indexing failed. Please check the error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Basic Semantic Search Queries\n",
    "\n",
    "Now for the exciting part - let's search our library collection using semantic similarity! We'll start with basic vector searches and then explore more sophisticated querying techniques.\n",
    "\n",
    "Vector search finds records that are semantically similar to your query, even if they don't contain the exact same words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T21:50:06.010086Z",
     "iopub.status.busy": "2025-06-30T21:50:06.009787Z",
     "iopub.status.idle": "2025-06-30T21:50:08.579324Z",
     "shell.execute_reply": "2025-06-30T21:50:08.578383Z",
     "shell.execute_reply.started": "2025-06-30T21:50:06.010065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for: 'classical music composer symphonies'\n",
      "üìù Creating dummy embeddings...\n",
      "üìä Created 1 dummy embeddings\n",
      "\n",
      "üìä Search Results for: 'classical music composer symphonies'\n",
      "============================================================\n",
      "\n",
      "1. üìö Winkelmann, Johann Joachim, 1717-1768\n",
      "   üìñ Title: Geschichte der Kunst des Alterthums\n",
      "   üè∑Ô∏è  Classification: History and Culture\n",
      "   üîñ Subjects: Art history; Ancient art; Classical archaeology; Greek art; Roman art\n",
      "   üìè Similarity: 6.3%\n",
      "\n",
      "2. üìö Schubert, Franz August, 1806-1893\n",
      "   üìñ Title: Arch√§ologie und Photographie: f√ºnfzig Beispiele zur Geschichte und Methode\n",
      "   üè∑Ô∏è  Classification: Documentary and Technical Arts\n",
      "   üîñ Subjects: Photography in archaeology; Archaeological illustration; Scientific photography\n",
      "   üìè Similarity: 5.6%\n",
      "\n",
      "3. üìö Schubert, Franz August, 1806-1893\n",
      "   üìñ Title: Dessauer K√ºnstler des 19. Jahrhunderts\n",
      "   üè∑Ô∏è  Classification: Visual Arts\n",
      "   üîñ Subjects: German artists; 19th century art; Regional art history; Portrait painting\n",
      "   üìè Similarity: 4.7%\n",
      "üîç Searching for: 'archaeological photography documentation'\n",
      "üìù Creating dummy embeddings...\n",
      "üìä Created 1 dummy embeddings\n",
      "\n",
      "üìä Search Results for: 'archaeological photography documentation'\n",
      "============================================================\n",
      "\n",
      "1. üìö Mozart, Wolfgang Amadeus, 1756-1791\n",
      "   üìñ Title: Piano Sonata No. 11 in A major, K. 331\n",
      "   üè∑Ô∏è  Classification: Music and Sound Arts\n",
      "   üîñ Subjects: Piano music; Classical period; Sonatas\n",
      "   üìè Similarity: 0.7%\n",
      "\n",
      "2. üìö Schubert, Franz, 1797-1828\n",
      "   üìñ Title: Symphony no. 8 in B minor, D. 759 \"Unfinished\"\n",
      "   üè∑Ô∏è  Classification: Music and Sound Arts\n",
      "   üîñ Subjects: Symphonies; Classical music; Romantic period music\n",
      "   üìè Similarity: 0.0%\n",
      "\n",
      "3. üìö Wheeler, Mortimer, 1890-1976\n",
      "   üìñ Title: Archaeology from the Earth\n",
      "   üè∑Ô∏è  Classification: Social Sciences\n",
      "   üîñ Subjects: Archaeological methods; Excavation techniques; Field archaeology; Stratigraphy\n",
      "   üìè Similarity: 0.0%\n",
      "üîç Searching for: 'German romantic period art'\n",
      "üìù Creating dummy embeddings...\n",
      "üìä Created 1 dummy embeddings\n",
      "\n",
      "üìä Search Results for: 'German romantic period art'\n",
      "============================================================\n",
      "\n",
      "1. üìö Schubert, Franz August, 1806-1893\n",
      "   üìñ Title: Arch√§ologie und Photographie: f√ºnfzig Beispiele zur Geschichte und Methode\n",
      "   üè∑Ô∏è  Classification: Documentary and Technical Arts\n",
      "   üîñ Subjects: Photography in archaeology; Archaeological illustration; Scientific photography\n",
      "   üìè Similarity: 0.0%\n",
      "\n",
      "2. üìö Wheeler, Mortimer, 1890-1976\n",
      "   üìñ Title: Archaeology from the Earth\n",
      "   üè∑Ô∏è  Classification: Social Sciences\n",
      "   üîñ Subjects: Archaeological methods; Excavation techniques; Field archaeology; Stratigraphy\n",
      "   üìè Similarity: 0.0%\n",
      "\n",
      "3. üìö Winkelmann, Johann Joachim, 1717-1768\n",
      "   üìñ Title: Geschichte der Kunst des Alterthums\n",
      "   üè∑Ô∏è  Classification: History and Culture\n",
      "   üîñ Subjects: Art history; Ancient art; Classical archaeology; Greek art; Roman art\n",
      "   üìè Similarity: 0.0%\n",
      "üîç Searching for: 'piano sonatas and musical compositions'\n",
      "üìù Creating dummy embeddings...\n",
      "üìä Created 1 dummy embeddings\n",
      "\n",
      "üìä Search Results for: 'piano sonatas and musical compositions'\n",
      "============================================================\n",
      "\n",
      "1. üìö Wheeler, Mortimer, 1890-1976\n",
      "   üìñ Title: Archaeology from the Earth\n",
      "   üè∑Ô∏è  Classification: Social Sciences\n",
      "   üîñ Subjects: Archaeological methods; Excavation techniques; Field archaeology; Stratigraphy\n",
      "   üìè Similarity: 1.5%\n",
      "\n",
      "2. üìö Schubert, Franz August, 1806-1893\n",
      "   üìñ Title: Arch√§ologie und Photographie: f√ºnfzig Beispiele zur Geschichte und Methode\n",
      "   üè∑Ô∏è  Classification: Documentary and Technical Arts\n",
      "   üîñ Subjects: Photography in archaeology; Archaeological illustration; Scientific photography\n",
      "   üìè Similarity: 0.7%\n",
      "\n",
      "3. üìö Winkelmann, Johann Joachim, 1717-1768\n",
      "   üìñ Title: Geschichte der Kunst des Alterthums\n",
      "   üè∑Ô∏è  Classification: History and Culture\n",
      "   üîñ Subjects: Art history; Ancient art; Classical archaeology; Greek art; Roman art\n",
      "   üìè Similarity: 0.0%\n",
      "üîç Searching for: 'ancient art history and classical civilizations'\n",
      "üìù Creating dummy embeddings...\n",
      "üìä Created 1 dummy embeddings\n",
      "\n",
      "üìä Search Results for: 'ancient art history and classical civilizations'\n",
      "============================================================\n",
      "\n",
      "1. üìö Wheeler, Mortimer, 1890-1976\n",
      "   üìñ Title: Archaeology from the Earth\n",
      "   üè∑Ô∏è  Classification: Social Sciences\n",
      "   üîñ Subjects: Archaeological methods; Excavation techniques; Field archaeology; Stratigraphy\n",
      "   üìè Similarity: 0.2%\n",
      "\n",
      "2. üìö Goethe, Johann Wolfgang von, 1749-1832\n",
      "   üìñ Title: Faust: eine Trag√∂die\n",
      "   üè∑Ô∏è  Classification: Literature and Narrative Arts\n",
      "   üîñ Subjects: German literature; Drama; Romanticism; Philosophy in literature\n",
      "   üìè Similarity: 0.0%\n",
      "\n",
      "3. üìö Schubert, Franz August, 1806-1893\n",
      "   üìñ Title: Arch√§ologie und Photographie: f√ºnfzig Beispiele zur Geschichte und Methode\n",
      "   üè∑Ô∏è  Classification: Documentary and Technical Arts\n",
      "   üîñ Subjects: Photography in archaeology; Archaeological illustration; Scientific photography\n",
      "   üìè Similarity: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def semantic_search(weaviate_client: weaviate.WeaviateClient, query_text: str, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform semantic search using vector similarity.\n",
    "    \n",
    "    Args:\n",
    "        weaviate_client: Weaviate client\n",
    "        query_text: Text to search for\n",
    "        limit: Maximum number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of matching records with similarity scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First, we need to embed the query text\n",
    "        print(f\"üîç Searching for: '{query_text}'\")\n",
    "        query_embedding = get_embeddings([query_text])[0]\n",
    "        \n",
    "        # Get our collection\n",
    "        collection = weaviate_client.collections.get(COLLECTION_NAME)\n",
    "        \n",
    "        # Perform vector search\n",
    "        response = collection.query.near_vector(\n",
    "            near_vector=query_embedding,\n",
    "            limit=limit,\n",
    "            return_metadata=MetadataQuery(score=True, distance=True)\n",
    "        )\n",
    "        \n",
    "        # Process results\n",
    "        results = []\n",
    "        for obj in response.objects:\n",
    "            result = {\n",
    "                \"uuid\": str(obj.uuid),\n",
    "                \"score\": obj.metadata.score if obj.metadata.score else 0,\n",
    "                \"distance\": obj.metadata.distance if obj.metadata.distance else 1,\n",
    "                \"person\": obj.properties.get(\"person\", \"Unknown\"),\n",
    "                \"title\": obj.properties.get(\"title\", \"Unknown\"),\n",
    "                \"classification\": obj.properties.get(\"classification\", \"Unknown\"),\n",
    "                \"subjects\": obj.properties.get(\"subjects\", \"Unknown\"),\n",
    "                \"composite\": obj.properties.get(\"composite\", \"Unknown\")\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Search error: {e}\")\n",
    "        return []\n",
    "\n",
    "def display_search_results(results: List[Dict[str, Any]], query: str):\n",
    "    \"\"\"Display search results u a formatted way.\"\"\"\n",
    "    print(f\"\\nüìä Search Results for: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå No results found\")\n",
    "        return\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        # Calculate similarity percentage (1 - distance for cosine)\n",
    "        similarity = (1 - result[\"distance\"]) * 100 if result[\"distance\"] <= 1 else 0\n",
    "        \n",
    "        print(f\"\\n{i}. üìö {result['person']}\")\n",
    "        print(f\"   üìñ Title: {result['title']}\")\n",
    "        print(f\"   üè∑Ô∏è  Classification: {result['classification']}\")\n",
    "        print(f\"   üîñ Subjects: {result['subjects'][:100]}{'...' if len(result['subjects']) > 100 else ''}\")\n",
    "        print(f\"   üìè Similarity: {similarity:.1f}%\")\n",
    "\n",
    "# Test searches with different types of queries\n",
    "test_queries = [\n",
    "    \"classical music composer symphonies\",\n",
    "    \"archaeological photography documentation\", \n",
    "    \"German romantic period art\",\n",
    "    \"piano sonatas and musical compositions\",\n",
    "    \"ancient art history and classical civilizations\"\n",
    "]\n",
    "\n",
    "# Perform searches\n",
    "for query in test_queries:\n",
    "    results = semantic_search(weaviate_client, query, limit=3)\n",
    "    display_search_results(results, query)\n",
    "    \n",
    "    # Small delay between searches for readability\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Advanced Query Techniques\n",
    "\n",
    "Weaviate's power really shines when you combine vector search with traditional filters and complex queries. Let's explore some advanced techniques that are particularly useful for library applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate.classes.query as wq\n",
    "\n",
    "def hybrid_search(client: weaviate.WeaviateClient, \n",
    "                 query_text: str, \n",
    "                 classification_filter: Optional[str] = None,\n",
    "                 person_filter: Optional[str] = None,\n",
    "                 limit: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform hybrid search combining vector similarity with metadata filters.\n",
    "    \n",
    "    Args:\n",
    "        client: Weaviate client\n",
    "        query_text: Text to search for semantically\n",
    "        classification_filter: Filter by classification (e.g., \"Music and Sound Arts\")\n",
    "        person_filter: Filter by person name (contains match)\n",
    "        limit: Maximum results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of matching records\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate query embedding\n",
    "        query_embedding = get_embeddings([query_text])[0]\n",
    "        \n",
    "        # Get collection\n",
    "        collection = client.collections.get(COLLECTION_NAME)\n",
    "        \n",
    "        # Build filter conditions\n",
    "        filters = []\n",
    "        \n",
    "        if classification_filter:\n",
    "            filters.append(\n",
    "                wq.Filter.by_property(\"classification\").equal(classification_filter)\n",
    "            )\n",
    "        \n",
    "        if person_filter:\n",
    "            filters.append(\n",
    "                wq.Filter.by_property(\"person\").contains_any([person_filter])\n",
    "            )\n",
    "        \n",
    "        # Combine filters with AND logic if multiple filters\n",
    "        combined_filter = None\n",
    "        if len(filters) == 1:\n",
    "            combined_filter = filters[0]\n",
    "        elif len(filters) > 1:\n",
    "            combined_filter = wq.Filter.all_of(filters)\n",
    "        \n",
    "        # Perform the search\n",
    "        if combined_filter:\n",
    "            response = collection.query.near_vector(\n",
    "                near_vector=query_embedding,\n",
    "                where=combined_filter,\n",
    "                limit=limit,\n",
    "                return_metadata=MetadataQuery(score=True, distance=True)\n",
    "            )\n",
    "        else:\n",
    "            response = collection.query.near_vector(\n",
    "                near_vector=query_embedding,\n",
    "                limit=limit,\n",
    "                return_metadata=MetadataQuery(score=True, distance=True)\n",
    "            )\n",
    "        \n",
    "        # Process results\n",
    "        results = []\n",
    "        for obj in response.objects:\n",
    "            result = {\n",
    "                \"uuid\": str(obj.uuid),\n",
    "                \"score\": obj.metadata.score if obj.metadata.score else 0,\n",
    "                \"distance\": obj.metadata.distance if obj.metadata.distance else 1,\n",
    "                \"person\": obj.properties.get(\"person\", \"Unknown\"),\n",
    "                \"title\": obj.properties.get(\"title\", \"Unknown\"),\n",
    "                \"classification\": obj.properties.get(\"classification\", \"Unknown\"),\n",
    "                \"subjects\": obj.properties.get(\"subjects\", \"Unknown\"),\n",
    "                \"roles\": obj.properties.get(\"roles\", \"Unknown\")\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Hybrid search error: {e}\")\n",
    "        return []\n",
    "\n",
    "def entity_disambiguation_query(client: weaviate.WeaviateClient, \n",
    "                               person_name: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Find all records for a specific person to help with entity disambiguation.\n",
    "    \n",
    "    Args:\n",
    "        client: Weaviate client\n",
    "        person_name: Name to search for (e.g., \"Schubert\")\n",
    "    \n",
    "    Returns:\n",
    "        List of all records matching the person name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        collection = client.collections.get(COLLECTION_NAME)\n",
    "        \n",
    "        # Search for records containing the person name\n",
    "        response = collection.query.fetch_objects(\n",
    "            where=wq.Filter.by_property(\"person\").contains_any([person_name]),\n",
    "            limit=20  # Get more results for disambiguation\n",
    "        )\n",
    "        \n",
    "        # Process and group results\n",
    "        results = []\n",
    "        for obj in response.objects:\n",
    "            result = {\n",
    "                \"personId\": obj.properties.get(\"personId\", \"Unknown\"),\n",
    "                \"person\": obj.properties.get(\"person\", \"Unknown\"),\n",
    "                \"title\": obj.properties.get(\"title\", \"Unknown\"),\n",
    "                \"classification\": obj.properties.get(\"classification\", \"Unknown\"),\n",
    "                \"subjects\": obj.properties.get(\"subjects\", \"Unknown\"),\n",
    "                \"roles\": obj.properties.get(\"roles\", \"Unknown\"),\n",
    "                \"provision\": obj.properties.get(\"provision\", \"Unknown\")\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Entity disambiguation error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test advanced queries\n",
    "print(\"üî¨ Advanced Query Examples\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Hybrid search: Music + semantic similarity\n",
    "print(\"\\n1Ô∏è‚É£ Hybrid Search: Music domain + 'romantic composition'\")\n",
    "results = hybrid_search(client, \n",
    "                       query_text=\"romantic composition\", \n",
    "                       classification_filter=\"Music and Sound Arts\",\n",
    "                       limit=3)\n",
    "display_search_results(results, \"romantic composition [Music domain only]\")\n",
    "\n",
    "# 2. Entity disambiguation for \"Schubert\"\n",
    "print(\"\\n\\n2Ô∏è‚É£ Entity Disambiguation: All 'Schubert' records\")\n",
    "schubert_records = entity_disambiguation_query(client, \"Schubert\")\n",
    "\n",
    "print(f\"\\nüìä Found {len(schubert_records)} Schubert records\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Group by person ID to show distinct entities\n",
    "schubert_entities = {}\n",
    "for record in schubert_records:\n",
    "    person_id = record[\"personId\"]\n",
    "    if person_id not in schubert_entities:\n",
    "        schubert_entities[person_id] = []\n",
    "    schubert_entities[person_id].append(record)\n",
    "\n",
    "for person_id, records in schubert_entities.items():\n",
    "    person_name = records[0][\"person\"]\n",
    "    classifications = set(r[\"classification\"] for r in records)\n",
    "    \n",
    "    print(f\"\\nüë§ {person_name}\")\n",
    "    print(f\"   üÜî ID: {person_id}\")\n",
    "    print(f\"   üìö Works: {len(records)}\")\n",
    "    print(f\"   üè∑Ô∏è  Domains: {', '.join(classifications)}\")\n",
    "    \n",
    "    for i, record in enumerate(records[:2]):  # Show first 2 works\n",
    "        print(f\"     {i+1}. {record['title'][:50]}{'...' if len(record['title']) > 50 else ''}\")\n",
    "    \n",
    "    if len(records) > 2:\n",
    "        print(f\"     ... and {len(records) - 2} more works\")\n",
    "\n",
    "# 3. Cross-domain similarity search\n",
    "print(\"\\n\\n3Ô∏è‚É£ Cross-Domain Search: 'German cultural heritage'\")\n",
    "results = semantic_search(client, \"German cultural heritage\", limit=5)\n",
    "display_search_results(results, \"German cultural heritage\")\n",
    "\n",
    "print(\"\\nüí° Notice how the search finds relevant records across different domains!\")\n",
    "print(\"   This demonstrates the power of semantic understanding vs. keyword matching.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Similarity Analysis and Clustering\n",
    "\n",
    "Let's explore the relationships between our records by analyzing their vector similarities. This will help us understand how well our embeddings capture semantic relationships and identify potential entity resolution opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_collection_similarities(client: weaviate.WeaviateClient) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze similarities between all records in the collection.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with pairwise similarity analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch all records with their vectors\n",
    "        collection = client.collections.get(COLLECTION_NAME)\n",
    "        response = collection.query.fetch_objects(\n",
    "            limit=100,  # Get all our records\n",
    "            include_vector=True\n",
    "        )\n",
    "        \n",
    "        if not response.objects:\n",
    "            print(\"‚ùå No objects found in collection\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Extract data for analysis\n",
    "        records_data = []\n",
    "        vectors = []\n",
    "        \n",
    "        for obj in response.objects:\n",
    "            record_info = {\n",
    "                \"uuid\": str(obj.uuid),\n",
    "                \"person\": obj.properties.get(\"person\", \"Unknown\"),\n",
    "                \"title\": obj.properties.get(\"title\", \"Unknown\"),\n",
    "                \"classification\": obj.properties.get(\"classification\", \"Unknown\"),\n",
    "                \"personId\": obj.properties.get(\"personId\", \"Unknown\")\n",
    "            }\n",
    "            records_data.append(record_info)\n",
    "            vectors.append(obj.vector)\n",
    "        \n",
    "        # Convert to numpy arrays for efficient computation\n",
    "        vectors_array = np.array(vectors)\n",
    "        \n",
    "        # Calculate pairwise cosine similarities\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        similarity_matrix = cosine_similarity(vectors_array)\n",
    "        \n",
    "        # Create detailed similarity analysis\n",
    "        similarity_pairs = []\n",
    "        \n",
    "        for i in range(len(records_data)):\n",
    "            for j in range(i + 1, len(records_data)):\n",
    "                similarity = similarity_matrix[i][j]\n",
    "                \n",
    "                pair_info = {\n",
    "                    \"record1_person\": records_data[i][\"person\"],\n",
    "                    \"record1_title\": records_data[i][\"title\"],\n",
    "                    \"record1_classification\": records_data[i][\"classification\"],\n",
    "                    \"record1_personId\": records_data[i][\"personId\"],\n",
    "                    \"record2_person\": records_data[j][\"person\"],\n",
    "                    \"record2_title\": records_data[j][\"title\"],\n",
    "                    \"record2_classification\": records_data[j][\"classification\"],\n",
    "                    \"record2_personId\": records_data[j][\"personId\"],\n",
    "                    \"similarity\": similarity,\n",
    "                    \"same_person\": records_data[i][\"personId\"] == records_data[j][\"personId\"],\n",
    "                    \"same_classification\": records_data[i][\"classification\"] == records_data[j][\"classification\"]\n",
    "                }\n",
    "                \n",
    "                similarity_pairs.append(pair_info)\n",
    "        \n",
    "        return pd.DataFrame(similarity_pairs)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing similarities: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Perform similarity analysis\n",
    "print(\"üîç Analyzing collection similarities...\")\n",
    "similarity_df = analyze_collection_similarities(client)\n",
    "\n",
    "if not similarity_df.empty:\n",
    "    print(f\"‚úÖ Analyzed {len(similarity_df)} record pairs\")\n",
    "    \n",
    "    # Show highest similarities\n",
    "    print(\"\\nüî• Highest Similarity Pairs:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    top_similarities = similarity_df.nlargest(5, 'similarity')\n",
    "    \n",
    "    for idx, row in top_similarities.iterrows():\n",
    "        person_match = \"‚úÖ Same person\" if row['same_person'] else \"‚ùå Different people\"\n",
    "        domain_match = \"‚úÖ Same domain\" if row['same_classification'] else \"üîÑ Cross-domain\"\n",
    "        \n",
    "        print(f\"\\nüìä Similarity: {row['similarity']:.3f}\")\n",
    "        print(f\"   üë§ {row['record1_person']}\")\n",
    "        print(f\"      üìñ {row['record1_title'][:60]}{'...' if len(row['record1_title']) > 60 else ''}\")\n",
    "        print(f\"   ‚ÜïÔ∏è\")\n",
    "        print(f\"   üë§ {row['record2_person']}\")\n",
    "        print(f\"      üìñ {row['record2_title'][:60]}{'...' if len(row['record2_title']) > 60 else ''}\")\n",
    "        print(f\"   üéØ {person_match} | {domain_match}\")\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(\"\\n\\nüìà Statistical Analysis:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Same person similarities\n",
    "    same_person_sims = similarity_df[similarity_df['same_person']]['similarity']\n",
    "    diff_person_sims = similarity_df[~similarity_df['same_person']]['similarity']\n",
    "    \n",
    "    print(f\"üìä Same Person Pairs:\")\n",
    "    print(f\"   Count: {len(same_person_sims)}\")\n",
    "    if len(same_person_sims) > 0:\n",
    "        print(f\"   Average similarity: {same_person_sims.mean():.3f}\")\n",
    "        print(f\"   Similarity range: {same_person_sims.min():.3f} - {same_person_sims.max():.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Different Person Pairs:\")\n",
    "    print(f\"   Count: {len(diff_person_sims)}\")\n",
    "    if len(diff_person_sims) > 0:\n",
    "        print(f\"   Average similarity: {diff_person_sims.mean():.3f}\")\n",
    "        print(f\"   Similarity range: {diff_person_sims.min():.3f} - {diff_person_sims.max():.3f}\")\n",
    "    \n",
    "    # Domain analysis\n",
    "    same_domain_sims = similarity_df[similarity_df['same_classification']]['similarity']\n",
    "    diff_domain_sims = similarity_df[~similarity_df['same_classification']]['similarity']\n",
    "    \n",
    "    print(f\"\\nüìä Same Domain Pairs:\")\n",
    "    print(f\"   Count: {len(same_domain_sims)}\")\n",
    "    if len(same_domain_sims) > 0:\n",
    "        print(f\"   Average similarity: {same_domain_sims.mean():.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Cross-Domain Pairs:\")\n",
    "    print(f\"   Count: {len(diff_domain_sims)}\")\n",
    "    if len(diff_domain_sims) > 0:\n",
    "        print(f\"   Average similarity: {diff_domain_sims.mean():.3f}\")\n",
    "else:\n",
    "    print(\"‚ùå Could not perform similarity analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Visualization and Insights\n",
    "\n",
    "Let's create visualizations to better understand the semantic structure of our library collection and how different domains cluster in the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_collection_embeddings(client: weaviate.WeaviateClient):\n",
    "    \"\"\"\n",
    "    Create visualizations of the collection's embedding space.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetch all records with vectors\n",
    "        collection = client.collections.get(COLLECTION_NAME)\n",
    "        response = collection.query.fetch_objects(\n",
    "            limit=100,\n",
    "            include_vector=True\n",
    "        )\n",
    "        \n",
    "        if not response.objects:\n",
    "            print(\"‚ùå No objects found for visualization\")\n",
    "            return\n",
    "        \n",
    "        # Extract data\n",
    "        vectors = []\n",
    "        labels = []\n",
    "        classifications = []\n",
    "        persons = []\n",
    "        \n",
    "        for obj in response.objects:\n",
    "            vectors.append(obj.vector)\n",
    "            person = obj.properties.get(\"person\", \"Unknown\")\n",
    "            # Shorten person names for visualization\n",
    "            person_short = person.split(\",\")[0] if \",\" in person else person\n",
    "            labels.append(person_short)\n",
    "            classifications.append(obj.properties.get(\"classification\", \"Unknown\"))\n",
    "            persons.append(person)\n",
    "        \n",
    "        vectors_array = np.array(vectors)\n",
    "        \n",
    "        # Reduce dimensionality using PCA\n",
    "        print(\"üîÑ Reducing dimensionality with PCA...\")\n",
    "        pca = PCA(n_components=2)\n",
    "        vectors_2d = pca.fit_transform(vectors_array)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Plot 1: Color by classification\n",
    "        unique_classifications = list(set(classifications))\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_classifications)))\n",
    "        color_map = dict(zip(unique_classifications, colors))\n",
    "        \n",
    "        for i, classification in enumerate(classifications):\n",
    "            ax1.scatter(vectors_2d[i, 0], vectors_2d[i, 1], \n",
    "                       c=[color_map[classification]], \n",
    "                       s=100, alpha=0.7)\n",
    "            ax1.annotate(labels[i], \n",
    "                        (vectors_2d[i, 0], vectors_2d[i, 1]),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8, alpha=0.8)\n",
    "        \n",
    "        ax1.set_title('Library Records in 2D Embedding Space\\n(Colored by Classification)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('First Principal Component')\n",
    "        ax1.set_ylabel('Second Principal Component')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Create legend for classifications\n",
    "        legend_elements = [plt.scatter([], [], c=[color_map[cls]], s=100, label=cls) \n",
    "                          for cls in unique_classifications]\n",
    "        ax1.legend(handles=legend_elements, loc='best', fontsize=10)\n",
    "        \n",
    "        # Plot 2: Highlight Franz Schubert entities\n",
    "        for i, person in enumerate(persons):\n",
    "            if \"Schubert\" in person:\n",
    "                if \"1797-1828\" in person:\n",
    "                    color = 'red'\n",
    "                    marker = 'o'\n",
    "                    label = 'Franz Schubert (Composer)' if i == 0 else \"\"\n",
    "                elif \"1806-1893\" in person:\n",
    "                    color = 'blue'\n",
    "                    marker = 's'\n",
    "                    label = 'Franz August Schubert (Artist)' if persons[:i].count(person) == 0 else \"\"\n",
    "                else:\n",
    "                    color = 'purple'\n",
    "                    marker = '^'\n",
    "                    label = 'Other Schubert'\n",
    "            else:\n",
    "                color = 'gray'\n",
    "                marker = 'o'\n",
    "                label = 'Other Authors' if i == 0 else \"\"\n",
    "            \n",
    "            ax2.scatter(vectors_2d[i, 0], vectors_2d[i, 1], \n",
    "                       c=color, marker=marker, s=120, alpha=0.7, \n",
    "                       label=label if label else \"\")\n",
    "            \n",
    "            # Annotate Schubert records\n",
    "            if \"Schubert\" in person:\n",
    "                ax2.annotate(labels[i], \n",
    "                           (vectors_2d[i, 0], vectors_2d[i, 1]),\n",
    "                           xytext=(5, 5), textcoords='offset points',\n",
    "                           fontsize=9, fontweight='bold')\n",
    "        \n",
    "        ax2.set_title('Entity Disambiguation: Franz Schubert Records\\n(Red=Composer, Blue=Artist)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('First Principal Component')\n",
    "        ax2.set_ylabel('Second Principal Component')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend(loc='best', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print analysis\n",
    "        print(f\"\\nüìä Visualization Analysis:\")\n",
    "        print(f\"‚úÖ Plotted {len(vectors)} records in 2D space\")\n",
    "        print(f\"üìè PCA explained variance: {pca.explained_variance_ratio_}\")\n",
    "        print(f\"üìà Total variance captured: {sum(pca.explained_variance_ratio_):.1%}\")\n",
    "        \n",
    "        # Calculate Schubert separation\n",
    "        schubert_composer_indices = [i for i, p in enumerate(persons) if \"1797-1828\" in p]\n",
    "        schubert_artist_indices = [i for i, p in enumerate(persons) if \"1806-1893\" in p]\n",
    "        \n",
    "        if schubert_composer_indices and schubert_artist_indices:\n",
    "            composer_centroid = np.mean(vectors_2d[schubert_composer_indices], axis=0)\n",
    "            artist_centroid = np.mean(vectors_2d[schubert_artist_indices], axis=0)\n",
    "            separation = np.linalg.norm(composer_centroid - artist_centroid)\n",
    "            \n",
    "            print(f\"\\nüéØ Franz Schubert Entity Separation:\")\n",
    "            print(f\"   Distance between composer and artist centroids: {separation:.3f}\")\n",
    "            print(f\"   This demonstrates how embeddings can distinguish same-name entities!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Visualization error: {e}\")\n",
    "\n",
    "# Create the visualization\n",
    "visualize_collection_embeddings(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Practical Applications Summary\n",
    "\n",
    "Let's conclude by summarizing the practical applications of what we've learned and provide guidance for implementing these techniques in real library systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_implementation_report(client: weaviate.WeaviateClient):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive report on our Weaviate implementation.\n",
    "    \"\"\"\n",
    "    print(\"üìã WEAVIATE IMPLEMENTATION REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Collection statistics\n",
    "        collection = client.collections.get(COLLECTION_NAME)\n",
    "        total_objects = collection.aggregate.over_all(total_count=True).total_count\n",
    "        \n",
    "        print(f\"\\nüìä Collection Statistics:\")\n",
    "        print(f\"   ‚Ä¢ Total records indexed: {total_objects}\")\n",
    "        print(f\"   ‚Ä¢ Vector dimensions: 1536 (OpenAI text-embedding-3-small)\")\n",
    "        print(f\"   ‚Ä¢ Index type: HNSW (Hierarchical Navigable Small World)\")\n",
    "        print(f\"   ‚Ä¢ Distance metric: Cosine similarity\")\n",
    "        \n",
    "        # Performance metrics\n",
    "        print(f\"\\n‚ö° Performance Characteristics:\")\n",
    "        print(f\"   ‚Ä¢ Search complexity: O(log n) approximate\")\n",
    "        print(f\"   ‚Ä¢ Index build time: ~{total_objects * 0.1:.1f} seconds estimated\")\n",
    "        print(f\"   ‚Ä¢ Query latency: <50ms for most searches\")\n",
    "        print(f\"   ‚Ä¢ Memory usage: ~{total_objects * 1536 * 4 / 1024 / 1024:.1f} MB for vectors\")\n",
    "        \n",
    "        # Test search performance\n",
    "        start_time = time.time()\n",
    "        test_results = semantic_search(client, \"classical music composition\", limit=3)\n",
    "        search_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        print(f\"\\nüîç Live Performance Test:\")\n",
    "        print(f\"   ‚Ä¢ Query: 'classical music composition'\")\n",
    "        print(f\"   ‚Ä¢ Results returned: {len(test_results)}\")\n",
    "        print(f\"   ‚Ä¢ Search time: {search_time:.1f} ms\")\n",
    "        \n",
    "        # Scalability projections\n",
    "        print(f\"\\nüìà Scalability Projections:\")\n",
    "        scales = [1000, 10000, 100000, 1000000]\n",
    "        for scale in scales:\n",
    "            memory_gb = (scale * 1536 * 4) / (1024**3)\n",
    "            search_time_ms = np.log2(scale) * 2  # Rough HNSW estimate\n",
    "            print(f\"   ‚Ä¢ {scale:,} records: ~{memory_gb:.1f} GB memory, ~{search_time_ms:.1f} ms search\")\n",
    "        \n",
    "        print(f\"\\nüí° Implementation Recommendations:\")\n",
    "        print(f\"   ‚úÖ Excellent for collections up to 1M records\")\n",
    "        print(f\"   ‚úÖ Sub-second search performance\")\n",
    "        print(f\"   ‚úÖ Handles complex metadata filtering\")\n",
    "        print(f\"   ‚úÖ Supports real-time updates\")\n",
    "        print(f\"   ‚ö†Ô∏è  Consider sharding for 10M+ records\")\n",
    "        print(f\"   ‚ö†Ô∏è  Monitor memory usage in production\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating report: {e}\")\n",
    "\n",
    "def practical_applications_guide():\n",
    "    \"\"\"\n",
    "    Provide guidance on practical applications in library systems.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\nüéØ PRACTICAL APPLICATIONS GUIDE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    applications = {\n",
    "        \"üîç Enhanced Discovery\": [\n",
    "            \"Semantic search that finds relevant works even with different terminology\",\n",
    "            \"'Related items' suggestions based on semantic similarity\",\n",
    "            \"Cross-lingual discovery (embeddings capture meaning across languages)\",\n",
    "            \"Subject heading expansion and suggestion\"\n",
    "        ],\n",
    "        \"üë• Entity Resolution\": [\n",
    "            \"Identify duplicate person records across different name forms\",\n",
    "            \"Distinguish between people with identical names (like our Schubert example)\",\n",
    "            \"Merge bibliographic records for the same work\",\n",
    "            \"Authority control automation and suggestion\"\n",
    "        ],\n",
    "        \"üìä Collection Analysis\": [\n",
    "            \"Identify gaps in collection coverage\",\n",
    "            \"Find thematically related materials for collection development\",\n",
    "            \"Analyze patron interests through search patterns\",\n",
    "            \"Automated subject classification and suggestion\"\n",
    "        ],\n",
    "        \"ü§ñ Workflow Automation\": [\n",
    "            \"Automated cataloging suggestions based on similar records\",\n",
    "            \"Quality control: flag potentially incorrect metadata\",\n",
    "            \"Batch processing for metadata enhancement\",\n",
    "            \"Integration with AI systems for catalog enrichment\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in applications.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"   ‚Ä¢ {item}\")\n",
    "    \n",
    "    print(f\"\\nüí∞ Cost Considerations:\")\n",
    "    print(f\"   ‚Ä¢ OpenAI embeddings: ~$0.02 per 1M tokens\")\n",
    "    print(f\"   ‚Ä¢ For 100K records: ~$20-50 one-time embedding cost\")\n",
    "    print(f\"   ‚Ä¢ Weaviate hosting: $50-500/month depending on scale\")\n",
    "    print(f\"   ‚Ä¢ Development time: 2-4 weeks for basic implementation\")\n",
    "    print(f\"   ‚Ä¢ ROI: Significant improvement in discovery and cataloger efficiency\")\n",
    "    \n",
    "    print(f\"\\nüõ†Ô∏è Next Steps for Implementation:\")\n",
    "    print(f\"   1. Start with a pilot collection (1000-10000 records)\")\n",
    "    print(f\"   2. Define use cases and success metrics\")\n",
    "    print(f\"   3. Establish data pipeline for embedding generation\")\n",
    "    print(f\"   4. Build search interface and user testing\")\n",
    "    print(f\"   5. Scale gradually with user feedback\")\n",
    "    print(f\"   6. Integrate with existing ILS and discovery systems\")\n",
    "\n",
    "# Generate comprehensive report\n",
    "generate_implementation_report(client)\n",
    "practical_applications_guide()\n",
    "\n",
    "print(f\"\\n\\nüéâ WORKSHOP COMPLETE!\")\n",
    "print(f\"üöÄ You now have hands-on experience with:\")\n",
    "print(f\"   ‚úÖ Setting up Weaviate vector database\")\n",
    "print(f\"   ‚úÖ Creating collections optimized for library metadata\")\n",
    "print(f\"   ‚úÖ Generating and indexing embeddings\")\n",
    "print(f\"   ‚úÖ Performing semantic similarity searches\")\n",
    "print(f\"   ‚úÖ Advanced querying with filters and conditions\")\n",
    "print(f\"   ‚úÖ Entity resolution and disambiguation\")\n",
    "print(f\"   ‚úÖ Collection analysis and visualization\")\n",
    "print(f\"\\nüìö Ready to revolutionize library discovery with semantic search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Connection Management\n",
    "\n",
    "Always remember to properly close your Weaviate connection when finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the connection\n",
    "try:\n",
    "    client.close()\n",
    "    print(\"‚úÖ Weaviate connection closed successfully\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Connection was already closed or not established\")\n",
    "\n",
    "print(\"\\nüéì Workshop completed successfully!\")\n",
    "print(\"üí° Don't forget to stop your Weaviate Docker container when done:\")\n",
    "print(\"   docker stop weaviate-workshop\")\n",
    "print(\"   docker rm weaviate-workshop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
