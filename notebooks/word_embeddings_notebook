{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Text Embeddings Workshop Part 1: Word2Vec and the Birth of Semantic Vectors\n",
    "\n",
    "**Yale Graduate Student AI Workshop**  \n",
    "**Learning Objective**: Understand how text embeddings encode semantic meaning through hands-on exploration\n",
    "\n",
    "---\n",
    "\n",
    "## What Are We Learning Today?\n",
    "\n",
    "Think of this workshop as a journey through the evolution of how computers understand language. We'll start with **Word2Vec**, the breakthrough that showed us words could be represented as mathematical vectors that capture meaning. Then we'll see how this foundation led to modern text embeddings that power today's AI systems.\n",
    "\n",
    "### Why Start with Word2Vec?\n",
    "\n",
    "Word2Vec (2013) was revolutionary because it proved that **semantic relationships could be encoded as mathematical relationships**. The famous example `king - man + woman = queen` isn't just a neat trick‚Äîit demonstrates that meaning can be computed through vector arithmetic.\n",
    "\n",
    "### Real-World Context: Library Entity Resolution\n",
    "\n",
    "We'll use examples from Yale University Library's catalog, where we need to distinguish between different people with similar names‚Äîlike Franz Schubert the composer (1797-1828) versus Franz Schubert the German artist working on archaeology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up Our Environment\n",
    "\n",
    "First, let's install the tools we need and understand what each one does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch numpy matplotlib pandas scikit-learn seaborn plotly\n",
    "\n",
    "# Import our essential libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Environment ready! Let's explore how computers learn language.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading the Pre-trained Word2Vec Model\n",
    "\n",
    "We're using a pre-trained Word2Vec model from Hugging Face that was trained on massive amounts of text. This model has already learned relationships between hundreds of thousands of words.\n",
    "\n",
    "### Why Use Pre-trained Models?\n",
    "\n",
    "Training Word2Vec from scratch requires enormous datasets and computational resources. The model we're using learned from billions of words, which means it understands subtle semantic relationships we couldn't capture with smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Word2Vec model from Hugging Face\n",
    "print(\"Loading Word2Vec model from Hugging Face...\")\n",
    "print(\"This model was trained on massive text corpora to learn word relationships.\")\n",
    "\n",
    "# We're using the NeuML/word2vec model which provides pre-trained Word2Vec embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NeuML/word2vec\")\n",
    "model = AutoModel.from_pretrained(\"NeuML/word2vec\")\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded successfully!\")\n",
    "print(f\"Model has {model.embeddings.weight.shape[0]} words in vocabulary\")\n",
    "print(f\"Each word is represented as a {model.embeddings.weight.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Tokenization\n",
    "\n",
    "Before we can work with words as vectors, we need to understand **tokenization**‚Äîhow text gets broken down into pieces the model can understand.\n",
    "\n",
    "### What Is Tokenization?\n",
    "\n",
    "Tokenization splits text into \"tokens\"‚Äîusually words, but sometimes subwords or characters. Different models use different strategies. Let's see how our Word2Vec model handles various texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore tokenization with library catalog examples\n",
    "sample_texts = [\n",
    "    \"Franz Schubert composer\",\n",
    "    \"Arch√§ologie und Photographie\",  # German text from your dataset\n",
    "    \"String quartets scores\",\n",
    "    \"king queen man woman\",  # For our classic example\n",
    "    \"Franz Schubert, 1797-1828\"  # With dates\n",
    "]\n",
    "\n",
    "print(\"üìù Understanding Tokenization\\n\")\n",
    "print(\"Let's see how different texts get broken into tokens:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in sample_texts:\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    print(f\"\\nOriginal text: '{text}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: Each unique token gets a numeric ID that maps to a vector in the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Extracting Word Vectors\n",
    "\n",
    "Now let's get the actual vector representations for individual words. Each word becomes a point in high-dimensional space where semantic similarity corresponds to geometric proximity.\n",
    "\n",
    "### The Vector Space Concept\n",
    "\n",
    "Imagine each word as a point in a space with hundreds of dimensions. Words with similar meanings cluster together, while unrelated words are far apart. This geometric organization enables mathematical operations on meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word):\n",
    "    \"\"\"Extract the vector representation for a single word.\"\"\"\n",
    "    try:\n",
    "        # Tokenize the word\n",
    "        tokens = tokenizer.tokenize(word.lower())\n",
    "        if not tokens:\n",
    "            return None\n",
    "        \n",
    "        # Get the token ID\n",
    "        token_id = tokenizer.convert_tokens_to_ids(tokens[0])\n",
    "        \n",
    "        # Extract the embedding vector\n",
    "        with torch.no_grad():\n",
    "            vector = model.embeddings.weight[token_id].numpy()\n",
    "        \n",
    "        return vector\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Test our function with key words for the famous analogy\n",
    "test_words = ['king', 'queen', 'man', 'woman', 'composer', 'music']\n",
    "\n",
    "print(\"üîç Extracting Word Vectors\\n\")\n",
    "word_vectors = {}\n",
    "\n",
    "for word in test_words:\n",
    "    vector = get_word_vector(word)\n",
    "    if vector is not None:\n",
    "        word_vectors[word] = vector\n",
    "        print(f\"‚úÖ '{word}' ‚Üí {vector.shape[0]}-dimensional vector\")\n",
    "        print(f\"   First 5 dimensions: {vector[:5].round(3)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå '{word}' not found in vocabulary\")\n",
    "\n",
    "print(f\"\\nüìä Successfully extracted vectors for {len(word_vectors)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The Famous King - Man + Woman = Queen Example\n",
    "\n",
    "Now for the moment we've been building toward! This example demonstrates that semantic relationships can be captured through vector arithmetic.\n",
    "\n",
    "### The Mathematics of Meaning\n",
    "\n",
    "The equation `king - man + woman ‚âà queen` works because:\n",
    "- `king - man` removes the \"male\" component from \"king\"\n",
    "- `+ woman` adds the \"female\" component\n",
    "- The result should be close to \"queen\" in vector space\n",
    "\n",
    "Let's test this hypothesis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_analogy(word1, word2, word3, word_vectors, top_n=5):\n",
    "    \"\"\"\n",
    "    Solve word analogies: word1 is to word2 as word3 is to ?\n",
    "    Using vector arithmetic: word2 - word1 + word3 ‚âà answer\n",
    "    \"\"\"\n",
    "    if not all(word in word_vectors for word in [word1, word2, word3]):\n",
    "        missing = [w for w in [word1, word2, word3] if w not in word_vectors]\n",
    "        print(f\"‚ùå Missing words in vocabulary: {missing}\")\n",
    "        return []\n",
    "    \n",
    "    # Perform the vector arithmetic\n",
    "    result_vector = word_vectors[word2] - word_vectors[word1] + word_vectors[word3]\n",
    "    \n",
    "    # Find the closest words to the result vector\n",
    "    similarities = []\n",
    "    \n",
    "    for word, vector in word_vectors.items():\n",
    "        if word not in [word1, word2, word3]:  # Exclude input words\n",
    "            # Calculate cosine similarity\n",
    "            similarity = cosine_similarity([result_vector], [vector])[0][0]\n",
    "            similarities.append((word, similarity))\n",
    "    \n",
    "    # Sort by similarity and return top results\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "# The famous example: king - man + woman = ?\n",
    "print(\"üëë Testing the Famous Analogy: King - Man + Woman = ?\\n\")\n",
    "print(\"Vector arithmetic: king‚Éó - man‚Éó + woman‚Éó\")\n",
    "print(\"Expected result: Should be close to 'queen'\\n\")\n",
    "\n",
    "results = vector_analogy('man', 'king', 'woman', word_vectors)\n",
    "\n",
    "print(\"üéØ Results (most similar words):\")\n",
    "print(\"=\" * 40)\n",
    "for i, (word, similarity) in enumerate(results, 1):\n",
    "    print(f\"{i}. {word:<15} (similarity: {similarity:.4f})\")\n",
    "\n",
    "# Check if 'queen' is in our results\n",
    "if any(word == 'queen' for word, _ in results):\n",
    "    queen_rank = next(i for i, (word, _) in enumerate(results, 1) if word == 'queen')\n",
    "    print(f\"\\nüéâ Success! 'queen' appears at rank {queen_rank}\")\n",
    "else:\n",
    "    print(\"\\nü§î 'queen' not in top results - this shows the complexity of real-world embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualizing the Vector Space\n",
    "\n",
    "Let's create compelling visualizations to see how words cluster in the high-dimensional space. We'll use dimensionality reduction to project the vectors into 2D space we can visualize.\n",
    "\n",
    "### Understanding Dimensionality Reduction\n",
    "\n",
    "Word vectors typically have 100-300 dimensions, but we can only visualize 2-3 dimensions. **Principal Component Analysis (PCA)** finds the 2 dimensions that capture the most variation in our data, preserving as much semantic structure as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add more words to make our visualization richer\n",
    "additional_words = [\n",
    "    # Royal/authority words\n",
    "    'prince', 'princess', 'royal', 'crown',\n",
    "    # Music words (for our Franz Schubert examples)\n",
    "    'composer', 'music', 'symphony', 'piano',\n",
    "    # Gender-related words\n",
    "    'father', 'mother', 'son', 'daughter',\n",
    "    # Artist words\n",
    "    'artist', 'painting', 'photography', 'gallery'\n",
    "]\n",
    "\n",
    "print(\"üìà Building Rich Vocabulary for Visualization\\n\")\n",
    "\n",
    "# Collect all words and their vectors\n",
    "all_words = test_words + additional_words\n",
    "visualization_vectors = {}\n",
    "\n",
    "for word in all_words:\n",
    "    vector = get_word_vector(word)\n",
    "    if vector is not None:\n",
    "        visualization_vectors[word] = vector\n",
    "        print(f\"‚úÖ Added '{word}' to visualization\")\n",
    "\n",
    "print(f\"\\nüìä Total words for visualization: {len(visualization_vectors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PCA\n",
    "words = list(visualization_vectors.keys())\n",
    "vectors = np.array(list(visualization_vectors.values()))\n",
    "\n",
    "print(f\"üîÑ Applying PCA to {vectors.shape[0]} words with {vectors.shape[1]} dimensions each\")\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(vectors)\n",
    "\n",
    "# Create a DataFrame for easy plotting\n",
    "df_viz = pd.DataFrame({\n",
    "    'word': words,\n",
    "    'x': vectors_2d[:, 0],\n",
    "    'y': vectors_2d[:, 1]\n",
    "})\n",
    "\n",
    "# Add categories for color coding\n",
    "def categorize_word(word):\n",
    "    if word in ['king', 'queen', 'prince', 'princess', 'royal', 'crown']:\n",
    "        return 'Royalty'\n",
    "    elif word in ['man', 'woman', 'father', 'mother', 'son', 'daughter']:\n",
    "        return 'Family/Gender'\n",
    "    elif word in ['composer', 'music', 'symphony', 'piano']:\n",
    "        return 'Music'\n",
    "    elif word in ['artist', 'painting', 'photography', 'gallery']:\n",
    "        return 'Visual Arts'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df_viz['category'] = df_viz['word'].apply(categorize_word)\n",
    "\n",
    "print(f\"‚úÖ PCA complete! Explained variance: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "print(\"üìä This means our 2D plot captures {:.1%} of the semantic structure\".format(pca.explained_variance_ratio_.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive visualization\n",
    "fig = px.scatter(\n",
    "    df_viz, \n",
    "    x='x', \n",
    "    y='y', \n",
    "    color='category',\n",
    "    text='word',\n",
    "    title='Word2Vec Semantic Space: Words as Points in Vector Space',\n",
    "    labels={'x': 'First Principal Component', 'y': 'Second Principal Component'},\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "fig.update_traces(\n",
    "    textposition='top center',\n",
    "    marker=dict(size=10, line=dict(width=1, color='white'))\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_x=0.5,\n",
    "    font=dict(size=12),\n",
    "    showlegend=True,\n",
    "    plot_bgcolor='white',\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text=\"Notice how semantically related words cluster together!\",\n",
    "            showarrow=False,\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            x=0.5,\n",
    "            y=-0.15,\n",
    "            xanchor='center',\n",
    "            font=dict(size=14, color=\"darkblue\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüéØ Key Observations:\")\n",
    "print(\"‚Ä¢ Words with similar meanings appear closer together\")\n",
    "print(\"‚Ä¢ Different semantic categories form distinct clusters\")\n",
    "print(\"‚Ä¢ The geometric structure encodes linguistic relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Entity Resolution Application\n",
    "\n",
    "Now let's apply what we've learned to a real problem from Yale's library catalog: distinguishing between different people named \"Franz Schubert.\"\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "Yale's catalog contains records for:\n",
    "1. **Franz Schubert (1797-1828)** - Austrian composer, string quartets, symphonies\n",
    "2. **Franz Schubert** - German artist, archaeology, photography\n",
    "\n",
    "How can embeddings help us distinguish them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data based on your actual dataset\n",
    "schubert_records = {\n",
    "    'composer_schubert': {\n",
    "        'person': 'Schubert, Franz, 1797-1828',\n",
    "        'title': 'Quartette f√ºr zwei Violinen, Viola, Violoncell',\n",
    "        'subjects': 'String quartets--Scores',\n",
    "        'composite': 'Title: Quartette f√ºr zwei Violinen, Viola, Violoncell\\nSubjects: String quartets--Scores\\nProvision information: Leipzig: C.F. Peters, [19--?] Partitur',\n",
    "        'keywords': ['music', 'composer', 'symphony', 'quartet', 'classical']\n",
    "    },\n",
    "    'artist_schubert': {\n",
    "        'person': 'Schubert, Franz',\n",
    "        'title': 'Arch√§ologie und Photographie: f√ºnfzig Beispiele zur Geschichte und Methode',\n",
    "        'subjects': 'Photography in archaeology',\n",
    "        'composite': 'Title: Arch√§ologie und Photographie: f√ºnfzig Beispiele zur Geschichte und Methode\\nSubjects: Photography in archaeology\\nProvision information: Mainz: P. von Zabern, 1978',\n",
    "        'keywords': ['archaeology', 'photography', 'art', 'visual', 'documentation']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üé≠ Franz Schubert Entity Resolution Challenge\\n\")\n",
    "print(\"We have two different people with the same name:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for key, record in schubert_records.items():\n",
    "    print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
    "    print(f\"  Name: {record['person']}\")\n",
    "    print(f\"  Work: {record['title'][:50]}...\")\n",
    "    print(f\"  Subject: {record['subjects']}\")\n",
    "    print(f\"  Keywords: {', '.join(record['keywords'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate semantic profiles for each Schubert using word embeddings\n",
    "def calculate_semantic_profile(keywords, word_vectors):\n",
    "    \"\"\"Calculate the average vector for a set of keywords\"\"\"\n",
    "    valid_vectors = []\n",
    "    found_words = []\n",
    "    \n",
    "    for word in keywords:\n",
    "        if word in word_vectors:\n",
    "            valid_vectors.append(word_vectors[word])\n",
    "            found_words.append(word)\n",
    "    \n",
    "    if valid_vectors:\n",
    "        profile = np.mean(valid_vectors, axis=0)\n",
    "        return profile, found_words\n",
    "    else:\n",
    "        return None, []\n",
    "\n",
    "print(\"üßÆ Computing Semantic Profiles\\n\")\n",
    "\n",
    "profiles = {}\n",
    "for key, record in schubert_records.items():\n",
    "    profile, found_words = calculate_semantic_profile(record['keywords'], word_vectors)\n",
    "    if profile is not None:\n",
    "        profiles[key] = profile\n",
    "        print(f\"‚úÖ {key}: Used words {found_words} to create semantic profile\")\n",
    "    else:\n",
    "        print(f\"‚ùå {key}: No words found in vocabulary\")\n",
    "\n",
    "# Calculate similarity between the two profiles\n",
    "if len(profiles) == 2:\n",
    "    profile_names = list(profiles.keys())\n",
    "    similarity = cosine_similarity(\n",
    "        [profiles[profile_names[0]]], \n",
    "        [profiles[profile_names[1]]]\n",
    "    )[0][0]\n",
    "    \n",
    "    print(f\"\\nüìä Semantic Similarity between the two Schuberts: {similarity:.4f}\")\n",
    "    print(f\"(Scale: -1 = opposite meanings, 0 = unrelated, 1 = identical meanings)\")\n",
    "    \n",
    "    if similarity < 0.3:\n",
    "        print(\"üéØ Result: LOW similarity - these are likely different people!\")\n",
    "    elif similarity < 0.7:\n",
    "        print(\"ü§î Result: MODERATE similarity - needs more investigation\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Result: HIGH similarity - might be the same person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: The Evolution to Modern Embeddings\n",
    "\n",
    "Word2Vec was groundbreaking, but it had limitations. Let's understand how we got from Word2Vec to today's powerful text embeddings.\n",
    "\n",
    "### Limitations of Word2Vec\n",
    "\n",
    "1. **Static embeddings**: Each word has one vector, regardless of context\n",
    "2. **Single word focus**: Doesn't handle phrases or sentences well\n",
    "3. **Limited vocabulary**: Out-of-vocabulary words are problematic\n",
    "\n",
    "### The Path Forward\n",
    "\n",
    "- **2017**: Transformer architecture (\"Attention Is All You Need\")\n",
    "- **2018**: BERT introduces contextual embeddings\n",
    "- **2019**: Sentence-BERT enables sentence-level embeddings\n",
    "- **2024**: Modern embedding models like OpenAI's text-embedding-3-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison visualization\n",
    "evolution_data = {\n",
    "    'Model': ['Word2Vec (2013)', 'GloVe (2014)', 'FastText (2016)', 'BERT (2018)', 'Sentence-BERT (2019)', 'OpenAI text-embedding-3 (2024)'],\n",
    "    'Context_Aware': [0, 0, 0, 1, 1, 1],\n",
    "    'Sentence_Level': [0, 0, 0, 0, 1, 1],\n",
    "    'Dimensions': [300, 300, 300, 768, 768, 1536],\n",
    "    'Year': [2013, 2014, 2016, 2018, 2019, 2024]\n",
    "}\n",
    "\n",
    "df_evolution = pd.DataFrame(evolution_data)\n",
    "\n",
    "# Create subplot figure\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Model Timeline', 'Vector Dimensions', 'Context Awareness', 'Capabilities Matrix'),\n",
    "    specs=[[{'type': 'scatter'}, {'type': 'bar'}],\n",
    "           [{'type': 'bar'}, {'type': 'heatmap'}]]\n",
    ")\n",
    "\n",
    "# Timeline\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_evolution['Year'], y=df_evolution['Dimensions'], \n",
    "               mode='markers+lines', name='Dimension Growth',\n",
    "               text=df_evolution['Model'], textposition='top center'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Dimensions bar chart\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_evolution['Model'], y=df_evolution['Dimensions'], name='Vector Size'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Context awareness\n",
    "fig.add_trace(\n",
    "    go.Bar(x=df_evolution['Model'], y=df_evolution['Context_Aware'], name='Context Aware'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Capabilities heatmap\n",
    "capabilities_matrix = df_evolution[['Context_Aware', 'Sentence_Level']].T\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=capabilities_matrix.values, \n",
    "               x=df_evolution['Model'], \n",
    "               y=['Context Aware', 'Sentence Level'],\n",
    "               colorscale='Viridis'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Evolution of Text Embeddings: From Word2Vec to Modern Models\")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüìà Key Evolutionary Milestones:\")\n",
    "print(\"‚Ä¢ 2013-2016: Static word embeddings (Word2Vec, GloVe, FastText)\")\n",
    "print(\"‚Ä¢ 2018: Contextual embeddings (BERT) - same word, different vectors in different contexts\")\n",
    "print(\"‚Ä¢ 2019: Sentence embeddings (Sentence-BERT) - entire sentences as vectors\")\n",
    "print(\"‚Ä¢ 2024: Massive scale embeddings with 1536+ dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Practical Applications in Entity Resolution\n",
    "\n",
    "Let's connect what we've learned to real-world applications in library and cultural heritage contexts.\n",
    "\n",
    "### Why Entity Resolution Matters\n",
    "\n",
    "In Yale's catalog with 17+ million records, we need to:\n",
    "- Identify when \"John Smith\" in record A is the same person as \"J. Smith\" in record B\n",
    "- Distinguish between different people with identical names\n",
    "- Link authors across different works and time periods\n",
    "\n",
    "### How Embeddings Help\n",
    "\n",
    "Embeddings capture semantic context that simple string matching misses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the power of embeddings vs simple string matching\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def string_similarity(a, b):\n",
    "    \"\"\"Calculate simple string similarity\"\"\"\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "def semantic_similarity(text1, text2, word_vectors):\n",
    "    \"\"\"Calculate semantic similarity using word embeddings\"\"\"\n",
    "    # Simple approach: average word vectors\n",
    "    def text_to_vector(text):\n",
    "        words = text.lower().split()\n",
    "        vectors = [word_vectors[word] for word in words if word in word_vectors]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n",
    "    \n",
    "    vec1 = text_to_vector(text1)\n",
    "    vec2 = text_to_vector(text2)\n",
    "    \n",
    "    if np.allclose(vec1, 0) or np.allclose(vec2, 0):\n",
    "        return 0\n",
    "    \n",
    "    return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "# Test cases comparing string vs semantic similarity\n",
    "test_cases = [\n",
    "    (\"Franz Schubert composer\", \"Franz Schubert musician\"),\n",
    "    (\"Franz Schubert composer\", \"Franz Schubert photographer\"),\n",
    "    (\"string quartet\", \"chamber music\"),\n",
    "    (\"archaeology photography\", \"archaeological documentation\"),\n",
    "    (\"classical music\", \"baroque symphony\")\n",
    "]\n",
    "\n",
    "print(\"‚öîÔ∏è String Matching vs Semantic Embeddings\\n\")\n",
    "print(f\"{'Text 1':<25} {'Text 2':<25} {'String Sim':<12} {'Semantic Sim':<15} {'Winner'}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for text1, text2 in test_cases:\n",
    "    str_sim = string_similarity(text1, text2)\n",
    "    sem_sim = semantic_similarity(text1, text2, word_vectors)\n",
    "    \n",
    "    winner = \"Semantic\" if sem_sim > str_sim else \"String\" if str_sim > sem_sim else \"Tie\"\n",
    "    \n",
    "    print(f\"{text1:<25} {text2:<25} {str_sim:<12.3f} {sem_sim:<15.3f} {winner}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: Embeddings capture meaning beyond surface-level text similarity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Looking Forward - Modern Embedding Applications\n",
    "\n",
    "As we wrap up this Word2Vec exploration, let's preview where this technology leads in modern AI applications.\n",
    "\n",
    "### From Word2Vec to Production Systems\n",
    "\n",
    "What we've learned today forms the foundation for:\n",
    "- **Vector databases** (like Weaviate) for similarity search\n",
    "- **Large language models** that understand context\n",
    "- **Recommendation systems** that find related content\n",
    "- **Entity resolution pipelines** that process millions of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization of what we've learned\n",
    "concepts_learned = [\n",
    "    'Tokenization: Text ‚Üí Numbers',\n",
    "    'Vector Space: Words as Points',\n",
    "    'Semantic Arithmetic: king - man + woman = queen',\n",
    "    'Dimensionality Reduction: 300D ‚Üí 2D visualization',\n",
    "    'Entity Resolution: Distinguishing Franz Schuberts',\n",
    "    'Evolution Path: Word2Vec ‚Üí Modern Embeddings'\n",
    "]\n",
    "\n",
    "# Create a concept map\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create a circular arrangement of concepts\n",
    "angles = np.linspace(0, 2*np.pi, len(concepts_learned), endpoint=False)\n",
    "x_coords = np.cos(angles) * 3\n",
    "y_coords = np.sin(angles) * 3\n",
    "\n",
    "# Plot concept nodes\n",
    "for i, (concept, x, y) in enumerate(zip(concepts_learned, x_coords, y_coords)):\n",
    "    circle = plt.Circle((x, y), 0.8, color=plt.cm.Set3(i), alpha=0.7)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, y, concept, ha='center', va='center', fontsize=10, \n",
    "            weight='bold', wrap=True)\n",
    "\n",
    "# Add center concept\n",
    "center_circle = plt.Circle((0, 0), 1.2, color='gold', alpha=0.8)\n",
    "ax.add_patch(center_circle)\n",
    "ax.text(0, 0, 'Text\\nEmbeddings\\nFoundations', ha='center', va='center', \n",
    "        fontsize=14, weight='bold')\n",
    "\n",
    "# Draw connections\n",
    "for x, y in zip(x_coords, y_coords):\n",
    "    ax.plot([0, x*0.7], [0, y*0.7], 'k-', alpha=0.3, linewidth=2)\n",
    "\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('What We\\'ve Learned: Text Embeddings Foundations', \n",
    "            fontsize=16, weight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéì Congratulations! You've completed Part 1 of the Text Embeddings Workshop\")\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"‚Ä¢ Part 2: Vector Databases and Similarity Search with Weaviate\")\n",
    "print(\"‚Ä¢ Part 3: Modern Text Embeddings and Classification\")\n",
    "print(\"‚Ä¢ Part 4: Production Entity Resolution Pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Foundation We've Built\n",
    "\n",
    "Today we've explored the foundational concepts that power modern AI text understanding:\n",
    "\n",
    "### Core Concepts Mastered\n",
    "\n",
    "1. **Tokenization**: How text becomes numbers computers can process\n",
    "2. **Vector Representations**: Words as points in high-dimensional space\n",
    "3. **Semantic Arithmetic**: Mathematical operations on meaning\n",
    "4. **Similarity Computation**: Measuring relatedness through cosine similarity\n",
    "5. **Entity Resolution**: Practical applications in library catalogs\n",
    "\n",
    "### The Bigger Picture\n",
    "\n",
    "Word2Vec showed us that **meaning can be computed**. This insight revolutionized natural language processing and laid the groundwork for:\n",
    "- Modern language models like GPT and BERT\n",
    "- Vector databases for semantic search\n",
    "- Recommendation systems\n",
    "- Entity resolution at scale\n",
    "\n",
    "### Ready for Next Steps\n",
    "\n",
    "With this foundation, you're prepared to understand how these concepts scale to production systems processing millions of documents, using sophisticated vector databases, and powering real-world applications.\n",
    "\n",
    "**The journey from Word2Vec to modern AI starts with understanding that language has geometry‚Äîand you now understand that geometry.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}