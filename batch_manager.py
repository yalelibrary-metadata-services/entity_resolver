#!/usr/bin/env python3
"""
Batch Manager for Entity Resolution

This script provides manual management of OpenAI batch jobs for embedding generation.
Use this when you want to create batch jobs and manually check their status rather
than having the script continuously poll for completion.
"""

import os
import sys
import logging
import argparse
import yaml
import pickle
import fcntl
import time
from typing import Dict, Any

# Add src directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.embedding_and_indexing_batch import BatchEmbeddingPipeline

# Import transition modules (with fallback for missing modules)
try:
    from src.batch_state_consolidator import consolidate_batch_state
    from src.transition_controller import TransitionController, transition_batch_to_realtime
    TRANSITION_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Transition modules not available: {e}")
    TRANSITION_AVAILABLE = False

class ProcessLock:
    """Simple file-based process lock to prevent multiple instances."""
    
    def __init__(self, lock_file: str):
        self.lock_file = lock_file
        self.lock_fd = None
    
    def __enter__(self):
        """Acquire the lock."""
        try:
            # Create lock directory if it doesn't exist
            os.makedirs(os.path.dirname(self.lock_file), exist_ok=True)
            
            # Open lock file
            self.lock_fd = open(self.lock_file, 'w')
            
            # Try to acquire exclusive lock (non-blocking)
            fcntl.flock(self.lock_fd.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
            
            # Write our process ID to the lock file
            self.lock_fd.write(f"{os.getpid()}\n{time.time()}\n")
            self.lock_fd.flush()
            os.fsync(self.lock_fd.fileno())  # Force write to disk
            
            return self
            
        except (IOError, OSError) as e:
            # Lock is already held by another process
            if self.lock_fd:
                self.lock_fd.close()
                self.lock_fd = None
            
            # Check if this is a "resource unavailable" error (lock already held)
            if e.errno == 35 or e.errno == 11 or "Resource temporarily unavailable" in str(e):
                # Try to read the existing lock file to show which process holds it
                try:
                    with open(self.lock_file, 'r') as f:
                        content = f.read().strip()
                        lines = content.split('\n') if content else []
                        if len(lines) >= 2 and lines[0] and lines[1]:
                            try:
                                pid = lines[0]
                                timestamp = float(lines[1])
                                lock_age = time.time() - timestamp
                                print(f"âŒ Another batch creation process is already running!")
                                print(f"   Process ID: {pid}")
                                print(f"   Lock age: {lock_age:.1f} seconds")
                                print(f"   Lock file: {self.lock_file}")
                                print(f"\nğŸ’¡ Options:")
                                print(f"   1. Wait for the other process to complete")
                                print(f"   2. Kill the other process: kill {pid}")
                                print(f"   3. Remove stale lock (if process is dead): rm {self.lock_file}")
                            except (ValueError, IndexError):
                                print(f"âŒ Another batch creation process is running (lock file: {self.lock_file})")
                        else:
                            print(f"âŒ Another batch creation process is running (lock file: {self.lock_file})")
                except:
                    print(f"âŒ Another batch creation process is running (lock file: {self.lock_file})")
            else:
                # Some other I/O error
                print(f"âŒ Error acquiring lock: {e}")
            
            raise SystemExit(1)
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Release the lock."""
        if self.lock_fd:
            try:
                fcntl.flock(self.lock_fd.fileno(), fcntl.LOCK_UN)
                self.lock_fd.close()
                # Remove lock file
                try:
                    os.unlink(self.lock_file)
                except:
                    pass
            except:
                pass
            finally:
                self.lock_fd = None

def load_config(config_path: str) -> Dict[str, Any]:
    """Load configuration from file."""
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        return config
    except Exception as e:
        print(f"Error loading configuration: {str(e)}")
        sys.exit(1)

def load_preprocessing_data(checkpoint_dir: str) -> tuple:
    """Load preprocessing data from checkpoints."""
    try:
        # Load string dictionary
        with open(os.path.join(checkpoint_dir, "string_dict.pkl"), 'rb') as f:
            string_dict = pickle.load(f)
        
        # Load field hash mapping
        with open(os.path.join(checkpoint_dir, "field_hash_mapping.pkl"), 'rb') as f:
            field_hash_mapping = pickle.load(f)
        
        # Load string counts
        with open(os.path.join(checkpoint_dir, "string_counts.pkl"), 'rb') as f:
            string_counts = pickle.load(f)
        
        return string_dict, field_hash_mapping, string_counts
        
    except FileNotFoundError as e:
        print(f"Preprocessing data not found: {str(e)}")
        print("Run preprocessing stage first: python main.py --config config.yml --start preprocessing --end preprocessing")
        sys.exit(1)
    except Exception as e:
        print(f"Error loading preprocessing data: {str(e)}")
        sys.exit(1)

def create_jobs(config: Dict[str, Any]) -> None:
    """Create batch jobs without waiting for completion."""
    print("ğŸš€ Creating OpenAI batch jobs...")
    
    checkpoint_dir = config.get("checkpoint_dir", "data/checkpoints")
    lock_file = os.path.join(checkpoint_dir, ".batch_creation.lock")
    
    # Use process lock to prevent multiple creation processes
    with ProcessLock(lock_file):
        print("ğŸ”’ Acquired process lock - proceeding with batch creation")
        
        # Load preprocessing data
        string_dict, field_hash_mapping, string_counts = load_preprocessing_data(checkpoint_dir)
        
        print(f"ğŸ“Š Loaded preprocessing data: {len(string_dict)} strings")
        
        # Initialize batch pipeline with proper context management
        try:
            with BatchEmbeddingPipeline(config) as pipeline:
                # Check if automated queue is enabled
                use_automated_queue = config.get("use_automated_queue", False)
                batch_manual_polling = config.get("batch_manual_polling", True)
                
                # Use automated queue if enabled and manual polling is also enabled
                if use_automated_queue and batch_manual_polling:
                    print(f"ğŸ¤– Using automated queue management (16-batch queue with 30-min polling)")
                    result = pipeline.create_batch_jobs_with_automated_queue(
                        string_dict, field_hash_mapping, string_counts, checkpoint_dir
                    )
                else:
                    print(f"ğŸ“‹ Using manual batch creation mode")
                    result = pipeline.create_batch_jobs_only(
                        string_dict, field_hash_mapping, string_counts, checkpoint_dir
                    )
                
                if result['status'] == 'jobs_created':
                    print(f"\nâœ… Successfully created {result['jobs_created']} batch jobs!")
                    print(f"ğŸ“‹ Total requests: {result['total_requests']:,}")
                    print(f"ğŸ’° Estimated cost savings: ${result['estimated_cost_savings']:.4f}")
                    print(f"â±ï¸  Creation time: {result['elapsed_time']:.2f} seconds")
                    print(f"\nğŸ“‹ Next steps:")
                    print(f"   1. Wait for OpenAI to process your jobs (up to 24 hours)")
                    print(f"   2. Check status: python batch_manager.py --status")
                    print(f"   3. Download results: python batch_manager.py --download")
                elif result['status'] == 'no_work':
                    print("â„¹ï¸  No new work to process - all eligible strings already processed")
                else:
                    print(f"âŒ Unexpected result: {result}")
                    
        except Exception as e:
            print(f"âŒ Error creating batch jobs: {str(e)}")
            sys.exit(1)
        
        print("ğŸ”“ Released process lock")

def check_status(config: Dict[str, Any]) -> None:
    """Check status of batch jobs."""
    print("ğŸ“Š Checking batch job status...")
    
    checkpoint_dir = config.get("checkpoint_dir", "data/checkpoints")
    
    # Debug: List existing files in checkpoint directory
    print(f"ğŸ” Debug: Checking checkpoint directory: {checkpoint_dir}")
    if os.path.exists(checkpoint_dir):
        import glob
        jsonl_files = glob.glob(os.path.join(checkpoint_dir, "*.jsonl"))
        if jsonl_files:
            print(f"   Found {len(jsonl_files)} JSONL files:")
            for file in jsonl_files:
                print(f"     - {os.path.basename(file)}")
        else:
            print("   No JSONL files found in checkpoint directory")
    else:
        print(f"   Checkpoint directory does not exist: {checkpoint_dir}")
    
    # Initialize batch pipeline
    pipeline = None
    try:
        pipeline = BatchEmbeddingPipeline(config)
        
        # Check status
        result = pipeline.check_batch_status(checkpoint_dir)
        
        if result['status'] == 'no_jobs':
            print("â„¹ï¸  No batch jobs found.")
            print("   Create jobs first: python batch_manager.py --create")
        elif result['status'] == 'checked':
            # Status already logged by the method - now show additional details
            
            # Show granular status breakdown if available
            if 'status_counts' in result:
                status_counts = result['status_counts']
                print(f"\nğŸ” GRANULAR STATUS BREAKDOWN:")
                
                # Define status display order and emojis
                status_display = [
                    ('pending', "â³ Pending"),
                    ('validating', "ğŸ” Validating"), 
                    ('in_progress', "ğŸ”„ In Progress"),
                    ('finalizing', "ğŸ Finalizing"),
                    ('cancelling', "ğŸ›‘ Cancelling"),
                    ('completed', "âœ… Completed"),
                    ('failed', "âŒ Failed"),
                    ('expired', "â° Expired"),
                    ('cancelled', "ğŸš« Cancelled"),
                    ('error', "âš ï¸  Error")
                ]
                
                # Show each status if count > 0
                for status_key, label in status_display:
                    count = status_counts.get(status_key, 0)
                    if count > 0:
                        print(f"   {label}: {count}")
            
            if result['ready_for_download']:
                print(f"\nğŸ‰ {result['completed']} jobs are ready for download!")
                print(f"   Run: python batch_manager.py --download")
                
                # Show detailed download/processing status for completed jobs
                job_statuses = result.get('job_statuses', {})
                completed_jobs = [job for job in job_statuses.values() if job['status'] == 'completed']
                
                if completed_jobs:
                    downloaded_count = sum(1 for job in completed_jobs if job.get('download_status') == 'downloaded')
                    processed_count = sum(1 for job in completed_jobs if job.get('processing_status') == 'processed')
                    
                    print(f"\nğŸ“Š Download/Processing Status:")
                    print(f"   âœ… Completed: {len(completed_jobs)} jobs")
                    print(f"   ğŸ“¥ Downloaded: {downloaded_count} jobs")
                    print(f"   ğŸ—„ï¸  Processed: {processed_count} jobs")
                    
                    if downloaded_count < len(completed_jobs):
                        print(f"   ğŸ’¡ {len(completed_jobs) - downloaded_count} jobs ready for download")
                    elif processed_count < downloaded_count:
                        print(f"   ğŸ’¡ {downloaded_count - processed_count} jobs ready for processing")
                    else:
                        print(f"   âœ¨ All completed jobs have been processed!")
                        
            elif result['pending'] > 0 or result['in_progress'] > 0:
                print(f"\nâ³ Jobs are still processing. Check again later.")
            else:
                print(f"\nâš ï¸  All jobs have completed or failed. Check logs for details.")
        else:
            print(f"âŒ Unexpected result: {result}")
            
    except Exception as e:
        print(f"âŒ Error checking batch status: {str(e)}")
        sys.exit(1)
    finally:
        if pipeline and hasattr(pipeline, 'weaviate_client'):
            try:
                pipeline.weaviate_client.close()
            except:
                pass

def download_results(config: Dict[str, Any]) -> None:
    """Download and process completed batch results."""
    print("ğŸ“¥ Processing completed batch jobs...")
    
    checkpoint_dir = config.get("checkpoint_dir", "data/checkpoints")
    
    # Initialize batch pipeline
    pipeline = None
    try:
        pipeline = BatchEmbeddingPipeline(config)
        
        # Process completed jobs
        result = pipeline.process_completed_jobs(checkpoint_dir)
        
        if result['status'] == 'no_jobs':
            print("â„¹ï¸  No batch jobs found.")
            print("   Create jobs first: python batch_manager.py --create")
        elif result['status'] == 'no_completed_jobs':
            print("â„¹ï¸  No completed jobs found.")
            print("   Check status first: python batch_manager.py --status")
        elif result['status'] == 'completed':
            # Results already logged by the method
            print(f"\nğŸ‰ Batch processing completed successfully!")
            if result['collection_count']:
                print(f"   Weaviate collection now contains {result['collection_count']:,} objects")
        else:
            print(f"âŒ Unexpected result: {result}")
            
    except Exception as e:
        print(f"âŒ Error processing batch results: {str(e)}")
        sys.exit(1)
    finally:
        if pipeline and hasattr(pipeline, 'weaviate_client'):
            try:
                pipeline.weaviate_client.close()
            except:
                pass

def cancel_uncompleted_jobs(config: Dict[str, Any]) -> None:
    """Cancel all uncompleted batch jobs to free up token quota."""
    print("ğŸ›‘ Canceling uncompleted batch jobs...")
    
    checkpoint_dir = config.get("checkpoint_dir", "data/checkpoints")
    
    # Initialize batch pipeline with proper context management
    try:
        with BatchEmbeddingPipeline(config) as pipeline:
            # Always discover jobs from OpenAI API (don't rely on checkpoints)
            print("ğŸ“¡ Discovering all batch jobs from OpenAI API...")
            
            try:
                # Discover all entity resolution batch jobs from OpenAI
                all_jobs = {}
                after = None
                total_discovered = 0
                
                # Paginate through ALL batch jobs to find ours
                while True:
                    if after:
                        batches = pipeline.openai_client.batches.list(limit=100, after=after)
                    else:
                        batches = pipeline.openai_client.batches.list(limit=100)
                    
                    total_discovered += len(batches.data)
                    
                    # Process this page of results
                    for batch in batches.data:
                        # Look for our batch jobs (filter by metadata)
                        metadata = getattr(batch, 'metadata', {})
                        if (metadata and 
                            metadata.get('created_by') == 'embedding_and_indexing_batch'):
                            all_jobs[batch.id] = batch
                    
                    # Check if there are more results
                    if not batches.has_more:
                        break
                        
                    # Get the last batch ID for pagination
                    if batches.data:
                        after = batches.data[-1].id
                    else:
                        break
                
                print(f"ğŸ“Š Scanned {total_discovered} total batches, found {len(all_jobs)} entity resolution jobs")
                
                if not all_jobs:
                    print("â„¹ï¸  No entity resolution batch jobs found in OpenAI API")
                    return
                
            except Exception as e:
                print(f"âŒ Error discovering jobs from OpenAI API: {e}")
                return
            
            # Find jobs that can be cancelled
            print(f"ğŸ“¡ Checking status of {len(all_jobs)} discovered batch jobs...")
            
            cancellable_statuses = ['pending', 'validating', 'in_progress', 'finalizing']
            # Note: 'cancelling' status means already being cancelled, so not in cancellable list
            jobs_to_cancel = []
            completed_jobs = []
            already_failed = []
            cancelled_jobs = []
            cancelling_jobs = []
            
            for job_id, batch_status in all_jobs.items():
                try:
                    current_status = batch_status.status
                    
                    if current_status in cancellable_statuses:
                        jobs_to_cancel.append((job_id, current_status))
                    elif current_status == 'completed':
                        completed_jobs.append(job_id)
                    elif current_status == 'failed':
                        already_failed.append(job_id)
                    elif current_status == 'cancelled':
                        cancelled_jobs.append(job_id)
                    elif current_status == 'cancelling':
                        cancelling_jobs.append(job_id)
                        
                except Exception as e:
                    print(f"âš ï¸  Could not check job {job_id}: {e}")
        
        # Show summary
        print(f"\nğŸ“Š Job Status Summary:")
        print(f"   Can be cancelled: {len(jobs_to_cancel)}")
        print(f"   Already completed: {len(completed_jobs)}")
        print(f"   Already failed: {len(already_failed)}")
        print(f"   Already cancelled: {len(cancelled_jobs)}")
        print(f"   Currently cancelling: {len(cancelling_jobs)}")
        
        if already_failed:
            print(f"\nğŸ’¡ Note: {len(already_failed)} failed jobs cannot be cancelled (they're already failed)")
            print(f"   Failed jobs do NOT consume quota once they've failed")
            print(f"   They will eventually be cleaned up by OpenAI automatically")
        
        if not jobs_to_cancel:
            print("â„¹ï¸  No jobs found that can be cancelled")
            if already_failed:
                print(f"   The {len(already_failed)} failed jobs shown above cannot be cancelled")
            return
        
        # Confirm cancellation
        print(f"\nâš ï¸  About to cancel {len(jobs_to_cancel)} jobs:")
        for job_id, status in jobs_to_cancel[:5]:  # Show first 5
            print(f"   {job_id[:12]}... ({status})")
        if len(jobs_to_cancel) > 5:
            print(f"   ... and {len(jobs_to_cancel) - 5} more")
        
        print(f"\nğŸš¨ WARNING: This will:")
        print(f"   â€¢ Cancel {len(jobs_to_cancel)} batch jobs")
        print(f"   â€¢ Free up token quota immediately")
        print(f"   â€¢ Jobs cannot be resumed once cancelled")
        
        # Get user confirmation
        try:
            response = input(f"\nDo you want to proceed? (yes/no): ").strip().lower()
            if response not in ['yes', 'y']:
                print("âŒ Cancelled by user")
                return
        except KeyboardInterrupt:
            print("\nâŒ Cancelled by user")
            return
        
        # Cancel jobs
        print(f"\nğŸ›‘ Cancelling {len(jobs_to_cancel)} jobs...")
        cancelled_count = 0
        failed_to_cancel = 0
        
        for job_id, status in jobs_to_cancel:
            try:
                # Cancel the job
                cancelled_batch = pipeline.openai_client.batches.cancel(job_id)
                
                # Update local tracking if job exists in our tracking
                if job_id in pipeline.batch_jobs:
                    pipeline.batch_jobs[job_id]['status'] = 'cancelled'
                
                cancelled_count += 1
                print(f"   âœ… Cancelled {job_id[:12]}... ({status})")
                
            except Exception as e:
                failed_to_cancel += 1
                print(f"   âŒ Failed to cancel {job_id[:12]}...: {e}")
        
        # Update checkpoint if we have local tracking
        try:
            pipeline.load_checkpoint(checkpoint_dir)
            # Update status for cancelled jobs in local tracking
            for job_id, status in jobs_to_cancel[:cancelled_count]:
                if job_id in pipeline.batch_jobs:
                    pipeline.batch_jobs[job_id]['status'] = 'cancelled'
            pipeline.save_checkpoint(checkpoint_dir)
        except Exception as e:
            print(f"â„¹ï¸  Could not update checkpoint (this is ok): {e}")
        
        print(f"\nğŸ“Š Cancellation Summary:")
        print(f"   Successfully cancelled: {cancelled_count}")
        print(f"   Failed to cancel: {failed_to_cancel}")
        print(f"   Token quota freed up immediately")
        
        if cancelled_count > 0:
            print(f"\nğŸ“‹ Next steps:")
            print(f"   1. Check status: python batch_manager.py --status")
            print(f"   2. Resubmit failed jobs: python batch_manager.py --resubmit")
            print(f"   3. Or create new jobs: python batch_manager.py --create")
        
    except Exception as e:
        print(f"âŒ Error cancelling jobs: {str(e)}")
        sys.exit(1)

def resubmit_failed_jobs(config: Dict[str, Any]) -> None:
    """Resubmit failed jobs that failed due to token limit."""
    print("ğŸ”„ Resubmitting failed batch jobs...")
    
    checkpoint_dir = config.get("checkpoint_dir", "data/checkpoints")
    
    # Initialize batch pipeline
    pipeline = None
    try:
        pipeline = BatchEmbeddingPipeline(config)
        
        # Load existing jobs
        pipeline.load_checkpoint(checkpoint_dir)
        if not pipeline.batch_jobs:
            pipeline._recover_batch_jobs_from_api(checkpoint_dir)
        
        if not pipeline.batch_jobs:
            print("â„¹ï¸  No batch jobs found")
            return
        
        # Check which jobs failed due to token limit
        print(f"ğŸ“¡ Checking status of batch jobs...")
        token_limit_failures = []
        
        for job_id, job_info in pipeline.batch_jobs.items():
            try:
                batch_status = pipeline.openai_client.batches.retrieve(job_id)
                if (batch_status.status == 'failed' and 
                    hasattr(batch_status, 'errors') and 
                    batch_status.errors):
                    
                    # Check if it failed due to token limit
                    for error in batch_status.errors.data:
                        if error.code == 'token_limit_exceeded':
                            token_limit_failures.append(job_id)
                            break
                            
            except Exception as e:
                print(f"âš ï¸  Could not check job {job_id}: {e}")
        
        if not token_limit_failures:
            print("â„¹ï¸  No jobs failed due to token limit")
            return
        
        print(f"ğŸ“Š Found {len(token_limit_failures)} jobs that failed due to token limit")
        
        # Check current quota usage
        active_jobs = 0
        for job_id, job_info in pipeline.batch_jobs.items():
            try:
                batch_status = pipeline.openai_client.batches.retrieve(job_id)
                if batch_status.status in ['pending', 'validating', 'in_progress', 'finalizing']:
                    active_jobs += 1
            except:
                pass
        
        print(f"ğŸ“Š Currently {active_jobs} jobs active (consuming token quota)")
        
        if active_jobs > 10:  # Arbitrary threshold
            print("â³ Too many active jobs. Wait for some to complete before resubmitting.")
            print("   Run this command again in 30-60 minutes.")
            return
        
        # Resubmit a small batch of failed jobs
        resubmit_count = min(5, len(token_limit_failures))  # Start with just 5
        jobs_to_resubmit = token_limit_failures[:resubmit_count]
        
        print(f"ğŸš€ Attempting to resubmit {resubmit_count} failed jobs...")
        
        successful_resubmits = 0
        for job_id in jobs_to_resubmit:
            try:
                # Get the original batch to resubmit
                original_batch = pipeline.openai_client.batches.retrieve(job_id)
                
                # Create new batch with same input file
                new_batch = pipeline.openai_client.batches.create(
                    input_file_id=original_batch.input_file_id,
                    endpoint="/v1/embeddings",
                    completion_window="24h",
                    metadata={
                        "description": f"Resubmitted batch (original: {job_id})",
                        "created_by": "embedding_and_indexing_batch",
                        "resubmitted_from": job_id
                    }
                )
                
                # Track the new job
                pipeline.batch_jobs[new_batch.id] = {
                    'batch_idx': len(pipeline.batch_jobs),
                    'input_file_id': new_batch.input_file_id,
                    'status': 'submitted',
                    'created_at': new_batch.created_at,
                    'resubmitted_from': job_id,
                    'recovered': False
                }
                
                successful_resubmits += 1
                print(f"   âœ… Resubmitted {job_id} as {new_batch.id}")
                
            except Exception as e:
                if "token_limit_exceeded" in str(e):
                    print(f"   â³ Still at token limit. Stop here and try again later.")
                    break
                else:
                    print(f"   âŒ Failed to resubmit {job_id}: {e}")
        
        # Save updated jobs
        pipeline.save_checkpoint(checkpoint_dir)
        
        print(f"\nğŸ“Š Resubmission Summary:")
        print(f"   Successfully resubmitted: {successful_resubmits}")
        print(f"   Remaining failed jobs: {len(token_limit_failures) - successful_resubmits}")
        
        if successful_resubmits > 0:
            print(f"\nğŸ“‹ Next steps:")
            print(f"   1. Wait 30-60 minutes for jobs to process")
            print(f"   2. Check status: python batch_manager.py --status")
            print(f"   3. Resubmit more: python batch_manager.py --resubmit")
        
    except Exception as e:
        print(f"âŒ Error resubmitting jobs: {str(e)}")
        sys.exit(1)
    finally:
        if pipeline and hasattr(pipeline, 'weaviate_client'):
            try:
                pipeline.weaviate_client.close()
            except:
                pass

def investigate_failures(config: Dict[str, Any]) -> None:
    """Investigate failed batch jobs to understand why they failed."""
    print("ğŸ” Investigating failed batch jobs...")
    
    checkpoint_dir = config.get("checkpoint_dir", "data/checkpoints")
    
    # Initialize batch pipeline
    pipeline = None
    try:
        pipeline = BatchEmbeddingPipeline(config)
        
        # Load or recover batch jobs
        pipeline.load_checkpoint(checkpoint_dir)
        if not pipeline.batch_jobs:
            pipeline._recover_batch_jobs_from_api(checkpoint_dir)
        
        if not pipeline.batch_jobs:
            print("â„¹ï¸  No batch jobs found to investigate")
            return
        
        # Check a sample of jobs to find failed ones (checking all 439 would take too long)
        sample_job_ids = list(pipeline.batch_jobs.keys())[:50]  # Check first 50 jobs
        print(f"ğŸ“¡ Checking current status of {len(sample_job_ids)} sample jobs with OpenAI...")
        failed_jobs = []
        
        for job_id in sample_job_ids:
            try:
                batch_status = pipeline.openai_client.batches.retrieve(job_id)
                if batch_status.status == 'failed':
                    failed_jobs.append((job_id, batch_status))
            except Exception as e:
                print(f"âš ï¸  Could not check status of job {job_id}: {e}")
        
        if not failed_jobs:
            print("â„¹ï¸  No failed jobs found")
            return
        
        print(f"ğŸ” Investigating {len(failed_jobs)} failed jobs...")
        
        # Sample a few failed jobs to investigate
        sample_size = min(5, len(failed_jobs))
        sample_jobs = failed_jobs[:sample_size]
        
        for i, (job_id, batch_status) in enumerate(sample_jobs):
            print(f"\n--- Failed Job {i+1}/{sample_size}: {job_id} ---")
            
            try:
                
                print(f"Status: {batch_status.status}")
                print(f"Created: {batch_status.created_at}")
                
                if hasattr(batch_status, 'request_counts') and batch_status.request_counts:
                    counts = batch_status.request_counts
                    print(f"Requests - Total: {getattr(counts, 'total', 0)}, "
                          f"Completed: {getattr(counts, 'completed', 0)}, "
                          f"Failed: {getattr(counts, 'failed', 0)}")
                
                if hasattr(batch_status, 'errors') and batch_status.errors:
                    print(f"Errors: {batch_status.errors}")
                
                # Try to get error file if available
                if hasattr(batch_status, 'error_file_id') and batch_status.error_file_id:
                    print(f"Error file ID: {batch_status.error_file_id}")
                    try:
                        error_content = pipeline.openai_client.files.content(batch_status.error_file_id)
                        error_lines = error_content.content.decode('utf-8').split('\n')[:5]  # First 5 error lines
                        print("Sample errors:")
                        for line in error_lines:
                            if line.strip():
                                print(f"  {line}")
                    except Exception as e:
                        print(f"  Could not retrieve error details: {e}")
                
            except Exception as e:
                print(f"Error investigating job {job_id}: {e}")
        
        print(f"\nğŸ“Š Summary:")
        print(f"   Total jobs: {len(pipeline.batch_jobs)}")
        print(f"   Failed jobs: {len(failed_jobs)}")
        print(f"   Failure rate: {len(failed_jobs)/len(pipeline.batch_jobs)*100:.1f}%")
        
    except Exception as e:
        print(f"âŒ Error investigating failures: {str(e)}")
        sys.exit(1)
    finally:
        if pipeline and hasattr(pipeline, 'weaviate_client'):
            try:
                pipeline.weaviate_client.close()
            except:
                pass

def recover_all_jobs(config: Dict[str, Any]) -> None:
    """Recover all batch jobs from OpenAI API."""
    print("ğŸ”„ Recovering all batch jobs from OpenAI API...")
    
    checkpoint_dir = config.get("checkpoint_dir", "data/checkpoints")
    
    # Initialize batch pipeline
    pipeline = None
    try:
        pipeline = BatchEmbeddingPipeline(config)
        
        # Force recovery from API (clear local batch jobs first)
        pipeline.batch_jobs = {}
        pipeline._recover_batch_jobs_from_api(checkpoint_dir)
        
        # Save the recovered jobs
        pipeline.save_checkpoint(checkpoint_dir)
        
        if pipeline.batch_jobs:
            print(f"\nğŸ‰ Successfully recovered {len(pipeline.batch_jobs)} batch jobs!")
            
            # Group by status for summary
            status_counts = {}
            for job_info in pipeline.batch_jobs.values():
                status = job_info['status']
                status_counts[status] = status_counts.get(status, 0) + 1
            
            print(f"\nğŸ“Š Recovered job status summary:")
            for status, count in status_counts.items():
                print(f"   {status}: {count} jobs")
                
            print(f"\nğŸ“‹ Next steps:")
            print(f"   1. Check status: python batch_manager.py --status")
            print(f"   2. Download completed results: python batch_manager.py --download")
            print(f"   3. Create new jobs if needed: python batch_manager.py --create")
        else:
            print("â„¹ï¸  No entity resolution batch jobs found in OpenAI API")
            
    except Exception as e:
        print(f"âŒ Error recovering batch jobs: {str(e)}")
        sys.exit(1)
    finally:
        if pipeline and hasattr(pipeline, 'weaviate_client'):
            try:
                pipeline.weaviate_client.close()
            except:
                pass

def reset_embedding_stage(config: Dict[str, Any]) -> None:
    """Reset all embedding_and_indexing stage data."""
    print("ğŸ—‘ï¸  Resetting embedding_and_indexing stage...")
    _reset_embedding_stage_internal(config, preserve_tracking=False)

def reset_embedding_stage_selective(config: Dict[str, Any]) -> None:
    """Reset embedding stage but preserve processed hash tracking to avoid reprocessing."""
    print("ğŸ—‘ï¸  Resetting embedding_and_indexing stage (preserving tracking data)...")
    _reset_embedding_stage_internal(config, preserve_tracking=True)

def _save_processed_files_blacklist(config: Dict[str, Any], checkpoint_dir: str) -> int:
    """
    Save a blacklist of all batch job IDs that have been processed to avoid reprocessing them.
    This is used during reset to ignore previous attempts with flawed source data.
    """
    try:
        from src.embedding_and_indexing_batch import BatchEmbeddingPipeline
        pipeline = BatchEmbeddingPipeline(config)
        
        blacklisted_jobs = set()
        after = None
        total_jobs_scanned = 0
        
        # Scan ALL batch jobs (any status) to build blacklist
        while True:
            if after:
                batches = pipeline.openai_client.batches.list(limit=100, after=after)
            else:
                batches = pipeline.openai_client.batches.list(limit=100)
            
            total_jobs_scanned += len(batches.data)
            
            for batch in batches.data:
                # Check if this is an embedding batch job from our pipeline
                metadata = getattr(batch, 'metadata', {})
                endpoint = getattr(batch, 'endpoint', '')
                
                if (('/embeddings' in endpoint or endpoint == '/v1/embeddings') and
                    metadata and metadata.get('created_by') == 'embedding_and_indexing_batch'):
                    
                    # Add batch job ID to blacklist regardless of status
                    blacklisted_jobs.add(batch.id)
            
            if not batches.has_more:
                break
                
            if batches.data:
                after = batches.data[-1].id
            else:
                break
        
        pipeline.close()
        
        # Save blacklist to checkpoint
        if blacklisted_jobs:
            blacklist_path = os.path.join(checkpoint_dir, 'batch_blacklisted_files.pkl')
            import pickle
            with open(blacklist_path, 'wb') as f:
                pickle.dump(list(blacklisted_jobs), f)
        
        print(f"   ğŸ“Š Scanned {total_jobs_scanned} total batch jobs from OpenAI")
        print(f"   ğŸš« Blacklisted {len(blacklisted_jobs)} batch job IDs to avoid reprocessing")
        
        return len(blacklisted_jobs)
        
    except Exception as e:
        print(f"   âŒ Error creating blacklist: {e}")
        return 0

def _reset_embedding_stage_internal(config: Dict[str, Any], preserve_tracking: bool = False) -> None:
    """Internal function to reset embedding stage with optional tracking preservation."""
    
    checkpoint_dir = config.get("checkpoint_dir", "data/checkpoints")
    
    files_deleted = []
    files_preserved = []
    weaviate_collection_deleted = False
    blacklisted_count = 0
    
    try:
        # FIRST: Create blacklist of all processed files to avoid reprocessing
        print("ğŸš« Creating blacklist of processed files to avoid reprocessing...")
        blacklisted_count = _save_processed_files_blacklist(config, checkpoint_dir)
        
        if blacklisted_count > 0:
            print(f"   âœ… Created blacklist with {blacklisted_count} files")
            print(f"   ğŸ“‹ Future processing will skip these files completely")
        else:
            print(f"   â„¹ï¸  No files found to blacklist")
        
        # Second, try to delete the Weaviate collection
        print("ğŸ—„ï¸  Dropping EntityString collection from Weaviate...")
        pipeline = None
        try:
            pipeline = BatchEmbeddingPipeline(config)
            
            # Check if collection exists and delete it
            try:
                pipeline.weaviate_client.collections.delete("EntityString")
                weaviate_collection_deleted = True
                print("   âœ… Deleted EntityString collection from Weaviate")
            except Exception as collection_error:
                if "not found" in str(collection_error).lower() or "does not exist" in str(collection_error).lower():
                    print("   â„¹ï¸  EntityString collection does not exist in Weaviate")
                else:
                    print(f"   âš ï¸  Could not delete EntityString collection: {collection_error}")
                    
        except Exception as weaviate_error:
            print(f"   âš ï¸  Could not connect to Weaviate: {weaviate_error}")
        finally:
            if pipeline and hasattr(pipeline, 'weaviate_client'):
                try:
                    pipeline.weaviate_client.close()
                except:
                    pass
        
        # Define files to potentially delete
        real_time_files = [
            os.path.join(checkpoint_dir, 'processed_hashes.pkl')
        ]
        
        batch_processing_files = [
            os.path.join(checkpoint_dir, 'batch_processed_hashes.pkl'),
            os.path.join(checkpoint_dir, 'batch_jobs.pkl'),
            os.path.join(checkpoint_dir, 'batch_queue_state.pkl')
        ]
        
        # Delete batch request/result files
        import glob
        batch_request_pattern = os.path.join(checkpoint_dir, 'batch_requests_*.jsonl')
        batch_result_pattern = os.path.join(checkpoint_dir, 'batch_results_*.jsonl')
        batch_files = glob.glob(batch_request_pattern) + glob.glob(batch_result_pattern)
        
        # Determine what to delete based on preserve_tracking flag
        if preserve_tracking:
            print("ğŸ“ Deleting job and file data (preserving hash tracking)...")
            # Preserve processed hashes but delete job tracking and files
            files_to_delete = [os.path.join(checkpoint_dir, 'batch_jobs.pkl')] + batch_files
            files_to_preserve = [
                os.path.join(checkpoint_dir, 'processed_hashes.pkl'),
                os.path.join(checkpoint_dir, 'batch_processed_hashes.pkl')
            ]
        else:
            print("ğŸ“ Deleting all checkpoint files...")
            # Delete everything
            files_to_delete = real_time_files + batch_processing_files + batch_files
            files_to_preserve = []
        
        # Delete specified files
        for file_path in files_to_delete:
            if os.path.exists(file_path):
                os.remove(file_path)
                files_deleted.append(os.path.basename(file_path))
                print(f"   âœ… Deleted: {os.path.basename(file_path)}")
        
        # Track preserved files
        for file_path in files_to_preserve:
            if os.path.exists(file_path):
                files_preserved.append(os.path.basename(file_path))
                print(f"   ğŸ’¾ Preserved: {os.path.basename(file_path)}")
        
        # Summary
        print(f"\nğŸ‰ Reset Summary:")
        if blacklisted_count > 0:
            print(f"   ğŸš« Blacklisted {blacklisted_count} processed files (will be ignored in future runs)")
        if weaviate_collection_deleted:
            print(f"   ğŸ—„ï¸  Dropped EntityString collection from Weaviate")
        if files_deleted:
            print(f"   ğŸ“ Deleted {len(files_deleted)} files:")
            for filename in files_deleted:
                print(f"      â€¢ {filename}")
        if files_preserved:
            print(f"   ğŸ’¾ Preserved {len(files_preserved)} tracking files:")
            for filename in files_preserved:
                print(f"      â€¢ {filename}")
        if not files_deleted and not files_preserved:
            print("   â„¹ï¸  No files found to delete or preserve")
        
        if preserve_tracking:
            print(f"\nğŸ“‹ Selective reset complete. Previously processed strings will be skipped.")
            print(f"   Benefits:")
            print(f"   â€¢ Avoids reprocessing already embedded strings")
            print(f"   â€¢ Saves API costs and processing time")
            print(f"   â€¢ Maintains processing history")
        else:
            print(f"\nğŸ“‹ Full reset complete. All tracking data cleared.")
        
        print(f"\n   You can now:")
        print(f"   â€¢ Run embedding with real-time processing")
        print(f"   â€¢ Run embedding with batch processing: python batch_manager.py --create")
        
    except Exception as e:
        print(f"âŒ Error resetting embedding stage: {str(e)}")
        sys.exit(1)

def consolidate_batch_state_cli(config: Dict[str, Any]) -> None:
    """CLI handler for batch state consolidation."""
    print("ğŸ”„ Consolidating batch processing state...")
    
    if not TRANSITION_AVAILABLE:
        print("âŒ Transition modules not available. Check imports.")
        sys.exit(1)
    
    try:
        results = consolidate_batch_state(config)
        
        if results['status'] == 'completed':
            summary = results['summary']
            print(f"\nâœ… Batch state consolidation completed successfully!")
            print(f"ğŸ“Š Consolidated {summary['consolidated_state']['total_processed_hashes']:,} processed hashes")
            print(f"ğŸ“Š Found {results['failed_request_analysis']['total_failed']:,} failed requests")
            print(f"ğŸ“Š {results['failed_request_analysis']['retryable']:,} requests are retryable")
            
            if results['transition_ready']['ready']:
                print("\nğŸŸ¢ System is ready for batch-to-real-time transition")
            else:
                print("\nğŸ”´ System is NOT ready for transition:")
                for issue in results['transition_ready']['issues']:
                    print(f"  âŒ {issue}")
        else:
            print(f"âŒ Consolidation failed: {results.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"âŒ Error in state consolidation: {str(e)}")
        sys.exit(1)

def analyze_transition_cli(config: Dict[str, Any]) -> None:
    """CLI handler for transition analysis."""
    print("ğŸ“Š Analyzing readiness for batch-to-real-time transition...")
    
    if not TRANSITION_AVAILABLE:
        print("âŒ Transition modules not available. Check imports.")
        sys.exit(1)
    
    try:
        controller = TransitionController(config)
        analysis = controller.pre_transition_analysis()
        
        print("\n" + "="*60)
        print("TRANSITION READINESS ANALYSIS")
        print("="*60)
        print(f"Transition Feasible: {'ğŸŸ¢ YES' if analysis['transition_feasible'] else 'ğŸ”´ NO'}")
        
        # Show batch state
        batch_state = analysis['batch_state']
        print(f"\nğŸ“¦ BATCH STATE:")
        print(f"  Processed hashes: {batch_state['batch_processed_hashes']:,}")
        print(f"  Failed requests: {batch_state['failed_requests']:,}")
        print(f"  Active jobs: {batch_state.get('queue_active', 0):,}")
        print(f"  Completed jobs: {batch_state.get('queue_completed', 0):,}")
        
        # Show real-time state
        realtime_state = analysis['realtime_state']
        print(f"\nâš¡ REAL-TIME STATE:")
        print(f"  Processed hashes: {realtime_state['realtime_processed_hashes']:,}")
        
        # Show issues
        if analysis['errors']:
            print("\nâŒ BLOCKING ISSUES:")
            for error in analysis['errors']:
                print(f"  â€¢ {error}")
        
        # Show warnings
        if analysis['warnings']:
            print("\nâš ï¸  WARNINGS:")
            for warning in analysis['warnings']:
                print(f"  â€¢ {warning}")
        
        # Show recommendations
        if analysis['recommendations']:
            print("\nğŸ’¡ RECOMMENDATIONS:")
            for rec in analysis['recommendations']:
                print(f"  â€¢ {rec}")
        
        if analysis['transition_feasible']:
            print("\nâœ… Ready to proceed with transition!")
            print("   Use --switch-to-realtime to execute the transition")
        else:
            print("\nâŒ Transition is not recommended at this time")
            print("   Resolve the blocking issues or use --force to proceed anyway")
            
    except Exception as e:
        print(f"âŒ Error in transition analysis: {str(e)}")
        sys.exit(1)

def switch_to_realtime_cli(config: Dict[str, Any], force: bool = False) -> None:
    """CLI handler for switching to real-time processing."""
    print("ğŸ”„ Switching from batch to real-time processing...")
    
    if not TRANSITION_AVAILABLE:
        print("âŒ Transition modules not available. Check imports.")
        sys.exit(1)
    
    if not force:
        print("\nâš ï¸  This will:")
        print("   1. Terminate batch processing")
        print("   2. Consolidate all batch state")
        print("   3. Switch to real-time processing")
        print("   4. Preserve all progress and retry failed requests")
        
        response = input("\nProceed with transition? (y/N): ")
        if response.lower() != 'y':
            print("âŒ Transition cancelled by user")
            return
    
    try:
        import pickle
        
        # Load preprocessing data
        checkpoint_dir = config.get("checkpoint_dir", "data/checkpoints")
        
        print("ğŸ“ Loading preprocessing data...")
        with open(os.path.join(checkpoint_dir, "string_dict.pkl"), 'rb') as f:
            string_dict = pickle.load(f)
        
        with open(os.path.join(checkpoint_dir, "field_hash_mapping.pkl"), 'rb') as f:
            field_hash_mapping = pickle.load(f)
        
        with open(os.path.join(checkpoint_dir, "string_counts.pkl"), 'rb') as f:
            string_counts = pickle.load(f)
        
        print(f"âœ… Loaded {len(string_dict):,} strings, {len(field_hash_mapping):,} field mappings")
        
        # Execute transition
        print("\nğŸš€ Executing batch-to-real-time transition...")
        results = transition_batch_to_realtime(
            config, string_dict, field_hash_mapping, string_counts, force
        )
        
        print("\n" + "="*60)
        print("TRANSITION RESULTS")
        print("="*60)
        
        if results['status'] == 'completed':
            print(f"âœ… Transition completed successfully in {results['elapsed_time']:.2f} seconds!")
            
            # Show consolidation stats
            if 'consolidation' in results and 'summary' in results['consolidation']:
                summary = results['consolidation']['summary']
                cs = summary.get('consolidated_state', {})
                print(f"ğŸ“Š Total processed hashes: {cs.get('total_processed_hashes', 0):,}")
                print(f"ğŸ“Š From batch processing: {cs.get('batch_only_hashes', 0):,}")
                print(f"ğŸ“Š From real-time processing: {cs.get('realtime_only_hashes', 0):,}")
            
            # Show real-time processing results
            if 'realtime_processing' in results:
                rt_results = results['realtime_processing']
                print(f"âš¡ Real-time processed: {rt_results.get('strings_processed', 0):,} strings")
                print(f"âš¡ Tokens used: {rt_results.get('tokens_used', 0):,}")
                
                if 'failed_requests' in rt_results:
                    failed = rt_results['failed_requests']
                    print(f"âš¡ Failed requests: {failed.get('total_failed', 0):,} total, {failed.get('retryable', 0):,} retryable")
            
            print("\nğŸ‰ System is now running in real-time mode!")
            print("   All batch progress has been preserved and failed requests will be retried.")
            
        elif results['status'] == 'blocked':
            print("âŒ Transition was blocked by pre-analysis issues")
            print("   Use --force to proceed anyway or resolve the issues first")
            
        else:
            print(f"âŒ Transition failed: {results.get('error', 'Unknown error')}")
            if 'transition_state' in results:
                ts = results['transition_state']
                if ts.get('errors'):
                    print("\nErrors encountered:")
                    for error in ts['errors']:
                        print(f"  â€¢ {error}")
            
    except Exception as e:
        print(f"âŒ Error in transition execution: {str(e)}")
        sys.exit(1)

def consolidate_batch_state_cli(config: Dict[str, Any]) -> None:
    """CLI handler for batch state consolidation."""
    if not TRANSITION_AVAILABLE:
        print("âŒ Transition modules not available")
        print("ğŸ’¡ Please ensure transition modules are properly installed")
        sys.exit(1)
    
    print("\nğŸ”„ Consolidating batch processing state...")
    print("="*50)
    
    try:
        from src.batch_state_consolidator import consolidate_batch_state
        
        # Run consolidation
        results = consolidate_batch_state(config)
        
        if results['status'] == 'completed':
            summary = results['summary']
            print(f"âœ… State consolidation completed successfully!")
            print(f"\nğŸ“Š CONSOLIDATION SUMMARY:")
            print(f"   â€¢ Total processed hashes: {summary['consolidated_state']['total_processed_hashes']:,}")
            print(f"   â€¢ From batch only: {summary['consolidated_state']['batch_only_hashes']:,}")
            print(f"   â€¢ From real-time only: {summary['consolidated_state']['realtime_only_hashes']:,}")
            print(f"   â€¢ Hash overlap: {summary['consolidated_state']['overlap_hashes']:,}")
            
            failed_analysis = results['failed_request_analysis']
            print(f"\nğŸ“‹ FAILED REQUEST ANALYSIS:")
            print(f"   â€¢ Total failed: {failed_analysis['total_failed']:,}")
            print(f"   â€¢ Retryable: {failed_analysis['retryable']:,}")
            print(f"   â€¢ Max retries exceeded: {failed_analysis['max_retries_exceeded']:,}")
            
            if results['transition_ready']['ready']:
                print(f"\nâœ… System is ready for batch-to-real-time transition")
                print(f"ğŸ’¡ Next step: python batch_manager.py --switch-to-realtime")
            else:
                print(f"\nâŒ System is NOT ready for transition:")
                for issue in results['transition_ready']['issues']:
                    print(f"   â€¢ {issue}")
                print(f"ğŸ’¡ Resolve issues or use --force flag")
                
        else:
            print(f"âŒ Consolidation failed: {results.get('error', 'Unknown error')}")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ Error during consolidation: {str(e)}")
        sys.exit(1)

def switch_to_realtime_cli(config: Dict[str, Any], force: bool = False) -> None:
    """CLI handler for switching to real-time processing."""
    if not TRANSITION_AVAILABLE:
        print("âŒ Transition modules not available")
        print("ğŸ’¡ Please ensure transition modules are properly installed")
        sys.exit(1)
    
    print("\nğŸš€ Switching from batch to real-time processing...")
    print("="*60)
    
    try:
        from src.transition_controller import transition_batch_to_realtime
        
        # Load preprocessing data
        checkpoint_dir = config.get("checkpoint_dir", "data/checkpoints")
        string_dict, field_hash_mapping, string_counts = load_preprocessing_data(checkpoint_dir)
        
        print(f"ğŸ“Š Loaded preprocessing data:")
        print(f"   â€¢ Strings: {len(string_dict):,}")
        print(f"   â€¢ Field mappings: {len(field_hash_mapping):,}")
        print(f"   â€¢ String counts: {len(string_counts):,}")
        
        # Execute transition
        print(f"\nğŸ”„ Executing transition (force={force})...")
        results = transition_batch_to_realtime(
            config, string_dict, field_hash_mapping, string_counts, force
        )
        
        print(f"\nğŸ“‹ TRANSITION RESULTS")
        print("="*30)
        print(f"Status: {results['status']}")
        
        if results['status'] == 'completed':
            print(f"âœ… Transition completed successfully!")
            print(f"â±ï¸  Elapsed Time: {results['elapsed_time']:.2f} seconds")
            
            # Show consolidation stats
            if 'consolidation' in results:
                summary = results['consolidation'].get('summary', {})
                if 'consolidated_state' in summary:
                    cs = summary['consolidated_state']
                    print(f"\nğŸ“Š FINAL STATE:")
                    print(f"   â€¢ Total processed hashes: {cs['total_processed_hashes']:,}")
                    print(f"   â€¢ From batch only: {cs['batch_only_hashes']:,}")
                    print(f"   â€¢ From real-time only: {cs['realtime_only_hashes']:,}")
            
            print(f"\nğŸš€ Real-time processing is now active!")
            print(f"ğŸ’¡ You can now run: python main.py --config config.yml")
            
        elif results['status'] == 'blocked':
            print("âŒ Transition blocked by pre-analysis issues")
            print("ğŸ’¡ Use --force to proceed anyway or resolve issues first")
            
            if 'pre_analysis' in results:
                analysis = results['pre_analysis']
                if analysis.get('errors'):
                    print("\nğŸš« BLOCKING ISSUES:")
                    for error in analysis['errors']:
                        print(f"   â€¢ {error}")
            
            sys.exit(1)
            
        else:
            print(f"âŒ Transition failed: {results.get('error', 'Unknown error')}")
            if 'transition_state' in results:
                ts = results['transition_state']
                if ts.get('errors'):
                    print("\nğŸ” ERROR DETAILS:")
                    for error in ts['errors']:
                        print(f"   â€¢ {error}")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ Error during transition: {str(e)}")
        sys.exit(1)

def analyze_transition_cli(config: Dict[str, Any]) -> None:
    """CLI handler for analyzing transition readiness."""
    if not TRANSITION_AVAILABLE:
        print("âŒ Transition modules not available")
        print("ğŸ’¡ Please ensure transition modules are properly installed")
        sys.exit(1)
    
    print("\nğŸ” Analyzing batch-to-real-time transition readiness...")
    print("="*60)
    
    try:
        from src.transition_controller import TransitionController
        
        controller = TransitionController(config)
        analysis = controller.pre_transition_analysis()
        
        print(f"Transition Feasible: {'âœ… YES' if analysis['transition_feasible'] else 'âŒ NO'}")
        
        if analysis['errors']:
            print(f"\nğŸš« BLOCKING ISSUES:")
            for error in analysis['errors']:
                print(f"   â€¢ {error}")
        
        if analysis['warnings']:
            print(f"\nâš ï¸  WARNINGS:")
            for warning in analysis['warnings']:
                print(f"   â€¢ {warning}")
        
        if analysis['recommendations']:
            print(f"\nğŸ’¡ RECOMMENDATIONS:")
            for rec in analysis['recommendations']:
                print(f"   â€¢ {rec}")
        
        # Show detailed state information
        if 'batch_state' in analysis:
            batch_state = analysis['batch_state']
            print(f"\nğŸ“Š BATCH PROCESSING STATE:")
            print(f"   â€¢ Processed hashes: {batch_state.get('processed_hashes', 0):,}")
            print(f"   â€¢ Failed requests: {batch_state.get('failed_requests', 0):,}")
            print(f"   â€¢ Batch jobs: {batch_state.get('batch_jobs', 0):,}")
            print(f"   â€¢ Active jobs: {batch_state.get('queue_active', 0):,}")
            print(f"   â€¢ Completed jobs: {batch_state.get('queue_completed', 0):,}")
        
        if 'realtime_state' in analysis:
            realtime_state = analysis['realtime_state']
            print(f"\nğŸ“Š REAL-TIME PROCESSING STATE:")
            print(f"   â€¢ Processed hashes: {realtime_state.get('processed_hashes', 0):,}")
        
        # Next steps
        print(f"\nğŸ“‹ NEXT STEPS:")
        if analysis['transition_feasible']:
            print(f"   1. Consolidate state: python batch_manager.py --consolidate-state")
            print(f"   2. Execute transition: python batch_manager.py --switch-to-realtime")
        else:
            print(f"   1. Resolve blocking issues listed above")
            print(f"   2. Or use --force flag to proceed anyway")
            print(f"   3. python batch_manager.py --switch-to-realtime --force")
            
    except Exception as e:
        print(f"âŒ Error during analysis: {str(e)}")
        sys.exit(1)

def main():
    """Main function."""
    parser = argparse.ArgumentParser(
        description='Manual Batch Manager for Entity Resolution',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Create batch jobs
  python batch_manager.py --create
  
  # Check job status
  python batch_manager.py --status
  
  # Download and process results
  python batch_manager.py --download
  
  # Transition to real-time processing
  python batch_manager.py --analyze-transition    # Analyze readiness
  python batch_manager.py --consolidate-state     # Consolidate state
  python batch_manager.py --switch-to-realtime    # Execute transition
  
  # Recover all batch jobs from OpenAI (if checkpoints corrupted)
  python batch_manager.py --recover
  
  # Investigate failed jobs
  python batch_manager.py --investigate
  
  # Resubmit failed jobs (for token limit issues)
  python batch_manager.py --resubmit
  
  # Cancel all uncompleted jobs (frees token quota)
  python batch_manager.py --cancel
  
  # Reset embedding stage (clear all checkpoints)
  python batch_manager.py --reset
  
  # Full workflow
  python batch_manager.py --create
  # ... wait for jobs to complete ...
  python batch_manager.py --status
  python batch_manager.py --download
        """
    )
    
    parser.add_argument('--config', default='config.yml', 
                       help='Path to configuration file (default: config.yml)')
    
    # Action arguments (mutually exclusive)
    action_group = parser.add_mutually_exclusive_group(required=True)
    action_group.add_argument('--create', action='store_true',
                             help='Create batch jobs and upload to OpenAI')
    action_group.add_argument('--status', action='store_true',
                             help='Check status of existing batch jobs')
    action_group.add_argument('--download', action='store_true',
                             help='Download and process completed batch results')
    action_group.add_argument('--reset', action='store_true',
                             help='Reset embedding_and_indexing stage (clear all checkpoints and files)')
    action_group.add_argument('--reset-selective', action='store_true',
                             help='Reset embedding stage but preserve processed hash tracking to avoid reprocessing')
    action_group.add_argument('--recover', action='store_true',
                             help='Recover all batch jobs from OpenAI API (useful when checkpoints are corrupted)')
    action_group.add_argument('--investigate', action='store_true',
                             help='Investigate failed batch jobs to understand failure reasons')
    action_group.add_argument('--resubmit', action='store_true',
                             help='Resubmit failed batch jobs (useful for token limit failures)')
    action_group.add_argument('--cancel', action='store_true',
                             help='Cancel all uncompleted batch jobs (frees up token quota)')
    action_group.add_argument('--consolidate-state', action='store_true',
                             help='Consolidate batch state for real-time transition')
    action_group.add_argument('--switch-to-realtime', action='store_true',
                             help='Switch from batch to real-time processing')
    action_group.add_argument('--analyze-transition', action='store_true',
                             help='Analyze readiness for batch-to-real-time transition')
    
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='Enable verbose logging')
    parser.add_argument('--force', action='store_true',
                       help='Force transition even with active batch jobs (for transition commands)')
    
    args = parser.parse_args()
    
    # Configure logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Load configuration
    config = load_config(args.config)
    
    # For reset, recover, investigate, resubmit, and cancel commands, we don't need batch processing to be enabled
    if not args.reset and not args.reset_selective and not args.recover and not args.investigate and not args.resubmit and not args.cancel and not config.get('use_batch_embeddings', False):
        print("âš ï¸  Batch embeddings are not enabled in configuration.")
        print("   Set 'use_batch_embeddings: true' in your config.yml")
        sys.exit(1)
    
    # Execute requested action
    try:
        if args.create:
            create_jobs(config)
        elif args.status:
            check_status(config)
        elif args.download:
            download_results(config)
        elif args.reset:
            reset_embedding_stage(config)
        elif args.reset_selective:
            reset_embedding_stage_selective(config)
        elif args.recover:
            recover_all_jobs(config)
        elif args.investigate:
            investigate_failures(config)
        elif args.resubmit:
            resubmit_failed_jobs(config)
        elif args.cancel:
            cancel_uncompleted_jobs(config)
        elif args.consolidate_state:
            consolidate_batch_state_cli(config)
        elif args.switch_to_realtime:
            switch_to_realtime_cli(config, args.force)
        elif args.analyze_transition:
            analyze_transition_cli(config)
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ Operation cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Unexpected error: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()