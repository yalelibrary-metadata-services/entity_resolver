#!/usr/bin/env python3
"""
Tokenization Analysis Visualization
Shows multi-language tokenization efficiency and challenges
"""

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from pathlib import Path

# Set up professional styling
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']
plt.rcParams['font.size'] = 10
plt.rcParams['axes.linewidth'] = 0.8

def create_tokenization_analysis():
    """Create comprehensive tokenization analysis visualization"""
    
    # Create figure with subplots
    fig = plt.figure(figsize=(16, 12))
    
    # Set up grid layout
    gs = fig.add_gridspec(3, 3, height_ratios=[1, 1.2, 1], width_ratios=[1.2, 1, 1])
    
    # Main title
    fig.suptitle('Tokenization Analysis: The Hidden Foundation of Text Processing', 
                fontsize=16, fontweight='bold', y=0.95)
    
    # 1. Multi-language efficiency comparison
    ax1 = fig.add_subplot(gs[0, :])
    
    # Sample text data across languages
    languages = ['English', 'Spanish', 'Portuguese', 'French', 'Russian', 'Chinese', 'Arabic']
    text_samples = [
        "The influence of baroque music on contemporary literature",
        "La influencia de la m√∫sica barroca en la literatura contempor√°nea", 
        "A influ√™ncia da m√∫sica barroca na literatura contempor√¢nea",
        "L'influence de la musique baroque sur la litt√©rature contemporaine",
        "–í–ª–∏—è–Ω–∏–µ –º—É–∑—ã–∫–∏ –±–∞—Ä–æ–∫–∫–æ –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—É",
        "Â∑¥Ê¥õÂÖãÈü≥‰πêÂØπÂΩì‰ª£ÊñáÂ≠¶ÁöÑÂΩ±Âìç",
        "ÿ™ÿ£ÿ´Ÿäÿ± ÿßŸÑŸÖŸàÿ≥ŸäŸÇŸâ ÿßŸÑÿ®ÿßÿ±ŸàŸÉŸäÿ© ÿπŸÑŸâ ÿßŸÑÿ£ÿØÿ® ÿßŸÑŸÖÿπÿßÿµÿ±"
    ]
    
    # Simulated tokenization data (based on realistic patterns)
    char_counts = [len(text) for text in text_samples]
    token_counts = [12, 16, 15, 14, 8, 13, 11]  # Realistic token counts
    efficiency = [char_counts[i]/token_counts[i] for i in range(len(languages))]
    
    # Create efficiency comparison
    colors = plt.cm.Set3(np.linspace(0, 1, len(languages)))
    bars = ax1.bar(languages, efficiency, color=colors, alpha=0.8, edgecolor='navy', linewidth=1)
    
    # Add value labels on bars
    for i, (bar, eff, tokens) in enumerate(zip(bars, efficiency, token_counts)):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                f'{eff:.1f}\n({tokens} tokens)',
                ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    ax1.set_ylabel('Characters per Token', fontweight='bold')
    ax1.set_title('Tokenization Efficiency Across Languages\n(Same semantic meaning: "baroque music influence on literature")', 
                 fontweight='bold', pad=20)
    ax1.tick_params(axis='x', rotation=45)
    ax1.grid(axis='y', alpha=0.3)
    ax1.set_ylim(0, max(efficiency) * 1.2)
    
    # Add efficiency line
    ax1.axhline(y=3.5, color='red', linestyle='--', alpha=0.7, linewidth=2)
    ax1.text(len(languages)/2, 3.7, 'Efficiency Benchmark (3.5)', 
             ha='center', va='bottom', color='red', fontweight='bold')
    
    # 2. Yale catalog examples - token length distribution
    ax2 = fig.add_subplot(gs[1, 0])
    
    # Realistic Yale catalog data
    record_types = ['Simple Names', 'Names + Dates', 'Basic Records', 'Full Records', 'Complex Multi-field']
    avg_tokens = [8, 25, 85, 180, 350]
    min_tokens = [3, 15, 45, 120, 200]
    max_tokens = [15, 45, 150, 280, 500]
    
    # Create horizontal bar chart with error bars
    y_pos = np.arange(len(record_types))
    bars = ax2.barh(y_pos, avg_tokens, 
                   xerr=[np.array(avg_tokens) - np.array(min_tokens), 
                         np.array(max_tokens) - np.array(avg_tokens)],
                   color=['#3498DB', '#9B59B6', '#E67E22', '#E74C3C', '#C0392B'],
                   alpha=0.8, capsize=5)
    
    # Add SetFit limit line
    ax2.axvline(x=128, color='red', linestyle='-', linewidth=3, alpha=0.8)
    ax2.text(135, 2, 'SetFit Limit\n(128 tokens)', 
             va='center', color='red', fontweight='bold',
             bbox=dict(boxstyle="round,pad=0.3", facecolor='white', 
                      edgecolor='red', alpha=0.9))
    
    ax2.set_yticks(y_pos)
    ax2.set_yticklabels(record_types)
    ax2.set_xlabel('Token Count', fontweight='bold')
    ax2.set_title('Yale Catalog Record\nToken Length Distribution', fontweight='bold')
    ax2.grid(axis='x', alpha=0.3)
    
    # Add percentage labels
    percentages = ['100%', '100%', '100%', '25%', '0%']  # Percentage that fit in SetFit
    for i, (bar, pct) in enumerate(zip(bars, percentages)):
        width = bar.get_width()
        color = 'green' if pct == '100%' else 'orange' if pct == '25%' else 'red'
        ax2.text(width + 10, bar.get_y() + bar.get_height()/2,
                f'{pct} fit SetFit',
                ha='left', va='center', fontweight='bold', color=color)
    
    # 3. Cost implications
    ax3 = fig.add_subplot(gs[1, 1])
    
    # Cost analysis data
    scenarios = ['Development\n(50K records)', 'Pilot\n(1M records)', 'Production\n(17.6M records)']
    standard_costs = [150, 2640, 52800]
    batch_costs = [75, 1320, 26400]
    
    x = np.arange(len(scenarios))
    width = 0.35
    
    bars1 = ax3.bar(x - width/2, standard_costs, width, label='Standard API', 
                   color='#E74C3C', alpha=0.8)
    bars2 = ax3.bar(x + width/2, batch_costs, width, label='Batch API (50% off)', 
                   color='#27AE60', alpha=0.8)
    
    ax3.set_ylabel('Cost (USD)', fontweight='bold')
    ax3.set_title('Embedding Costs by Scale\n(text-embedding-3-small)', fontweight='bold')
    ax3.set_xticks(x)
    ax3.set_xticklabels(scenarios)
    ax3.legend()
    ax3.grid(axis='y', alpha=0.3)
    
    # Add cost labels
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + height*0.02,
                    f'${height:,.0f}',
                    ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    # 4. Token limit challenges
    ax4 = fig.add_subplot(gs[1, 2])
    
    # Model comparison
    models = ['Word2Vec\n(2013)', 'BERT\n(2018)', 'SetFit\n(2022)', 'OpenAI\n(2024)']
    token_limits = [None, 512, 128, 8192]
    colors = ['#95A5A6', '#3498DB', '#E74C3C', '#27AE60']
    
    # Handle None value for Word2Vec
    display_limits = [0, 512, 128, 8192]
    
    bars = ax4.bar(models, display_limits, color=colors, alpha=0.8)
    
    # Special handling for Word2Vec
    ax4.text(0, 50, 'No limit\n(word-level)', ha='center', va='center', 
             fontweight='bold', color='white')
    
    # Add labels for others
    for i, (bar, limit) in enumerate(zip(bars[1:], token_limits[1:]), 1):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height/2,
                f'{limit}\ntokens',
                ha='center', va='center', fontweight='bold', color='white')
    
    ax4.set_ylabel('Token Limit', fontweight='bold')
    ax4.set_title('Model Token Limits\nEvolution', fontweight='bold')
    ax4.tick_params(axis='x', rotation=45)
    
    # Add Yale requirement line
    ax4.axhline(y=200, color='orange', linestyle='--', linewidth=2, alpha=0.8)
    ax4.text(2, 250, 'Yale Avg Need\n(200 tokens)', 
             ha='center', va='bottom', color='orange', fontweight='bold')
    
    # 5. Key insights panel
    ax5 = fig.add_subplot(gs[2, :])
    ax5.axis('off')
    
    # Create insight boxes
    insights = [
        {
            'title': 'üåç Language Efficiency',
            'text': 'English: 3.5 chars/token\nChinese: 2.1 chars/token\nCosts vary by language!',
            'color': '#3498DB'
        },
        {
            'title': 'üìè Length Matters', 
            'text': 'Real catalog records: 180+ tokens\nSetFit limit: 128 tokens\n75% of data excluded!',
            'color': '#E74C3C'
        },
        {
            'title': 'üí∞ Cost Impact',
            'text': 'Production: $26K (batch) vs $53K\nBatch processing = 50% savings\nLanguage choice affects costs',
            'color': '#27AE60'
        },
        {
            'title': 'üéØ Model Choice',
            'text': 'Token limits drove architecture\nOpenAI: 8K tokens vs SetFit: 128\nConstraints shape solutions',
            'color': '#9B59B6'
        },
        {
            'title': 'üîç Hidden Complexity',
            'text': 'Tokenization is invisible but crucial\nTesting with realistic data essential\nEarly validation prevents late failures',
            'color': '#F39C12'
        }
    ]
    
    box_width = 0.18
    box_height = 0.8
    
    for i, insight in enumerate(insights):
        x = 0.02 + i * 0.196
        
        # Create colored box
        rect = plt.Rectangle((x, 0.1), box_width, box_height, 
                           facecolor=insight['color'], alpha=0.1,
                           edgecolor=insight['color'], linewidth=2)
        ax5.add_patch(rect)
        
        # Add title
        ax5.text(x + box_width/2, 0.75, insight['title'], 
                ha='center', va='center', fontweight='bold', 
                fontsize=11, color=insight['color'])
        
        # Add text
        ax5.text(x + box_width/2, 0.4, insight['text'], 
                ha='center', va='center', fontsize=9,
                bbox=dict(boxstyle="round,pad=0.1", facecolor='white', alpha=0.8))
    
    ax5.set_xlim(0, 1)
    ax5.set_ylim(0, 1)
    
    plt.tight_layout()
    
    return fig

def main():
    """Generate and save the tokenization analysis visualization"""
    
    # Create the visualization
    fig = create_tokenization_analysis()
    
    # Save to img directory
    output_path = Path(__file__).parent / "img" / "tokenization_comparison.png"
    fig.savefig(output_path, dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    
    print(f"‚úÖ Tokenization analysis saved to {output_path}")
    
    # Clean up
    plt.close(fig)

if __name__ == "__main__":
    main()